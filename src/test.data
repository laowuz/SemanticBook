Introduction problem searching patterns data fundamental long successful extensive astronomical observations Tycho Brahe 16th century allowed Johannes Kepler discover empirical laws planetary turn provided springboard development classical discovery regularities atomic spectra played key role development verification quantum physics early twentieth field pattern recognition concerned automatic discovery regularities data use algorithms use regularities actions classifying data different Consider example recognizing handwritten illustrated Figure digit corresponds pixel image represented vector comprising real goal build machine vector input produce identity digit nontrivial problem wide variability INTRODUCTION Figure Examples hand-written digits taken zip tackled handcrafted rules heuristics distinguishing digits based shapes practice approach leads proliferation rules exceptions rules invariably gives poor Far better results obtained adopting machine learning approach large set digits called training set used tune parameters adaptive categories digits training set known typically inspecting individually hand-labelling express category digit target vector represents identity corresponding Suitable techniques representing categories terms vectors discussed Note target vector digit image result running machine learning algorithm expressed function takes new digit image input generates output vector encoded way target precise form function determined training known learning basis training model trained determine identity new digit said comprise test ability categorize correctly new examples differ used training known practical variability input vectors training data comprise tiny fraction possible input generalization central goal pattern practical original input variables typically preprocessed transform new space variables pattern recognition problem easier digit recognition images digits typically translated scaled digit contained box fixed greatly reduces variability digit location scale digits makes easier subsequent pattern recognition algorithm distinguish different pre-processing stage called feature Note new test data pre-processed steps training Pre-processing performed order speed goal real-time face detection high-resolution video handle huge numbers pixels presenting directly complex pattern recognition algorithm computationally aim useful features fast INTRODUCTION preserve useful discriminatory information enabling faces distinguished features used inputs pattern recognition average value image intensity rectangular subregion evaluated extremely efficiently set features prove effective fast face number features smaller number kind pre-processing represents form dimensionality Care taken pre-processing information information important solution problem overall accuracy Applications training data comprises examples input vectors corresponding target vectors known supervised learning Cases digit recognition aim assign input vector finite number discrete called classification desired output consists continuous task called example regression problem prediction yield chemical manufacturing process inputs consist concentrations pattern recognition training data consists set input vectors corresponding target goal unsupervised learning problems discover groups similar examples called determine distribution data input known density project data high-dimensional space dimensions purpose technique reinforcement learning concerned problem finding suitable actions situation order maximize learning algorithm examples optimal contrast supervised instead discover process trial Typically sequence states actions learning algorithm interacting current action affects immediate reward impact reward subsequent time appropriate reinforcement learning techniques neural network learn play game backgammon high standard network learn board position result dice produce strong having network play copy million major challenge game backgammon involve dozens end game form reward attributed appropriately moves led moves good ones example credit assignment general feature reinforcement learning trade-off tries new kinds actions effective makes use actions known yield high strong focus exploration exploitation yield poor Reinforcement learning continues active area machine learning INTRODUCTION Figure Plot training data set shown blue comprising observation input variable corresponding target variable green curve shows function used generate goal predict value new value knowledge green detailed treatment lies scope tasks needs tools key ideas underpin common main goals chapter relatively informal important concepts illustrate simple Later book shall ideas re-emerge context sophisticated models applicable real-world pattern recognition chapter provides self-contained introduction important tools used probability decision information sound like daunting fact clear understanding essential machine learning techniques used best effect practical Polynomial Curve Fitting begin introducing simple regression shall use running example chapter motivate number key Suppose observe real-valued input variable wish use observation predict value real-valued target variable present instructive consider artificial example synthetically generated data know precise process generated data comparison learned data example generated function random noise included target described Appendix suppose training set comprising observations written xN corresponding observations values denoted Figure shows plot training set comprising data input data set Figure generated choosing values spaced uniformly range target data set obtained computing corresponding values function Polynomial Curve Fitting adding small level random noise having Gaussian distribution Gaussian distribution discussed Section point order obtain corresponding value generating data capturing property real data possess underlying wish individual observations corrupted random noise arise intrinsically stochastic processes radioactive decay typically sources variability goal exploit training set order make predictions value target variable new value input shall involves implicitly trying discover underlying function intrinsically difficult problem generalize finite data Furthermore observed data corrupted uncertainty appropriate value Probability discussed Section provides framework expressing uncertainty precise quantitative decision discussed Section allows exploit probabilistic representation order make predictions optimal according appropriate shall proceed informally consider simple approach based curve shall fit data polynomial function form w1x w2x2 wMxM wjx order xj denotes raised power polynomial coefficients wM collectively denoted vector Note polynomial function nonlinear function linear function coefficients linear unknown parameters important properties called linear models discussed extensively Chapters values coefficients determined fitting polynomial training minimizing error function measures misfit function value training set data simple choice error widely sum squares errors predictions data point xn corresponding target values minimize factor included later shall discuss motivation choice error function later moment simply note nonnegative quantity zero INTRODUCTION Figure error function corresponds half sum squares displacements vertical green data point function xn function pass exactly training data geometrical interpretation sum-of-squares error function illustrated Figure solve curve fitting problem choosing value small error function quadratic function coefficients derivatives respect coefficients linear elements minimization error function unique denoted closed resulting polynomial isExercise function remains problem choosing order shall turn example important concept called model comparison model Figure examples results fitting polynomials having orders data set shown Figure notice constant order polynomials poor fits data consequently poor representations function order polynomial best fit function examples shown Figure higher order polynomial obtain excellent fit training polynomial passes exactly data point fitted curve oscillates wildly gives poor representation function behaviour known noted goal achieve good generalization making accurate predictions new obtain quantitative insight dependence generalization performance considering separate test set comprising data points generated exactly procedure used generate training set points new choices random noise values included target choice evaluate residual value training evaluate test data convenient use root-mean-square Polynomial Curve Fitting Figure Plots polynomials having various orders shown red fitted data set shown Figure error defined ERMS division allows compare different sizes data sets equal square root ensures ERMS measured scale target variable Graphs training test set RMS errors various values Figure test set error measure doing predicting values new data observations note Figure small values relatively large values test set attributed fact corresponding polynomials inflexible incapable capturing oscillations function Values range small values test set reasonable representations generating function case Figure INTRODUCTION Figure Graphs root-mean-square defined evaluated training set independent test set various values Training Test training set error goes expect polynomial contains degrees freedom corresponding coefficients tuned exactly data points training test set error large saw Figure corresponding function exhibits wild paradoxical polynomial order contains lower order polynomials special polynomial capable generating results good suppose best predictor new data function data generated shall later know power series expansion function contains terms expect results improve monotonically increase gain insight problem examining values coefficients obtained polynomials various shown Table magnitude coefficients typically gets particular coefficients finely tuned data developing large positive negative values correspondTable Table coefficients polynomials various Observe typical magnitude coefficients increases dramatically order polynomial Polynomial Curve Fitting Figure Plots solutions obtained minimizing sum-of-squares error function polynomial data points data points increasing size data set reduces over-fitting ing polynomial function matches data points data points near ends function exhibits large oscillations observed Figure happening flexible polynomials larger values increasingly tuned random noise target interesting examine behaviour model size data set shown Figure model over-fitting problem severe size data set way say larger data complex words model afford fit rough heuristic advocated number data points multiple number adaptive parameters shall Chapter number parameters necessarily appropriate measure model unsatisfying having limit number parameters model according size available training reasonable choose complexity model according complexity problem shall squares approach finding model parameters represents specific case maximum likelihood Section over-fitting problem understood general property maximum adopting Bayesian theSection over-fitting problem shall difficulty Bayesian perspective employing models number parameters greatly exceeds number data Bayesian model effective number parameters adapts automatically size data instructive continue current approach consider practice apply data sets limited size INTRODUCTION Figure Plots polynomials fitted data set shown Figure regularized error function values regularization parameter corresponding case corresponding shown right Figure wish use relatively complex flexible technique used control over-fitting phenomenon cases involves adding penalty term error function order discourage coefficients reaching large simplest penalty term takes form sum squares leading modified error function form wTw w20 w21 w2M coefficient governs relative importance regularization term compared sum-of-squares error Note coefficient omitted regularizer inclusion causes results depend choice origin target variable et included regularization coefficient shall discuss topic Section error function minimized exactly closed Techniques knownExercise statistics literature shrinkage methods reduce value particular case quadratic regularizer called ridge regression context neural approach known weight Figure shows results fitting polynomial order data set regularized error function value over-fitting suppressed obtain closer representation underlying function use large value obtain poor shown Figure corresponding coefficients fitted polynomials Table showing regularization desired effect reducing Polynomial Curve Fitting Table Table coefficients polynomials various values regularization parameter Note corresponds model graph right Figure value typical magnitude coefficients gets magnitude impact regularization term generalization error seen plotting value RMS error training test sets shown Figure effect controls effective complexity model determines degree issue model complexity important discussed length Section simply note trying solve practical application approach minimizing error way determine suitable value model results suggest simple way achieving taking available data partitioning training used determine coefficients separate validation called hold-out used optimize model complexity prove wasteful valuable training seek sophisticated far discussion polynomial curve fitting appealed largely seek principled approach solving problems pattern recognition turning discussion probability providing foundation nearly subsequent developments Figure Graph root-mean-square error versus Training Test INTRODUCTION important insights concepts introduced context polynomial curve fitting allow extend complex Probability Theory key concept field pattern recognition arises noise finite size data Probability theory provides consistent framework quantification manipulation uncertainty forms central foundations pattern combined decision discussed Section allows make optimal predictions information available information incomplete introduce basic concepts probability theory considering simple Imagine red red box apples blue box apples illustrated Figure suppose randomly pick boxes box randomly select item having observed sort fruit replace box imagine repeating process Let suppose doing pick red box time pick blue box remove item fruit box equally likely select pieces fruit identity box chosen random shall denote random variable possible red blue identity fruit random variable denoted values begin shall define probability event fraction times event occurs total number limit total number trials goes probability selecting red box Figure use simple example coloured boxes containing fruit shown green oranges shown introduce basic ideas Probability Theory Figure derive sum product rules probability considering random takes values takes values illustration consider total number instances denote number instances xi yj nij number points corresponding cell number points column corresponding denoted number points row corresponding yj denoted rj rjyj xi nij probability selecting blue box write probabilities Note probabilities lie interval events mutually exclusive include possible outcomes example box red probabilities events sum ask questions overall probability selection procedure pick chosen probability box chose blue answer questions complex questions associated problems pattern equipped elementary rules known sum rule product Having obtained shall return boxes fruit order derive rules consider slightly general example shown Figure involving random variables instance Box Fruit variables considered shall suppose values xi values yj Consider total trials sample variables let number trials xi yj nij let number trials takes value xi value denoted similarly let number trials takes value yj denoted rj probability value xi value yj written called joint probability xi yj number points falling cell fraction total number nij implicitly considering limit probability takes value xi irrespective value written fraction total number points fall column number instances column Figure just sum number instances cell nij INTRODUCTION sum rule Note called marginal obtained summing variables case consider instances fraction instances yj written yj called conditional probability yj obtained finding fraction points column fall cell yj nij derive following relationship nij nij yj product rule far quite careful make distinction random box fruit values random variable example box red probability takes value denoted helps avoid leads cumbersome cases need simply write denote distribution random variable denote distribution evaluated particular value provided interpretation clear compact write fundamental rules probability theory following Rules Probability sum rule product rule joint probability verbalized probability quantity conditional probability verbalized probability quantity marginal probability Probability Theory simply probability simple rules form basis probabilistic machinery use product symmetry property immediately obtain following relationship conditional probabilities called theorem plays central role pattern recognition machine sum denominator theorem expressed terms quantities appearing numerator view denominator theorem normalization constant required ensure sum conditional probability left-hand values equals Figure simple example involving joint distribution variables illustrate concept marginal conditional finite sample data points drawn joint distribution shown right histogram fractions data points having values definition fractions equal corresponding probabilities limit view histogram simple way model probability distribution finite number points drawn Modelling distributions data lies heart statistical pattern recognition explored great remaining plots Figure corresponding histogram estimates Let return example involving boxes shall explicit distinguishing random variables seen probabilities selecting red blue boxes Note satisfy suppose pick box turns blue probability selecting apple just fraction apples blue box write conditional probabilities type selected box INTRODUCTION Figure illustration distribution takes possible takes possible left figure shows sample points drawn joint probability distribution remaining figures histogram estimates marginal distributions conditional distribution corresponding row left note probabilities normalized similarly use sum product rules probability evaluate overall probability choosing apple sum Probability Theory Suppose instead told piece fruit selected like know box came requires evaluate probability distribution boxes conditioned identity probabilities probability distribution fruit conditioned identity solve problem reversing conditional probability theorem sum follows provide important interpretation theorem asked box chosen told identity selected item complete information available provided probability prior probability probability available observe identity told fruit use theorem compute probability shall posterior probability probability obtained observed Note prior probability selecting red box likely select blue box red observed piece selected fruit posterior probability red box likely box selected fact red result accords proportion oranges higher red box blue observation fruit orange provides significant evidence favouring red evidence sufficiently strong outweighs prior makes likely red box chosen blue note joint distribution variables factorizes product said product conditional distribution independent value boxes fruit box contained fraction apples probability apple independent box Probability densities considering probabilities defined discrete sets wish consider probabilities respect continuous shall limit relatively informal probability real-valued variable falling interval called probability density illustrated Figure probability lie interval INTRODUCTION Figure concept probability discrete variables extended probability density continuous variable probability lying interval probability density expressed derivative cumulative distribution function probabilities value lie real probability density satisfy conditions dx nonlinear change probability density transforms differently simple Jacobian consider change variables function consider probability density corresponds density respect new variable suffices denote fact different Observations falling range small values transformed range dxdy consequence property concept maximum probability density dependent choice probability lies interval cumulative distribution function defined dx satisfies shown Figure continuous variables denoted collectively vector define joint probability density Probability Theory probability falling infinitesimal volume containing point multivariate probability density satisfy dx integral taken consider joint probability distributions combination discrete continuous Note discrete called probability mass function regarded set concentrated allowed values sum product rules apply equally case probability combinations discrete continuous real sum product rules form dy formal justification sum product rules continuous variables requires branch mathematics called measure theory lies outside scope validity seen dividing real variable intervals width considering discrete probability distribution Taking limit turns sums integrals gives desired Expectations covariances important operations involving probabilities finding weighted averages average value function probability distribution called expectation denoted discrete average weighted relative probabilities different values case continuous expectations expressed terms integration respect corresponding probability density finite number points drawn probability distribution probability expectation approximated INTRODUCTION finite sum points shall make extensive use result discuss sampling methods Chapter approximation exact limit considering expectations functions case use subscript indicate variable averaged instance denotes average function respect distribution Note function consider conditional expectation respect conditional analogous definition continuous variance defined provides measure variability mean value Expanding variance written terms expectations consider variance variable random variables covariance defined expresses extent vary covariance case vectors random variables covariance matrix consider covariance components vector use slightly simpler notation Probability Theory Bayesian probabilities far viewed probabilities terms frequencies repeatable shall refer classical frequentist interpretation turn general Bayesian probabilities provide quantification Consider uncertain example moon orbit Arctic ice cap disappeared end events repeated numerous times order define notion probability did earlier context boxes generally quickly think polar ice obtain fresh instance new Earth observation satellite gathering novel forms diagnostic revise opinion rate ice assessment matters affect actions instance extent endeavour reduce emission greenhouse like able quantify expression uncertainty make precise revisions uncertainty light new subsequently able optimal actions decisions achieved Bayesian interpretation use probability represent ad-hoc inevitable respect common sense making rational coherent Cox showed numerical values used represent degrees simple set axioms encoding common sense properties beliefs leads uniquely set rules manipulating degrees belief equivalent sum product rules provided rigorous proof probability theory regarded extension Boolean logic situations involving uncertainty Numerous authors proposed different sets properties axioms measures uncertainty satisfy resulting numerical quantities behave precisely according rules natural refer quantities field pattern helpful general noThomas Bayes Thomas Bayes born Tunbridge Wells clergyman amateur scientist studied logic theology Edinburgh University elected Fellow Royal Society 18th issues regarding probability arose connection gambling new concept particularly important problem concerned so-called inverse solution proposed Thomas Bayes paper solving problem doctrine published years Philosophical Transactions Royal Bayes formulated theory case uniform Pierre-Simon Laplace independently rediscovered theory general form demonstrated broad INTRODUCTION tion Consider example polynomial curve fitting discussed Section reasonable apply frequentist notion probability random values observed variables like address quantify uncertainty surrounds appropriate choice model parameters shall Bayesian use machinery probability theory uncertainty model parameters choice model theorem acquires new Recall boxes fruit observation identity fruit provided relevant information altered probability chosen box red theorem used convert prior probability posterior probability incorporating evidence provided observed shall adopt similar approach making inferences quantities parameters polynomial curve fitting capture assumptions observing form prior probability distribution effect observed data expressed conditional probability shall Section represented takes form allows evaluate uncertainty observed form posterior probability quantity right-hand theorem evaluated observed data set viewed function parameter vector case called likelihood expresses probable observed data set different settings parameter vector Note likelihood probability distribution integral respect does equal definition state theorem words posterior prior quantities viewed functions denominator normalization ensures posterior distribution left-hand valid probability density integrates integrating sides respect express denominator theorem terms prior distribution likelihood function Bayesian frequentist likelihood function plays central manner used fundamentally different frequentist considered fixed value determined form error bars Probability Theory estimate obtained considering distribution possible data sets Bayesian viewpoint single data set actually uncertainty parameters expressed probability distribution widely used frequentist estimator maximum set value maximizes likelihood function corresponds choosing value probability observed data set machine learning negative log likelihood function called error negative logarithm monotonically decreasing maximizing likelihood equivalent minimizing approach determining frequentist error bars bootstrap Hastie et multiple data sets created Suppose original data set consists data points create new data set XB drawing points random points replicated points absent process repeated times generate data sets size obtained sampling original data set statistical accuracy parameter estimates evaluated looking variability predictions different bootstrap data advantage Bayesian viewpoint inclusion prior knowledge arises fair-looking coin tossed times lands heads classical maximum likelihood estimate probability landing heads implying future tosses landSection Bayesian approach reasonable prior lead extreme controversy debate associated relative merits frequentist Bayesian helped fact unique common criticism Bayesian approach prior distribution selected basis mathematical convenience reflection prior subjective nature conclusions dependence choice prior seen source Reducing dependence prior motivation so-called noninformative lead difficulties comparing different Bayesian methods based poor choices prior poor results high Frequentist evaluation methods offer protection techniques cross-validation remain useful areas modelSection book places strong emphasis Bayesian reflecting huge growth practical importance Bayesian methods past discussing useful frequentist concepts Bayesian framework origins 18th practical application Bayesian methods long time severely limited difficulties carrying Bayesian particularly need marginalize parameter shall INTRODUCTION required order make predictions compare different development sampling Markov chain Monte Carlo Chapter dramatic improvements speed memory capacity opened door practical use Bayesian techniques impressive range problem Monte Carlo methods flexible applied wide range computationally intensive mainly used small-scale highly efficient deterministic approximation schemes variational Bayes expectation propagation Chapter offer complementary alternative sampling methods allowed Bayesian techniques used large-scale applications et Gaussian distribution shall devote Chapter study various probability distributions key introduce important probability distributions continuous called normal Gaussian shall make extensive use distribution remainder chapter case single real-valued variable Gaussian distribution defined exp governed called called square root called standard reciprocal written called shall motivation terms Figure shows plot Gaussian form Gaussian distribution satisfies straightforward Gaussian thatExercise Pierre-Simon Laplace said Laplace seriously lacking modesty point declared best mathematician France claim arguably prolific numerous contributions including nebular hypothesis earth thought formed condensation cooling large rotating disk gas published edition Analytique des Laplace states theory common sense reduced work included discussion inverse probability calculation termed theorem used solve problems life planetary error Probability Theory Figure Plot univariate Gaussian showing mean standard deviation dx satisfies requirements valid probability readily expectations functions Gaussian average value byExercise parameter represents average value referred second order moment dx follows variance referred variance maximum distribution known mode coincides interested Gaussian distribution defined D-dimensional vector continuous exp D-dimensional vector called matrix called denotes determinant shall make use multivariate Gaussian distribution briefly properties studied Section INTRODUCTION Figure Illustration likelihood function Gaussian shown red black points denote data set values likelihood function corresponds product blue Maximizing likelihood involves adjusting mean variance Gaussian maximize xn suppose data set observations xN representing observations scalar variable Note typeface distinguish single observation vector-valued variable denote shall suppose observations drawn independently Gaussian distribution mean variance like determine parameters data Data points drawn independently distribution said independent identically abbreviated seen joint probability independent events product marginal probabilities event data set write probability data form viewed function likelihood function Gaussian interpreted diagrammatically Figure common criterion determining parameters probability distribution observed data set parameter values maximize likelihood like strange criterion foregoing discussion probability natural maximize probability parameters probability data criteria shall discuss context curve shall determine values unknown parameters Gaussian maximizing likelihood function convenient maximize log likelihood logarithm monotonically increasing function maximization log function equivalent maximization function Taking log simplifies subsequent mathematical helps numerically product large number small probabilities easily underflow numerical precision resolved computing instead sum log log likelihood Probability Theory function written form Maximizing respect obtain maximum likelihood solution byExercise xn sample mean observed values maximizing respect obtain maximum likelihood solution variance form sample variance measured respect sample mean Note performing joint maximization respect case Gaussian distribution solution decouples evaluate subsequently use result evaluate Later subsequent shall highlight significant limitations maximum likelihood indication problem context solutions maximum likelihood parameter settings univariate Gaussian shall maximum likelihood approach systematically underestimates variance example phenomenon called bias related problem over-fitting encountered context polynomial curve note maximum likelihood solutions functions data set values xN Consider expectations quantities respect data set come Gaussian distribution parameters straightforward thatExercise average maximum likelihood estimate obtain correct mean underestimate true variance factor intuition result Figure follows following estimate variance parameter unbiased ML INTRODUCTION Figure Illustration bias arises maximum likelihood determine variance green curve shows true Gaussian distribution data red curves Gaussian distributions obtained fitting data consisting data points shown maximum likelihood results Averaged data mean variance systematically under-estimated measured relative sample mean relative true Section shall result arises automatically adopt Bayesian Note bias maximum likelihood solution significant number data points limit maximum likelihood solution variance equals true variance distribution generated small bias prove book shall interested complex models bias problems associated maximum likelihood shall issue bias maximum likelihood lies root over-fitting problem encountered earlier context polynomial curve Curve fitting re-visited seen problem polynomial curve fitting expressed terms error return curve fitting example view itSection probabilistic gaining insights error functions taking Bayesian goal curve fitting problem able make predictions target variable new value input variable basis set training data comprising input values xN corresponding target values express uncertainty value target variable probability shall assume value corresponding value Gaussian distribution mean equal value polynomial curve consistency notation later defined precision parameter corresponding inverse variance illustrated schematically Figure Probability Theory Figure Schematic illustration Gaussian conditional distribution mean polynomial function precision parameter related variance xx0 use training data determine values unknown parameters maximum data assumed drawn independently distribution likelihood function did case simple Gaussian distribution convenient maximize logarithm likelihood Substituting form Gaussian obtain log likelihood function form Consider determination maximum likelihood solution polynomial denoted determined maximizing respect omit terms right-hand depend note scaling log likelihood positive constant coefficient does alter location maximum respect replace coefficient instead maximizing log equivalently minimize negative log maximizing likelihood far determining minimizing sum-of-squares error function defined sum-of-squares error function arisen consequence maximizing likelihood assumption Gaussian noise use maximum likelihood determine precision parameter Gaussian conditional Maximizing respect gives INTRODUCTION determine parameter vector wML governing mean subsequently use precision case simple Gaussian Having determined parameters make predictions new values probabilistic expressed terms predictive distribution gives probability distribution simply point obtained substituting maximum likelihood parameters let step Bayesian approach introduce prior distribution polynomial coefficients let consider Gaussian distribution form exp wTw precision total number elements vector th order Variables control distribution model called posterior distribution proportional product prior distribution likelihood function determine finding probable value words maximizing posterior technique called maximum simply Taking negative logarithm combining maximum posterior minimum maximizing posterior distribution equivalent minimizing regularized sum-of-squares error function encountered earlier form regularization parameter Bayesian curve fitting included prior distribution far making point estimate does Bayesian fully Bayesian consistently apply sum product rules shall integrate values marginalizations lie heart Bayesian methods pattern Probability Theory curve fitting training data new test point goal predict value wish evaluate predictive distribution shall assume parameters fixed known advance later chapters shall discuss parameters inferred data Bayesian Bayesian treatment simply corresponds consistent application sum product rules allow predictive distribution written form omitted dependence simplify posterior distribution normalizing right-hand shall Section problems curve-fitting posterior distribution Gaussian evaluated integration performed analytically result predictive distribution Gaussian form mean variance matrix unit defined vector elements xi predictive distribution dependent term represents uncertainty predicted value noise target variables expressed maximum likelihood predictive distribution second term arises uncertainty parameters consequence Bayesian predictive distribution synthetic sinusoidal regression problem illustrated Figure INTRODUCTION Figure predictive distribution resulting Bayesian treatment polynomial curve fitting fixed parameters known noise red curve denotes mean predictive distribution red region corresponds standard deviation Model Selection example polynomial curve fitting saw optimal order polynomial gave best order polynomial controls number free parameters model governs model regularized regularization coefficient controls effective complexity complex mixture distributions neural networks multiple parameters governing practical need determine values principal objective doing usually achieve best predictive performance new finding appropriate values complexity parameters wish consider range different types model order best particular seen maximum likelihood performance training set good indicator predictive performance unseen data problem data approach simply use available data train range model range values complexity compare independent called validation select having best predictive model design iterated times limited size data over-fitting validation data occur necessary aside test set performance selected model finally supply data training testing order build good wish use available data possible validation set relatively noisy estimate predictive solution dilemma use illustrated Figure allows proportion available data used training making use Curse Dimensionality Figure technique S-fold illustrated case involves taking available data partitioning groups simplest case equal groups used train set models evaluated remaining procedure repeated possible choices held-out indicated red performance scores runs run run run run data assess data particularly appropriate consider case total number data gives leave-one-out major drawback cross-validation number training runs performed increased factor prove problematic models training computationally problem techniques cross-validation use separate data assess performance multiple complexity parameters single model regularization Exploring combinations settings parameters worst require number training runs exponential number need better rely training data allow multiple hyperparameters model types compared single training need measure performance depends training data does suffer bias Historically various proposed attempt correct bias maximum likelihood addition penalty term compensate over-fitting complex Akaike information AIC chooses model quantity best-fit log number adjustable parameters variant called Bayesian information discussed Section criteria account uncertainty model practice tend favour overly simple turn Section fully Bayesian approach shall complexity penalties arise natural principled Curse Dimensionality polynomial curve fitting example just input variable practical applications pattern deal spaces INTRODUCTION Figure Scatter plot oil flow data input variables red denotes green denotes blue denotes goal classify new test point denoted high dimensionality comprising input poses challenges important factor influencing design pattern recognition order illustrate problem consider synthetically generated data set representing measurements taken pipeline containing mixture gas materials present different geometrical configurations known fractions materials data point comprises 12-dimensional input vector consisting measurements taken gamma ray densitometers measure attenuation gamma rays passing narrow beams data set described Appendix Figure shows points data set plot showing measurements remaining input values ignored purposes data point labelled according geometrical classes belongs goal use data training set order able classify new observation denoted cross Figure observe cross surrounded numerous red suppose belongs red plenty green points think instead belong green unlikely belongs blue intuition identity cross determined strongly nearby points training set strongly distant intuition turns reasonable discussed fully later turn intuition learning simple approach divide input space regular indicated Figure test point wish predict decide cell belongs training data points Curse Dimensionality Figure Illustration simple approach solution classification problem input space divided cells new test point assigned class majority number representatives cell test shall simplistic approach severe fall identity test point predicted class having largest number training points cell test point ties broken numerous problems naive severe apparent consider extension problems having larger numbers input corresponding input spaces higher origin problem illustrated Figure shows divide region space regular number cells grows exponentially dimensionality problem exponentially large number cells need exponentially large quantity training data order ensure cells hope applying technique space need sophisticated gain insight problems high-dimensional spaces returning example polynomial curve fitting considering wouldSection Figure Illustration curse showing number regions regular grid grows exponentially dimensionality subset cubical regions shown INTRODUCTION extend approach deal input spaces having input general polynomial coefficients order form wixi wijxixj number independent coefficients coefficients independent interchange symmetries grows proportionally capture complex dependencies need use higher-order polynomial order growth number coefficients like DM power law exponential points method rapidly unwieldy limited practical geometrical formed life spent space fail badly consider spaces higher simple consider sphere radius space ask fraction volume sphere lies radius evaluate fraction noting volume sphere radius dimensions scale write KDrD constant KD depends required fraction byExercise plotted function various values Figure large fraction tends small values spaces high volume sphere concentrated shell near direct relevance pattern consider behaviour Gaussian distribution high-dimensional transform Cartesian polar integrate directional obtain expression density function radius probability mass inside shell thickness located radius distribution various values Figure large probability mass Gaussian concentrated severe difficulty arise spaces dimensions called curse dimensionality shall make extensive use illustrative examples involving input spaces makes particularly easy illustrate techniques reader intuitions developed spaces low dimensionality generalize spaces Curse Dimensionality Figure Plot fraction volume sphere lying range various values dimensionality vo lu fra ct io curse dimensionality certainly raises important issues pattern recognition does prevent finding effective techniques applicable high-dimensional reasons real data confined region space having lower effective particular directions important variations target variables occur real data typically exhibit smoothness properties small changes input variables produce small changes target exploit local interpolation-like techniques allow make predictions target variables new values input Successful pattern recognition techniques exploit application manufacturing images captured identical planar objects conveyor goal determine image point Figure Plot probability density respect radius Gaussian distribution various values dimensionality high-dimensional probability mass Gaussian located shell specific INTRODUCTION high-dimensional space dimensionality determined number objects occur different positions image different degrees freedom variability set images live dimensional manifold embedded high-dimensional complex relationships object position orientation pixel manifold highly goal learn model input image output orientation object irrespective degree freedom variability manifold Decision Theory seen Section probability theory provides consistent mathematical framework quantifying manipulating turn discussion decision theory combined probability allows make optimal decisions situations involving uncertainty encountered pattern Suppose input vector corresponding vector target goal predict new value regression comprise continuous classification problems represent class joint probability distribution provides complete summary uncertainty associated Determination set training data example inference typically difficult problem solution forms subject practical make specific prediction value generally specific action based understanding values likely aspect subject decision medical diagnosis problem taken X-ray image wish determine patient cancer input vector set pixel intensities output variable represent presence denote class absence denote class choose binary variable corresponds class corresponds class shall later choice label values particularly convenient probabilistic general inference problem involves determining joint distribution equivalently gives complete probabilistic description useful informative end decide treatment patient like choice optimal appropriate sense decision subject decision theory tell make optimal decisions appropriate shall decision stage generally solved inference introduction key ideas decision theory required Decision Theory rest detailed Berger Bather giving detailed let consider informally expect probabilities play role making obtain X-ray image new goal decide classes assign interested probabilities classes probabilities expressed form Note quantities appearing theorem obtained joint distribution marginalizing conditioning respect appropriate interpret prior probability class corresponding posterior represents probability person X-ray corresponding revised theorem light information contained aim minimize chance assigning wrong intuitively choose class having higher posterior intuition discuss general criteria making Minimizing misclassification rate Suppose goal simply make misclassifications need rule assigns value available rule divide input space regions Rk called decision points Rk assigned class boundaries decision regions called decision boundaries decision Note decision region need contiguous comprise number disjoint shall encounter examples decision boundaries decision regions later order optimal decision consider case cancer problem mistake occurs input vector belonging class assigned class vice probability occurring dx free choose decision rule assigns point Clearly minimize arrange assigned whichever class smaller value integrand value assign class product rule probability factor common restate result saying minimum INTRODUCTION Figure Schematic illustration joint probabilities classes plotted decision boundary Values bx classified class belong decision region points bx classified belong Errors arise red bx errors points class misclassified sum red green conversely points region bx errors points class misclassified blue vary location bx decision combined areas blue green regions remains size red region optimal choice bx curves corresponding bx case red region equivalent minimum misclassification rate decision assigns value class having higher posterior probability probability making mistake obtained value assigned class posterior probability result illustrated single input variable Figure general case slightly easier maximize probability Rk dx maximized regions Rk chosen assigned class product rule noting factor common assigned class having largest posterior probability Decision Theory Figure example loss matrix elements cancer treatment rows correspond true columns correspond assignment class decision cancer normal cancer normal Minimizing expected loss objective complex simply minimizing number Let consider medical diagnosis note patient does cancer incorrectly diagnosed having consequences patient distress plus need patient cancer diagnosed result premature death lack consequences types mistake dramatically clearly better make fewer mistakes second expense making mistakes formalize issues introduction loss called cost overall measure loss incurred taking available decisions goal minimize total loss Note authors consider instead utility value aim equivalent concepts utility simply negative text shall use loss function Suppose new value true class Ck assign class equal incur level loss denote view element loss cancer loss matrix form shown Figure particular loss matrix says loss incurred correct decision loss healthy patient diagnosed having loss patient having cancer diagnosed optimal solution minimizes loss loss function depends true input vector uncertainty true class expressed joint probability distribution seek instead minimize average average computed respect Rj assigned independently decision regions Rj goal choose regions Rj order minimize expected loss implies minimize use product rule eliminate common factor decision rule minimizes expected loss assigns INTRODUCTION Figure Illustration reject Inputs larger posterior probabilities equal threshold reject region new class clearly trivial know posterior class probabilities reject option seen classification errors arise regions input space largest posterior probabilities significantly equivalently joint distributions comparable regions relatively uncertain class appropriate avoid making decisions difficult cases anticipation lower error rate examples classification decision known reject hypothetical medical appropriate use automatic classify X-ray images little doubt correct leaving human expert classify ambiguous achieve introducing threshold rejecting inputs largest posterior probabilities equal illustrated case single continuous input variable Figure Note setting ensure examples classes setting ensure examples fraction examples rejected controlled value easily extend reject criterion minimize expected loss matrix taking account loss incurred reject decision Inference decision broken classification problem separate inference stage use training data learn model Decision Theory subsequent decision stage use posterior probabilities make optimal class alternative possibility solve problems simply learn function maps inputs directly function called discriminant identify distinct approaches solving decision used practical decreasing order solve inference problem determining class-conditional densities class Ck separately infer prior class probabilities use theorem form posterior class probabilities denominator theorem terms quantities appearing model joint distribution directly normalize obtain posterior Having posterior use decision theory determine class membership new input Approaches explicitly implicitly model distribution inputs outputs known generative sampling possible generate synthetic data points input solve inference problem determining posterior class probabilities subsequently use decision theory assign new Approaches model posterior probabilities directly called discriminative function called discriminant maps input directly class case two-class binary valued represents class represents class probabilities play Let consider relative merits Approach demanding involves finding joint distribution high consequently need large training set order able determine class-conditional densities reasonable Note class priors estimated simply fractions training set data points advantage approach allows marginal density data determined useful detecting new data points low probability model predictions INTRODUCTION cl ns itie Figure Example class-conditional densities classes having single input variable corresponding posterior probabilities Note left-hand mode class-conditional density shown blue left effect posterior vertical green line right plot shows decision boundary gives minimum misclassification low known outlier detection novelty detection wish make classification wasteful computational excessively demanding joint distribution fact really need posterior probabilities obtained directly approach classconditional densities contain lot structure little effect posterior illustrated Figure exploring relative merits generative discriminative approaches machine finding ways combine Lasserre et simpler approach use training data discriminant function maps directly class combining inference decision stages single learning example Figure correspond finding value shown vertical green decision boundary giving minimum probability option longer access posterior probabilities powerful reasons wanting compute posterior subsequently use make Minimizing Consider problem elements loss matrix subjected revision time time occur financial Decision Theory know posterior trivially revise minimum risk decision criterion modifying discriminant change loss matrix require return training data solve classification problem Reject Posterior probabilities allow determine rejection criterion minimize misclassification generally expected fraction rejected data Compensating class Consider medical X-ray problem suppose collected large number X-ray images general population use training data order build automated screening cancer rare general examples corresponds presence used data set train adaptive run severe difficulties small proportion cancer classifier assigned point normal class achieve accuracy difficult avoid trivial large data set contain examples X-ray images corresponding learning algorithm exposed broad range examples images likely generalize balanced data set selected equal numbers examples classes allow accurate compensate effects modifications training Suppose used modified data set models posterior theorem posterior probabilities proportional prior interpret fractions points simply posterior probabilities obtained artificially balanced data set divide class fractions data set multiply class fractions population wish apply need normalize ensure new posterior probabilities sum Note procedure applied learned discriminant function directly instead determining posterior Combining complex wish break problem number smaller subproblems tackled separate hypothetical medical diagnosis information available blood tests X-ray combine heterogeneous information huge input effective build interpret Xray images different interpret blood long models gives posterior probabilities combine outputs systematically rules simple way assume class distributions inputs X-ray denoted blood denoted INTRODUCTION example conditional independence indepen-Section dence holds distribution conditioned class posterior X-ray blood need class prior probabilities easily estimate fractions data points need normalize resulting posterior probabilities sum particular conditional independence assumption example naive Bayes Note joint marginal distribution typically factorize shall later chapters construct models combining data require conditional independence assumption Loss functions regression discussed decision theory context classification turn case regression curve fitting example discussed decision stage consists choosing specific esti-Section mate value input Suppose doing incur loss loss common choice loss function regression problems squared loss expected loss written goal choose minimize assume completely flexible function formally calculus variations toAppendix dt Solving sum product rules obtain dt dt Decision Theory Figure regression function minimizes expected squared mean conditional distribution xx0 conditional average conditioned known regression result illustrated Figure readily extended multiple target variables represented vector case optimal solution conditional average derive result slightly different shed light nature regression Armed knowledge optimal solution conditional expand square term follows notation use denote Substituting loss function performing integral cross-term vanishes obtain expression loss function form dx function seek determine enters minimized equal case term simply result derived previously shows optimal squares predictor conditional second term variance distribution averaged represents intrinsic variability target data regarded independent represents irreducible minimum value loss classification determine appropriate probabilities use make optimal build models make decisions identify distinct approaches solving regression problems order decreasing solve inference problem determining joint density normalize conditional density finally marginalize conditional mean INTRODUCTION solve inference problem determining conditional density subsequently marginalize conditional mean regression function directly training relative merits approaches follow lines classification problems squared loss possible choice loss function situations squared loss lead poor results need develop sophisticated important example concerns situations conditional distribution arises solution inverse consider briefly simpleSection generalization squared called Minkowski expectation dxdt reduces expected squared loss function plotted various values Figure minimum conditional mean conditional median conditional mode Information Theory discussed variety concepts probability theory decision theory form foundations subsequent discussion close chapter introducing additional concepts field information prove useful development pattern recognition machine learning shall focus key refer reader detailed discussions Cover begin considering discrete random variable ask information received observe specific value information viewed learning value told highly improbable event just received information told likely event just knew event certain happen receive measure information content depend probability distribution look quantity monotonic function probability expresses information form noting events information gain observing sum information gained unrelated events statistically independent easily shown logarithm haveExercise Information Theory Figure Plots quantity Lq various values log2 negative sign ensures information positive Note low probability events correspond high information choice basis logarithm moment shall adopt convention prevalent information theory logarithms base shall units bits suppose sender wishes transmit value random variable average information transmit process obtained taking expectation respect distribution log2 important quantity called entropy random variable Note shall encounter value far heuristic motivation definition informa50 INTRODUCTION tion corresponding entropy definitions possess useful Consider random variable having possible equally order communicate value need transmit message length Notice entropy variable log2 consider example variable having possible states respective probabilities entropy case log2 log2 log2 log2 log2 nonuniform distribution smaller entropy uniform shall gain insight shortly discuss interpretation entropy terms let consider transmit identity state 3-bit advantage nonuniform distribution shorter codes probable expense longer codes probable hope getting shorter average code representing states following set code average length code transmitted average code length bits entropy random Note shorter code strings used possible disambiguate concatenation strings component decodes uniquely state sequence relation entropy shortest coding length general noiseless coding theorem states entropy lower bound number bits needed transmit state random shall switch use natural logarithms defining provide convenient link ideas entropy measured units instead differ simply factor introduced concept entropy terms average information needed specify state random concept entropy earlier origins physics introduced context equilibrium thermodynamics later deeper interpretation measure disorder developments statistical understand alternative view entropy considering set identical objects divided set ni objects ith Consider Information Theory number different ways allocating objects ways choose ways choose second leading total ways allocate objects denotes product wish distinguish rearrangements objects ith bin ways reordering total number ways allocating objects bins called entropy defined logarithm multiplicity scaled appropriate constant lnW lnN consider limit fractions held apply approximation lnN lnN gives lim pi pi used ni pi probability object assigned ith physics specific arrangements objects bins called overall distribution occupation expressed ratios called multiplicity known weight interpret bins states xi discrete random variable entropy random variable Distributions sharply peaked values relatively low spread evenly values higher illustrated Figure pi entropy equal minimum value pi pj maximum entropy configuration maximizing Lagrange multiplier enforce normalization constraintAppendix maximize INTRODUCTION ob ab ilit ob ab ilit Figure Histograms probability distributions bins illustrating higher value entropy broader largest entropy arise uniform distribution equal total number states corresponding value entropy lnM result derived inequality discussed verify stationary point canExercise evaluate second derivative gives pi Iij elements identity extend definition entropy include distributions continuous variables divide bins width assuming mean value theorem tells exist value xi dx quantize continuous variable assigning value value xi falls ith probability observing value xi gives discrete distribution entropy takes form used follows omit second term right-hand consider limit Information Theory term right-hand approach integral limit lim dx quantity right-hand called differential discrete continuous forms entropy differ quantity diverges limit reflects fact specify continuous variable precisely requires large number density defined multiple continuous denoted collectively vector differential entropy case discrete saw maximum entropy configuration corresponded equal distribution probabilities possible states Let consider maximum entropy configuration continuous order maximum necessary constrain second moments preserving normalization maximize differential entropy Ludwig Boltzmann Ludwig Eduard Boltzmann Austrian physicist created field statistical Prior concept entropy known classical thermodynamics quantifies fact energy energy typically available useful Boltzmann showed thermodynamic entropy macroscopic related statistical properties microscopic expressed famous equation represents number possible microstates units Joules known ideas disputed scientists difficulty saw arose second law states entropy closed tends increase microscopic level classical Newtonian equations physics difficult explain fully appreciate statistical nature concluded entropy decrease time simply overwhelming probability generally Boltzmann longrunning dispute editor leading German physics journal refused let refer atoms molecules convenient theoretical continued attacks work lead bouts eventually committed Shortly new experiments Perrin colloidal suspensions verified theories confirmed value Boltzmann equation carved INTRODUCTION constraints dx dx dx constrained maximization performed Lagrange multipliers thatAppendix maximize following functional respect dx calculus set derivative functional zero givingAppendix exp Lagrange multipliers substitution result constraint leading finally resultExercise exp distribution maximizes differential entropy Note did constrain distribution nonnegative maximized resulting distribution hindsight constraint evaluate differential entropy obtainExercise entropy increases distribution result shows differential unlike discrete Suppose joint distribution draw pairs values value additional information needed specify corresponding value average additional information needed specify written dy dx Information Theory called conditional entropy easily product conditional entropy satisfies relationExercise differential entropy differential entropy marginal distribution information needed sum information needed plus additional information required specify Relative entropy mutual information far introduced number concepts information including key notion start relate ideas pattern Consider unknown distribution suppose modelled approximating distribution use construct coding scheme purpose transmitting values average additional information required specify value choose efficient coding result instead true distribution dx known relative entropy Kullback-Leibler KL divergence distributions Note symmetrical say Kullback-Leibler divergence satisfies equality introduce concept convex function said convex property chord lies shown Figure value interval written form corresponding point chord Claude Shannon graduating Michigan Shannon joined Bell Telephone laboratories paper Mathematical Theory published Bell Technical Journal laid foundations modern information paper introduced word concept information sent stream 1s 0s paved way communications said von Neumann recommended Shannon use term similarity quantity used knows entropy really discussion INTRODUCTION Figure convex function chord lies function xa chord corresponding value function Convexity implies equivalent requirement second derivative function Examples convex functions lnx AExercise function called strictly convex equality satisfied function opposite chord lies called corresponding definition strictly function technique proof aExercise convex function satisfies set points result known interpret probability distribution discrete variable taking values written denotes continuous inequality takes form dx apply inequality form Kullback-Leibler divergence dx dx Information Theory used fact lnx convex normalization condition dx lnx strictly convex equality hold interpret Kullback-Leibler divergence measure dissimilarity distributions intimate relationship data compression density estimation problem modelling unknown probability efficient compression achieved know true use distribution different true necessarily efficient average additional information transmitted equal Kullback-Leibler divergence Suppose data generated unknown distribution wish try approximate distribution parametric distribution governed set adjustable parameters example multivariate way determine minimize Kullback-Leibler divergence respect directly know observed finite set training points drawn expectation respect approximated finite sum second term right-hand independent term negative log likelihood function distribution evaluated training minimizing Kullback-Leibler divergence equivalent maximizing likelihood consider joint distribution sets variables sets variables joint distribution factorize product marginals variables gain idea independent considering Kullback-Leibler divergence joint distribution product dxdy called mutual information variables properties Kullback-Leibler equality sum product rules mutual information related conditional entropy throughExercise INTRODUCTION view mutual information reduction uncertainty virtue told value vice Bayesian view prior distribution posterior distribution observed new data mutual information represents reduction uncertainty consequence new observation Exercises www Consider sum-of-squares error function function polynomial coefficients minimize error function solution following set linear equations Aijwj Ti Aij Ti suffix denotes index denotes raised power Write set coupled linear analogous satisfied coefficients wi minimize regularized sum-of-squares error function Suppose coloured boxes Box contains box contains box contains box chosen random probabilities piece fruit removed box equal probability selecting items probability selecting observe selected fruit fact probability came green www Consider probability density defined continuous variable suppose make nonlinear change variable density transforms according differentiating location maximum density general related location maximum density simple functional relation consequence Jacobian shows maximum probability density contrast simple dependent choice Verify case linear location maximum transforms way variable definition satisfies Exercises variables covariance www prove normalization condition univariate integral exp dx evaluate writing square form exp make transformation Cartesian coordinates polar coordinates substitute performing integrals taking square root obtain use result Gaussian distribution www change verify univariate Gaussian distribution satisfies differentiating sides normalization condition dx respect verify Gaussian satisfies www mode Gaussian distribution mode multivariate Gaussian www Suppose variables statistically mean variance sum satisfies setting derivatives log likelihood function respect equal verify results INTRODUCTION www results xn xm denote data points sampled Gaussian distribution mean variance Inm satisfies Inm Inm prove results Suppose variance Gaussian estimated result maximum likelihood estimate replaced true value estimator property expectation true variance arbitrary square matrix elements wij written form wij wSij wAij wSij wAij symmetric anti-symmetric satisfying wSij wSji wAij consider second order term higher order polynomial wijxixj wijxixj wSijxixj contribution anti-symmetric matrix loss matrix coefficients wij chosen elements matrix chosen number independent parameters matrix wSij www exercise explore number independent parameters polynomial grows order polynomial dimensionality input start writing th order term polynomial dimensions form xi1xi2 coefficients comprise DM number independent parameters significantly fewer interchange symmetries factor xi1xi2 Begin showing redundancy coefficients removed rewriting th order term form xi1xi2 Exercises Note precise relationship coefficients coefficients need Use result number independent parameters appear order satisfies following recursion relation use proof induction following result holds proving result arbitrary making use result assuming correct dimension verifying correct dimension use previous proof result true value comparison result Exercise make use result holds order hold order Exercise proved result number independent parameters th order term D-dimensional expression total number independent parameters terms including M6th satisfies number independent parameters term order make use result proof proving result holds arbitrary assuming holds order showing holds order make use approximation form large quantity grows like DM grows like Consider cubic polynomial evaluate numerically total number independent parameters correspond typical small-scale medium-scale machine learning INTRODUCTION www gamma function defined integration prove relation www use result derive expression surface area volume sphere unit radius consider following obtained transforming Cartesian polar coordinates dxi SD definition Gamma evaluate sides SD integrating respect radius volume unit sphere dimensions VD SD use results reduce usual expressions Consider sphere radius D-dimensions concentric hypercube sphere touches hypercube centres results Exercise ratio volume sphere volume cube volume sphere volume cube make use formula form valid ratio goes ratio distance centre hypercube divided perpendicular distance goes results space high volume cube concentrated large number long Exercises www explore behaviour Gaussian distribution high-dimensional Consider Gaussian distribution dimensions exp wish density respect radius polar coordinates direction variables integrated integral probability density shell radius thickness SDr exp SD surface area unit sphere function single stationary point large considering large exp shows maximum radial probability density decays exponentially away maximum length scale seen large probability mass concentrated shell large probability density larger origin radius factor probability mass high-dimensional Gaussian distribution located different radius region high probability property distributions spaces high dimensionality important consequences consider Bayesian inference model parameters later Consider nonnegative numbers Use result decision regions two-class classification problem chosen minimize probability probability satisfy www loss matrix elements expected risk minimized choose class minimizes Verify loss matrix Ikj Ikj elements identity reduces criterion choosing class having largest posterior interpretation form loss Derive criterion minimizing expected loss general loss matrix general prior probabilities INTRODUCTION www Consider classification problem loss incurred input vector class Ck classified belonging class loss matrix loss incurred selecting reject option decision criterion minimum expected Verify reduces reject criterion discussed Section loss matrix Ikj relationship rejection threshold www Consider generalization squared loss function single target variable case multiple target variables described vector calculus function expected loss minimized result reduces case single target variable expansion square derive result analogous function minimizes expected squared loss case vector target variables conditional expectation www Consider expected loss regression problems Lq loss function Write condition satisfy order minimize solution represents conditional function probability mass minimum expected Lq loss conditional function equal value maximizes Section introduced idea entropy information gained observing value random variable having distribution saw independent variables entropy functions derive relation form function induction positive positive implies positive rational continuity positive real implies form www Consider -state discrete random variable use inequality form entropy distribution satisfies lnM Evaluate Kullback-Leibler divergence Gaussians Exercises Table joint distribution binary variables used Exercise www Consider variables having joint distribution differential entropy pair variables satisfies equality statistically Consider vector continuous variables distribution corresponding entropy Suppose make nonsingular linear transformation obtain new variable corresponding entropy denotes determinant Suppose conditional entropy discrete random variables values variable function words value www Use calculus variations stationary point functional use constraints eliminate Lagrange multipliers maximum entropy solution Gaussian www Use results entropy univariate Gaussian strictly convex function defined chord lies equivalent condition second derivative function definition product rule prove result www proof inequality convex functions implies result Consider binary variables having joint distribution Table Evaluate following quantities Draw diagram relationship various INTRODUCTION applying inequality arithmetic mean set real numbers geometrical www sum product rules mutual information satisfies relation 
Probability Distributions Chapter emphasized central role played probability theory solution pattern recognition turn exploration particular examples probability distributions great distributions form building blocks complex models used extensively distributions introduced chapter serve important provide opportunity discuss key statistical Bayesian context simple models encounter complex situations later role distributions discussed chapter model probability distribution random variable finite set problem known density purposes shall assume data points independent identically emphasized problem density estimation fun67 PROBABILITY DISTRIBUTIONS damentally infinitely probability distributions rise observed finite data distribution nonzero data points potential issue choosing appropriate distribution relates problem model selection encountered context polynomial curve fitting Chapter central issue pattern begin considering binomial multinomial distributions discrete random variables Gaussian distribution continuous random specific examples parametric so-called governed small number adaptive mean variance case Gaussian apply models problem density need procedure determining suitable values observed data frequentist choose specific values parameters optimizing likelihood Bayesian treatment introduce prior distributions parameters use theorem compute corresponding posterior distribution observed shall important role played conjugate lead posterior distributions having functional form lead greatly simplified Bayesian conjugate prior parameters multinomial distribution called Dirichlet conjugate prior mean Gaussian distributions examples exponential family possess number important discussed limitation parametric approach assumes specific functional form turn inappropriate particular alternative approach nonparametric density estimation methods form distribution typically depends size data models contain control model complexity form end chapter considering nonparametric methods based respectively Binary Variables begin considering single binary random variable outcome flipping representing representing imagine damaged coin probability landing heads necessarily landing probability denoted parameter Binary Variables follows probability distribution written form known Bernoulli easily verified distributionExercise normalized mean variance suppose data set observed values construct likelihood function assumption observations drawn independently frequentist estimate value maximizing likelihood equivalently maximizing logarithm case Bernoulli log likelihood function worth noting log likelihood function depends observations xn sum sum provides example sufficient statistic data shall study important role sufficient statistics set derivative respect equal obtain maximum likelihood estimator xn Jacob Bernoulli Jacob known Jacques James Swiss mathematician Bernoulli family pursue career science compelled study philosophy theology travelled extensively graduating order meet leading scientists including Boyle Hooke returned taught mechanics Professor Mathematics Basel rivalry Jacob younger brother Johann turned initially productive collaboration bitter public significant contributions mathematics appeared Art Conjecture published years deals topics probability theory including known Bernoulli PROBABILITY DISTRIBUTIONS Figure Histogram plot binomial distribution function known sample denote number observations data set write form probability landing heads maximum likelihood fraction observations heads data suppose flip times happen observe maximum likelihood result predict future observations Common sense tells fact extreme example over-fitting associated maximum shall shortly arrive sensible conclusions introduction prior distribution work distribution number observations data set size called binomial proportional order obtain normalization coefficient note coin add possible ways obtaining binomial distribution written number ways choosing objects total identical Figure shows plot binomial distribution mean variance binomial distribution result Exercise shows independent events mean sum sum variance sum sum xN observation mean variance Binary Variables results proved directly beta distribution seen maximum likelihood setting parameter Bernoulli binomial fraction observations data set having severely over-fitted results small data order develop Bayesian treatment need introduce prior distribution parameter consider form prior distribution simple interpretation useful analytical motivate note likelihood function takes form product factors form choose prior proportional powers posterior proportional product prior likelihood functional form property called conjugacy examples later choose called beta gamma function defined coefficient ensures beta distribution thatExercise mean variance beta distribution byExercise ab parameters called hyperparameters control distribution parameter Figure shows plots beta distribution various values posterior distribution obtained multiplying beta prior binomial likelihood function Keeping factors depend posterior distribution form PROBABILITY DISTRIBUTIONS Figure Plots beta distribution function various values hyperparameters corresponds number coin functional dependence prior reflecting conjugacy properties prior respect likelihood simply beta normalization coefficient obtained comparison effect observing data set observations observations increase value value going prior distribution posterior allows provide simple interpretation hyperparameters prior effective number observations Note need posterior distribution act prior subsequently observe additional imagine taking observations time observation updating current posterior Binary Variables prior likelihood function posterior Figure Illustration step sequential Bayesian prior beta distribution parameters likelihood corresponds single observation posterior beta distribution parameters distribution multiplying likelihood function new observation normalizing obtain revised posterior posterior beta distribution total number observed values parameters Incorporation additional observation simply corresponds incrementing value observation increment Figure illustrates step sequential approach learning arises naturally adopt Bayesian independent choice prior likelihood function depends assumption Sequential methods make use observations small discard observations real-time learning scenarios steady stream data predictions data require data set stored loaded sequential methods useful large data Maximum likelihood methods cast sequential goal best outcome evaluate predictive distribution observed data set sum product rules takes form result posterior distribution result mean beta obtain simple interpretation total fraction observations real observations fictitious prior correspond Note limit infinitely large data set result reduces maximum likelihood result shall general property Bayesian maximum likelihood results agree limit infinitely PROBABILITY DISTRIBUTIONS large data finite data posterior mean lies prior mean maximum likelihood estimate corresponding relative frequencies events Figure number observations posterior distribution sharply seen result variance beta variance goes zero wonder general property Bayesian learning observe uncertainty represented posterior distribution steadily address frequentist view Bayesian learning property does Consider general Bayesian inference problem parameter observed data set described joint distribution following resultExercise ED dD says posterior mean averaged distribution generating equal prior mean ED varD term left-hand prior variance righthand term average posterior variance second term measures variance posterior mean variance positive result shows posterior variance smaller prior reduction variance greater variance posterior mean result holds particular observed data set possible posterior variance larger prior Multinomial Variables Binary variables used quantities possible encounter discrete variables possible mutually exclusive various alternative ways express shall shortly particularly convenient representation 1-of-K scheme variable represented K-dimensional vector elements xk equals remaining elements equal Multinomial Variables instance variable states particular observation variable happens correspond state represented Note vectors satisfy xk denote probability xk parameter distribution parameters constrained satisfy represent distribution regarded generalization Bernoulli distribution easily seen distribution normalized consider data set independent observations corresponding likelihood function takes form likelihood function depends data points quantities mk xnk represent number observations xk called sufficient statistics order maximum likelihood solution need maximize respect taking account constraint sum achieved Lagrange multiplier maximizingAppendix mk Setting derivative respect obtain PROBABILITY DISTRIBUTIONS solve Lagrange multiplier substituting obtain maximum likelihood solution form mk fraction observations xk consider joint distribution quantities mK conditioned parameters total number takes form mK m1m2 mK known multinomial normalization coefficient number ways partitioning objects groups size mK m1m2 mK mK Note variables mk subject constraint mk Dirichlet distribution introduce family prior distributions parameters multinomial distribution inspection form multinomial conjugate prior parameters denotes Note summation distribution space confined simplex dimensionality illustrated Figure normalized form distribution byExercise called Dirichlet gamma function defined Multinomial Variables Figure Dirichlet distribution variables confined simplex bounded linear form consequence constraints Plots Dirichlet distribution various settings parameters shown Figure Multiplying prior likelihood function obtain posterior distribution parameters form posterior distribution takes form Dirichlet confirming Dirichlet conjugate prior allows determine normalization coefficient comparison denoted case binomial distribution beta interpret parameters Dirichlet prior effective number observations xk Note two-state quantities represented binary variables Lejeune Dirichlet Johann Peter Gustav Lejeune Dirichlet modest reserved mathematician contributions number gave rigorous analysis Fourier family originated Richelet Lejeune Dirichlet comes jeune young person published brought instant concerned claims positive integer solutions xn yn Dirichlet gave partial proof case sent Legendre review turn completed Dirichlet gave complete proof proof theorem arbitrary wait work Andrew Wiles closing years 20th PROBABILITY DISTRIBUTIONS Figure Plots Dirichlet distribution horizontal axes coordinates plane simplex vertical axis corresponds value left centre right modelled binomial distribution 1-of-2 variables modelled multinomial distribution Gaussian Distribution known normal widely used model distribution continuous case single variable Gaussian distribution written form exp mean D-dimensional vector multivariate Gaussian distribution takes form exp D-dimensional mean covariance denotes determinant Gaussian distribution arises different contexts motivated variety different seen forSection single real distribution maximizes entropy property applies multivariate situation Gaussian distribution arises consider sum multiple random central limit theorem tells subject certain mild sum set random course random distribution increasingly Gaussian number terms sum increases Gaussian Distribution Figure Histogram plots mean uniformly distributed numbers various values observe distribution tends illustrate considering variables xN uniform distribution interval considering distribution mean xN large distribution tends illustrated Figure convergence Gaussian increases consequence result binomial distribution distribution defined sum observations random binary variable tend Gaussian Figure case Gaussian distribution important analytical shall consider section technically involved earlier require familiarity various matrix strongly encourage reader pro-Appendix ficient manipulating Gaussian distributions techniques presented prove invaluable understanding complex models presented later begin considering geometrical form Gaussian Carl Friedrich Gauss said Gauss went elementary school age teacher trying class asked pupils sum integers Gauss arrived answer matter moments noting sum represented pairs added giving answer believed problem actually set form somewhat harder sequence larger starting value larger Gauss German mathematician scientist reputation hard-working contributions squares derived assumption normally distributed created early formulation non-Euclidean geometry self-consistent geometrical theory violates axioms reluctant discuss openly fear reputation suffer seen believed Gauss asked conduct geodetic survey state led formulation normal known study diaries revealed discovered important mathematical results years decades published PROBABILITY DISTRIBUTIONS functional dependence Gaussian quadratic form appears quantity called Mahalanobis distance reduces Euclidean distance identity Gaussian distribution constant surfaces x-space quadratic form note matrix taken loss antisymmetric component disappear consider eigenvector equation covariance matrixExercise symmetric matrix eigenvalues eigenvectors chosen form orthonormal thatExercise uTi uj Iij Iij element identity matrix satisfies Iij covariance matrix expressed expansion terms eigenvectors formExercise similarly inverse covariance matrix expressed uiuTi Substituting quadratic form y2i defined yi uTi interpret new coordinate defined orthonormal vectors ui shifted rotated respect original xi Forming vector Gaussian Distribution Figure red curve shows elliptical surface constant probability density Gaussian two-dimensional space density value major axes ellipse defined eigenvectors ui covariance corresponding eigenvalues matrix rows uTi follows orthogonal satisfies UUT UTU IAppendix identity quadratic Gaussian constant surfaces eigenvalues surfaces represent centres axes oriented scaling factors directions axes illustrated Figure Gaussian distribution necessary eigenvalues covariance matrix strictly distribution properly matrix eigenvalues strictly positive said positive Chapter encounter Gaussian distributions eigenvalues case distribution singular confined subspace lower eigenvalues covariance matrix said positive consider form Gaussian distribution new coordinate defined going coordinate Jacobian matrix elements Jij Uji Uji elements matrix orthonormality property matrix square determinant Jacobian matrix determinant covariance matrix written PROBABILITY DISTRIBUTIONS product yj coordinate Gaussian distribution takes form exp product independent univariate Gaussian eigenvectors define new set shifted rotated coordinates respect joint probability distribution factorizes product independent integral distribution coordinate dy exp dyj used result normalization univariate confirms multivariate Gaussian look moments Gaussian distribution provide interpretation parameters expectation Gaussian distribution exp xdx exp dz changed variables note exponent function components integrals taken range term factor vanish refer mean Gaussian consider second order moments univariate considered second order moment multivariate second order moments group form matrix matrix written exp xxT dx exp dz Gaussian Distribution changed variables Note cross-terms involving vanish term constant taken outside unity Gaussian distribution Consider term involving make use eigenvector expansion covariance matrix completeness set write yjuj yj uTj gives exp zzT dz uiuTj exp y2k yiyj dy uiuTi use eigenvector equation fact integral right-hand middle line vanishes symmetry unless final line use results single random subtracted mean taking second moments order define multivariate case convenient subtract giving rise covariance random vector defined specific case Gaussian make use result parameter matrix governs covariance Gaussian called covariance Gaussian distribution widely used density suffers significant Consider number free parameters general symmetric covariance matrix independent independent parameters giv-Exercise ing parameters large total number parameters PROBABILITY DISTRIBUTIONS Figure Contours constant probability density Gaussian distribution dimensions covariance matrix general elliptical contours aligned coordinate proportional identity contours concentric grows quadratically computational task manipulating inverting large matrices way address problem use restricted forms covariance consider covariance matrices total 2D independent parameters density corresponding contours constant density axis-aligned restrict covariance matrix proportional identity known isotropic giving independent parameters model spherical surfaces constant possibilities isotropic covariance matrices illustrated Figure approaches limit number degrees freedom distribution make inversion covariance matrix faster greatly restrict form probability density limit ability capture interesting correlations limitation Gaussian distribution intrinsically unimodal single unable provide good approximation multimodal Gaussian distribution sense having limited range distributions adequately later introduction latent called hidden variables unobserved allows problems rich family multimodal distributions obtained introducing discrete latent variables leading mixtures discussed Section introduction continuous latent described Chapter leads models number free parameters controlled independently dimensionality data space allowing model capture dominant correlations data approaches combined extended derive rich set hierarchical models adapted broad range practical Gaussian version Markov random widely used probabilistic model Gaussian distribution joint space pixel intensities rendered tractable imposition considerable structure reflecting spatial organization linear dynamical used model time series data applications suchSection joint Gaussian distribution potentially large number observed latent variables tractable structure imposed powerful framework expressing form properties Gaussian Distribution complex distributions probabilistic graphical form subject Chapter Conditional Gaussian distributions important property multivariate Gaussian distribution sets variables jointly conditional distribution set conditioned marginal distribution set Consider case conditional Suppose D-dimensional vector Gaussian distribution partition disjoint subsets xa loss xa form components xb comprising remaining xa xb define corresponding partitions mean vector covariance matrix Note symmetry covariance matrix implies convenient work inverse covariance matrix known precision shall properties Gaussian distributions naturally expressed terms simpler form viewed terms introduce partitioned form precision matrix corresponding partitioning vector inverse symmetric matrix whileExercise stressed point simply inverse shall shortly examine relation inverse partitioned matrix inverses Let begin finding expression conditional distribution product rule conditional distribution PROBABILITY DISTRIBUTIONS evaluated joint distribution simply fixing xb observed value normalizing resulting expression obtain valid probability distribution Instead performing normalization obtain solution efficiently considering quadratic form exponent Gaussian distribution reinstating normalization coefficient end make use partitioning obtain function quadratic corresponding conditional distribution distribution completely characterized mean goal identify expressions mean covariance inspection example common operation associated Gaussian called quadratic form defining exponent terms Gaussian need determine corresponding mean problems solved straightforwardly noting exponent general Gaussian distribution written const denotes terms independent use symmetry general quadratic form express form right-hand immediately equate matrix coefficients entering second order term inverse covariance matrix coefficient linear term obtain let apply procedure conditional Gaussian distribution quadratic form exponent denote mean covariance distribution Consider functional dependence xa xb regarded pick terms second order xTa immediately conclude covariance aa Gaussian Distribution consider terms linear xa xTa used discussion general form coefficient xa expression equal use results expressed terms partitioned precision matrix original joint distribution express results terms corresponding partitioned covariance make use following identity inverse partitioned matrixExercise defined quantity known Schur complement matrix left-hand respect submatrix making use obtain following expressions mean covariance conditional distribution bb Comparing conditional distribution takes simpler form expressed terms partitioned precision matrix expressed terms partitioned covariance Note mean conditional distribution linear function xb independent represents example linear-Gaussian PROBABILITY DISTRIBUTIONS Marginal Gaussian distributions seen joint distribution conditional distribution turn discussion marginal distribution dxb shall strategy evaluating distribution efficiently focus quadratic form exponent joint distribution identify mean covariance marginal distribution quadratic form joint distribution partitioned precision form goal integrate easily achieved considering terms involving xb completing square order facilitate Picking just terms involve xTb defined dependence xb cast standard quadratic form Gaussian distribution corresponding term right-hand plus term does depend xb does depend exponential quadratic integration xb required exp integration easily performed noting integral unnormalized result reciprocal normalization know form normalized Gaussian coefficient independent mean depends determinant covariance completing square respect integrate xb term remaining contributions left-hand depends xa term right-hand Combining term remaining terms Gaussian Distribution depend obtain xTa const xTa const denotes quantities independent comparison covariance marginal distribution mean used covariance expressed terms partitioned precision matrix rewrite terms corresponding partitioning covariance matrix did conditional partitioned matrices related Making use obtain intuitively satisfying result marginal distribution mean covariance marginal mean covariance simply expressed terms partitioned covariance contrast conditional distribution partitioned precision matrix gives rise simpler results marginal conditional distributions partitioned Gaussian summarized Partitioned Gaussians joint Gaussian distribution xa xb PROBABILITY DISTRIBUTIONS xa xb xb xa Figure plot left shows contours Gaussian distribution plot right shows marginal distribution conditional distribution xb Conditional Marginal illustrate idea conditional marginal distributions associated multivariate Gaussian example involving variables Figure theorem Gaussian variables Sections considered Gaussian partitioned vector subvectors expressions conditional distribution marginal distribution noted mean conditional distribution linear function shall suppose Gaussian marginal distribution Gaussian conditional distribution mean linear function covariance independent example Gaussian Distribution linear Gaussian model shall study greater generality Section wish marginal distribution conditional distribution problem arise frequently subsequent prove convenient derive general results shall marginal conditional distributions parameters governing precision dimensionality dimensionality matrix size expression joint distribution define consider log joint distribution const denotes terms independent quadratic function components Gaussian precision consider second order terms written yTLy yTLAx xTATLy ATLA zTRz Gaussian distribution precision matrix ATLA covariance matrix taking inverse matrix inversion formula giveExercise PROBABILITY DISTRIBUTIONS mean Gaussian distribution identifying linear terms xTATLb yTLb Lb earlier result obtained completing square quadratic form multivariate mean Lb Making use obtainExercise expression marginal distribution marginalized Recall marginal distribution subset components Gaussian random vector takes particularly simple form expressed terms partitioned covariance mean andSection covariance Making use mean covariance marginal distribution special case result case reduces convolution mean convolution sum mean covariance convolution sum seek expression conditional Recall results conditional distribution easily expressed terms partitioned precision Applying results andSection conditional distribution mean covariance evaluation conditional seen example interpret distribution prior distribution variable conditional distribution represents corresponding posterior distribution Having marginal conditional effectively expressed joint distribution form results summarized Gaussian Distribution Marginal Conditional Gaussians marginal Gaussian distribution conditional Gaussian distribution form marginal distribution conditional distribution Maximum likelihood Gaussian data set observations assumed drawn independently multivariate Gaussian estimate parameters distribution maximum log likelihood function simple likelihood function depends data set quantities xnxTn known sufficient statistics Gaussian derivative log likelihood respect byAppendix setting derivative obtain solution maximum likelihood estimate mean xn PROBABILITY DISTRIBUTIONS mean observed set data maximization respect simplest approach ignore symmetry constraint resulting solution symmetric Alternative derivations impose symmetry positive definiteness constraints Magnus Neudecker result expected takes form involves result joint maximization respect Note solution does depend evaluate use evaluate evaluate expectations maximum likelihood solutions true obtain following resultsExercise expectation maximum likelihood estimate mean equal true maximum likelihood estimate covariance expectation true correct bias defining different estimator Clearly expectation equal Sequential estimation discussion maximum likelihood solution parameters Gaussian distribution provides convenient opportunity general discussion topic sequential estimation maximum Sequential methods allow data points processed time discarded important on-line large data sets involved batch processing data points Consider result maximum likelihood estimator mean denote ML based Gaussian Distribution Figure schematic illustration correlated random variables regression function conditional expectation RobbinsMonro algorithm provides general sequential procedure finding root dissect contribution final data point xN obtain ML xn xN xn xN ML result nice observing data points estimated observe data point xN obtain revised estimate moving old estimate small proportional direction Note contribution successive data points gets result clearly answer batch result formulae able derive sequential algorithm seek general formulation sequential leads Robbins-Monro Consider pair random variables governed joint distribution conditional expectation defines deterministic function dz illustrated schematically Figure Functions defined way called regression goal root large data set observations model regression function directly obtain estimate observe values time wish corresponding sequential estimation scheme following general procedure solving problems PROBABILITY DISTRIBUTIONS Robbins Monro shall assume conditional variance finite shall loss consider case case Figure Robbins-Monro procedure defines sequence successive estimates root observed value takes value coefficients represent sequence positive numbers satisfy conditions lim a2N shown sequence estimates does converge root probability Note condition ensures successive corrections decrease magnitude process converge limiting second condition required ensure algorithm does converge short condition needed ensure accumulated noise finite variance does spoil let consider general maximum likelihood problem solved sequentially Robbins-Monro maximum likelihood solution stationary point log likelihood function satisfies Exchanging derivative taking limit lim finding maximum likelihood solution corresponds finding root regression apply Robbins-Monro takes form Gaussian Distribution Figure case Gaussian corresponding mean regression function illustrated Figure takes form straight shown random variable corresponds derivative log likelihood function expectation defines regression function straight line root regression function corresponds maximum likelihood estimator specific consider sequential estimation mean Gaussian case parameter estimate ML mean random variable distribution Gaussian mean illustrated Figure Substituting obtain univariate form provided choose coefficients form Note focussed case single restrictions coefficients apply equally multivariate case Bayesian inference Gaussian maximum likelihood framework gave point estimates parameters develop Bayesian treatment introducing prior distributions Let begin simple example consider single Gaussian random variable shall suppose variance consider task inferring mean set observations likelihood probability observed data viewed function exp emphasize likelihood function probability distribution likelihood function takes form exponential quadratic form choose prior PROBABILITY DISTRIBUTIONS conjugate distribution likelihood function corresponding posterior product exponentials quadratic functions prior distribution posterior distribution Simple manipulation involving completing square exponent shows theExercise posterior distribution maximum likelihood solution sample mean worth spending moment studying form posterior mean note mean posterior distribution compromise prior mean maximum likelihood solution number observed data points reduces prior mean posterior mean maximum likelihood consider result variance posterior naturally expressed terms inverse called precisions precision posterior precision prior plus contribution data precision observed data increase number observed data precision steadily corresponding posterior distribution steadily decreasing observed data prior number data points variance goes zero posterior distribution infinitely peaked maximum likelihood maximum likelihood result point estimate recovered precisely Bayesian formalism limit infinite number Note finite limit prior infinite variance posterior mean reduces maximum likelihood posterior variance Gaussian Distribution Figure Illustration Bayesian inference mean Gaussian variance assumed curves prior distribution curve labelled case posterior distribution increasing numbers data data points generated Gaussian mean variance prior chosen mean prior likelihood variance set true illustrate analysis Bayesian inference mean Gaussian distribution Figure generalization result case Ddimensional Gaussian random variable known covariance unknown mean seen maximum likelihood expression mean Gaussian re-cast sequential update formula mean afterSection observing data points expressed terms mean observing data points contribution data point xN Bayesian paradigm leads naturally sequential view inference context inference mean write posterior distribution contribution final data point xN separated term square brackets normalization just posterior distribution observing data viewed prior combined theorem likelihood function associated data point xN arrive posterior distribution observing data sequential view Bayesian inference general applies problem observed data assumed independent identically assumed variance Gaussian distribution data known goal infer let suppose mean known wish infer calculations greatly simplified choose conjugate form prior turns convenient work precision likelihood function takes form exp PROBABILITY DISTRIBUTIONS Figure Plot gamma distribution defined various values parameters corresponding conjugate prior proportional product power exponential linear function corresponds gamma distribution defined gamma function defined ensures correctly gamma distribution finite integral distribution finite various values Figure mean variance gamma distribution byExercise Consider prior distribution multiply likelihood function obtain posterior distribution exp recognize gamma distribution form bN bN ML maximum likelihood estimator Note need track normalization constants prior likelihood function correct coefficient end normalized form gamma Gaussian Distribution effect observing data points increase value coefficient interpret parameter prior terms 2a0 prior data points contribute parameter interpret parameter prior arising 2a0 prior observations having variance Recall analogous interpretation Dirichlet distributionsSection examples exponential shall interpretation conjugate prior terms effective fictitious data points general exponential family Instead working consider variance conjugate prior case called inverse gamma shall discuss convenient work suppose mean precision conjugate consider dependence likelihood function exp exp exp xn x2n wish identify prior distribution functional dependence likelihood function form exp exp exp exp write Gaussian precision linear function gamma normalized prior takes form defined new constants distribution called normal-gamma Gaussian-gamma distribution plotted Figure Note simply product independent Gaussian prior gamma prior precision linear function chose prior posterior distribution exhibit coupling precision value PROBABILITY DISTRIBUTIONS Figure Contour plot normal-gamma distribution parameter values case multivariate Gaussian distribution Ddimensional variable conjugate prior distribution mean assuming precision known mean unknown precision matrix conjugate prior Wishart distribution byExercise exp called number degrees freedom scale denotes normalization constant possible define conjugate prior covariance matrix precision leads inverse Wishart shall discuss mean precision following similar line reasoning univariate conjugate prior known normal-Wishart Gaussian-Wishart t-distribution seen conjugate prior precision Gaussian gamma univariate Gaussian togetherSection Gamma prior integrate obtain marginal distribution formExercise Gaussian Distribution Figure Plot t-distribution various values limit corresponds Gaussian distribution mean precision exp ba change variable convention define new parameters 2a terms distribution takes form known parameter called precision general equal inverse parameter called degrees effect illustrated Figure particular case t-distribution reduces Cauchy limit t-distribution Gaussian mean precision t-distribution obtained adding infinite number Gaussian distributions having mean different interpreted infinite mixture Gaussians mixtures discussed Section result distribution general longer seen Figure gives tdistribution important property called means sensitive Gaussian presence data points robustness t-distribution illustrated Figure compares maximum likelihood solutions Gaussian Note maximum likelihood solution t-distribution expectationmaximization effect small number ofExercise PROBABILITY DISTRIBUTIONS Figure Illustration robustness t-distribution compared Histogram distribution data points drawn Gaussian maximum likelihood fit obtained t-distribution Gaussian largely hidden red t-distribution contains Gaussian special case gives solution data set additional outlying data points showing Gaussian strongly distorted t-distribution relatively outliers significant t-distribution Outliers arise practical applications process generates data corresponds distribution having heavy tail simply mislabelled Robustness important property regression squares approach regression does exhibit corresponds maximum likelihood Gaussian basing regression model heavy-tailed distribution obtain robust substitute alternative parameters t-distribution written form generalize multivariate Gaussian obtain corresponding multivariate t-distribution form technique univariate evaluate integral giveExercise Gaussian Distribution dimensionality squared Mahalanobis distance defined multivariate form t-distribution satisfies following propertiesExercise corresponding results univariate Periodic variables Gaussian distributions great practical right building blocks complex probabilistic situations inappropriate density models continuous important arises practical periodic example periodic variable wind direction particular geographical measure values wind direction number days wish summarize parametric example calendar interested modelling quantities believed periodic hours annual quantities conveniently represented angular coordinate tempted treat periodic variables choosing direction origin applying conventional distribution results strongly dependent arbitrary choice observations model standard univariate Gaussian choose origin sample mean data set standard deviation choose origin mean standard deviation clearly need develop special approach treatment periodic Let consider problem evaluating mean set observations periodic shall assume measured seen simple average strongly coordinate invariant measure note observations viewed points unit circle described instead two-dimensional unit vectors illustrated Figure average vectors PROBABILITY DISTRIBUTIONS Figure Illustration representation values periodic variable twodimensional vectors xn living unit shown average x3x4 instead xn corresponding angle definition ensure location mean independent origin angular Note typically lie inside unit Cartesian coordinates observations xn sin write Cartesian coordinates sample mean form cos sin Substituting equating components gives cos cos sin sin Taking identity tan sin cos solve sin cos shall result arises naturally maximum likelihood estimator appropriately defined distribution periodic consider periodic generalization Gaussian called von Mises shall limit attention univariate periodic distributions hyperspheres arbitrary extensive discussion periodic Mardia Jupp consider distributions period probability density defined nonnegative integrate Gaussian Distribution Figure von Mises distribution derived considering two-dimensional Gaussian form density contours shown blue conditioning unit circle shown satisfy conditions follows integer easily obtain Gaussian-like distribution satisfies properties Consider Gaussian distribution variables having mean covariance matrix identity exp contours constant illustrated Figure suppose consider value distribution circle fixed construction distribution determine form distribution transforming Cartesian coordinates polar coordinates cos sin map mean polar coordinates writing cos sin substitute transformations two-dimensional Gaussian distribution condition unit circle noting interested dependence Focussing exponent Gaussian distribution cos cos sin sin r20 2r0 cos cos 2r0 sin sin const PROBABILITY DISTRIBUTIONS Figure von Mises distribution plotted different parameter shown Cartesian plot left corresponding polar plot denotes terms independent use following trigonometrical identitiesExercise cos2 sin2 cos cos sinA sin define obtain final expression distribution unit circle form exp called von Mises circular parameter corresponds mean known concentration analogous inverse variance normalization coefficient expressed terms zeroth-order Bessel function kind defined exp cos large distribution approximately von Mises dis-Exercise tribution plotted Figure function plotted Figure consider maximum likelihood estimators parameters von Mises log likelihood function Gaussian Distribution Figure Plot Bessel function defined function defined Setting derivative respect equal zero gives solve make use trigonometric identity cos cos sinB obtainExercise tan sin cos recognize result obtained earlier mean observations viewed two-dimensional Cartesian maximizing respect making use substituted maximum likelihood solution performing joint optimization defined function plotted Figure Making use trigonometric identity write form cos cos sin sin PROBABILITY DISTRIBUTIONS Figure Plots data blue curves contours constant probability left single Gaussian distribution fitted data maximum Note distribution fails capture clumps data places probability mass central region clumps data relatively right distribution linear combination Gaussians fitted data maximum likelihood techniques discussed Chapter gives better representation right-hand easily function inverted mention briefly alternative techniques construction periodic simplest approach use histogram observations angular coordinate divided fixed virtue simplicity flexibility suffers significant shall discuss histogram methods Section approach like von Mises Gaussian distribution Euclidean space marginalizes unit circle conditioning leads complex forms distribution discussed valid distribution real axis turned periodic distribution mapping successive intervals width periodic variable corresponds real axis unit resulting distribution complex handle von Mises limitation von Mises distribution forming mixtures von Mises obtain flexible framework modelling periodic variables handle example machine learning application makes use von Mises Lawrence et extensions modelling conditional densities regression Bishop Nabney Mixtures Gaussians Gaussian distribution important analytical suffers significant limitations comes modelling real data Consider example shown Figure known data comprises measurements eruption Old Faithful geyser Yellowstone National Park measurement comprises duration ofAppendix Gaussian Distribution Figure Example Gaussian mixture distribution dimension showing Gaussians scaled blue sum eruption minutes time minutes eruption data set forms dominant simple Gaussian distribution unable capture linear superposition Gaussians gives better characterization data formed taking linear combinations basic distributions formulated probabilistic models known mixture distributions McLachlan Figure linear combination Gaussians rise complex sufficient number adjusting means covariances coefficients linear continuous density approximated arbitrary consider superposition Gaussian densities form called mixture Gaussian density called component mixture mean covariance Contour surface plots Gaussian mixture having components shown Figure section shall consider Gaussian components illustrate framework mixture mixture models comprise linear combinations Section shall consider mixtures Bernoulli distributions example mixture model discrete parameters called mixing integrate sides respect note individual Gaussian components obtain requirement implies Combining condition obtain PROBABILITY DISTRIBUTIONS Figure Illustration mixture Gaussians two-dimensional Contours constant density mixture components denoted blue values mixing coefficients shown Contours marginal probability density mixture surface plot distribution mixing coefficients satisfy requirements sum product marginal density equivalent view prior probability picking kth density probability conditioned shall later important role played posterior probabilities known theorem shall discuss probabilistic interpretation mixture distribution greater Chapter form Gaussian mixture distribution governed parameters used notation way set values parameters use maximum log likelihood function Exponential Family immediately situation complex single presence summation inside maximum likelihood solution parameters longer closed-form analytical approach maximizing likelihood function use iterative numerical optimization techniques Nocedal Bishop Alternatively employ powerful framework called expectation discussed length Chapter Exponential Family probability distributions studied far chapter exception Gaussian specific examples broad class distributions called exponential family Bernardo Members exponential family important properties illuminating discuss properties exponential family distributions parameters defined set distributions form scalar discrete called natural parameters function function interpreted coefficient ensures distribution normalized satisfies exp dx integration replaced summation discrete begin taking examples distributions introduced earlier chapter showing members exponential Consider Bernoulli distribution Expressing right-hand exponential exp exp Comparison allows identify PROBABILITY DISTRIBUTIONS solve called logistic sigmoid write Bernoulli distribution standard representation form used easily proved Comparison shows consider multinomial distribution single observation takes form exp xk xN write standard representation defined comparing Note parameters independent parameters subject constraint parameters value remaining parameter convenient remove constraint expressing distribution terms achieved relationship eliminate expressing terms remaining leaving Note remaining parameters subject constraints Exponential Family Making use constraint multinomial distribution representation exp xk exp xk xk exp xk identify solve summing sides rearranging back-substituting called softmax normalized multinomial distribution takes form standard form exponential parameter vector let consider Gaussian univariate exp exp PROBABILITY DISTRIBUTIONS simple cast standard exponential family form withExercise exp Maximum likelihood sufficient statistics Let consider problem estimating parameter vector general exponential family distribution technique maximum Taking gradient sides respect exp dx exp dx making use gives exp dx used obtain result Note covariance expressed terms second derivatives similarly higher order provided normalize aExercise distribution exponential moments simple consider set independent identically distributed data denoted likelihood function exp Setting gradient respect following condition satisfied maximum likelihood estimator Exponential Family principle solved obtain solution maximum likelihood estimator depends data called sufficient statistic distribution need store entire data set value sufficient Bernoulli function just need sum data points Gaussian sum sum consider limit right-hand comparing limit equal true value sufficiency property holds Bayesian shall defer discussion Chapter equipped tools graphical models gain deeper insight important Conjugate priors encountered concept conjugate prior example context Bernoulli distribution conjugate prior beta Gaussian conjugate prior mean conjugate prior precision Wishart probability distribution seek prior conjugate likelihood posterior distribution functional form member exponential family exists conjugate prior written form normalization function appears let multiply prior likelihood function obtain posterior normalization form exp takes functional form prior confirming parameter interpreted effective number pseudo-observations value sufficient statistic Noninformative priors applications probabilistic prior knowledge conveniently expressed prior prior assigns zero probability value posterior distribution necessarily assign zero probability irrespective PROBABILITY DISTRIBUTIONS subsequent observations little idea form distribution seek form prior called noninformative intended little influence posterior distribution possible Box Bernardo referred data speak distribution governed parameter tempted propose prior distribution const suitable discrete variable simply amounts setting prior probability state case continuous potential difficulties domain prior distribution correctly normalized integral priors called improper priors used provided corresponding posterior distribution correctly uniform prior distribution mean posterior distribution observed data second difficulty arises transformation behaviour probability density nonlinear change function change variables choose density density density issue does arise use maximum likelihood function simple function free use convenient choose prior distribution care use appropriate representation consider simple examples noninformative priors density takes form parameter known location family densities exhibits translation invariance shift constant defined density takes form new variable original density independent choice like choose prior distribution reflects translation invariance choose prior assigns equal probability mass Exponential Family interval shifted interval hold choices implies example location parameter mean Gaussian conjugate prior distribution case Gaussian obtain noninformative prior taking limit gives posterior distribution contributions prior second consider density form Note normalized density provided correctly parameter known scale density exhibitsExercise scale invariance scale constant defined transformation corresponds change example meters kilometers like choose prior distribution reflects scale consider interval scaled interval prior assign equal probability mass hold choices Note improper prior integral distribution convenient think prior distribution scale parameter terms density log transformation rule densities prior probability mass range range PROBABILITY DISTRIBUTIONS example scale parameter standard deviation Gaussian taken account location parameter discussed convenient work terms precision transformation rule distribution corresponds distribution form seen conjugate prior gamma distribution noninformative prior obtainedSection special case examine results posterior distribution posterior depends terms arising data Nonparametric Methods focussed use probability distributions having specific functional forms governed small number parameters values determined data called parametric approach density important limitation approach chosen density poor model distribution generates result poor predictive process generates data aspect distribution captured necessarily final consider nonparametric approaches density estimation make assumptions form shall focus mainly simple frequentist reader nonparametric Bayesian methods attracting increasing et Teh et Let start discussion histogram methods density encountered context marginal conditional distributions Figure context central limit theorem Figure explore properties histogram density models focussing case single continuous variable Standard histograms simply partition distinct bins width count number ni observations falling bin order turn count normalized probability simply divide total number observations width bins obtain probability values bin pi ni easily seen dx gives model density constant width bins chosen width Nonparametric Methods Figure illustration histogram approach density data set data points generated distribution shown green Histogram density based common bin width shown various values Figure example histogram density data drawn corresponding green formed mixture shown examples histogram density estimates corresponding different choices bin width small resulting density model lot structure present underlying distribution generated data large result model smooth consequently fails capture bimodal property green best results obtained intermediate value histogram density model dependent choice edge location typically significant value Note histogram method property methods discussed histogram data set advantageous data set histogram approach easily applied data points arriving histogram technique useful obtaining quick visualization data dimensions unsuited density estimation obvious problem estimated density discontinuities bin edges property underlying distribution generated major limitation histogram approach scaling divide variable D-dimensional space total number bins exponential scaling example curse space high dimensional-Section quantity data needed provide meaningful estimates local probability density histogram approach density estimation teach important estimate probability density particular consider data points lie local neighbourhood Note concept locality requires assume form distance assuming Euclidean PROBABILITY DISTRIBUTIONS neighbourhood property defined natural parameter describing spatial extent local case bin value smoothing parameter large small order obtain good reminiscent choice model complexity polynomial curve fitting discussed Chapter degree alternatively value regularization optimal intermediate large Armed turn discussion widely used nonparametric techniques density kernel estimators nearest better scaling dimensionality simple histogram Kernel density estimators Let suppose observations drawn unknown probability density D-dimensional shall wish estimate value earlier discussion let consider small region containing probability mass associated region suppose collected data set comprising observations drawn data point probability falling total number points lie inside distributed according binomial distributionSection mean fraction points falling inside region similarly variance mean large distribution sharply peaked mean assume regionR sufficiently small probability density roughly constant volume Combining obtain density estimate form NV Note validity depends contradictory region sufficiently small density approximately constant region sufficiently large relation value number points falling inside region sufficient binomial distribution sharply Nonparametric Methods exploit result different fix determine value gives rise K-nearest-neighbour technique discussed fix determine giving rise kernel shown K-nearest-neighbour density estimator kernel density estimator converge true probability density limit provided shrinks suitably grows begin discussing kernel method start region small hypercube centred point wish determine probability order count number points falling convenient define following function represents unit cube centred function example kernel context called Parzen quantity data point xn lies inside cube centred zero total number data points lying inside cube xn Substituting expression gives following result estimated density hD xn used hD volume hypercube symmetry function re-interpret single cube centred sum cubes centred data points kernel density estimator suffer problems histogram method suffered presence artificial case boundaries obtain smoother density model choose smoother kernel common choice gives rise following kernel density model exp 2h2 represents standard deviation Gaussian density model obtained placing Gaussian data point adding contributions data dividing density correctly Figure apply model data PROBABILITY DISTRIBUTIONS Figure Illustration kernel density model applied data set used demonstrate histogram approach Figure acts smoothing parameter set small result noisy density set large bimodal nature underlying distribution data generated green washed best density model obtained intermediate value set used earlier demonstrate histogram parameter plays role smoothing trade-off sensitivity noise small over-smoothing large optimization problem model analogous choice bin width histogram density degree polynomial used curve choose kernel function subject conditions du ensure resulting probability distribution nonnegative integrates class density model called kernel density Parzen great merit computation involved phase simply requires storage training great weaknesses computational cost evaluating density grows linearly size data Nearest-neighbour methods difficulties kernel approach density estimation parameter governing kernel width fixed regions high data large value lead over-smoothing washing structure extracted reducing lead noisy estimates data space density optimal choice dependent location data issue addressed nearest-neighbour methods density return general result local density instead fixing determining value consider fixed value use data appropriate value consider small sphere centred point wish estimate Nonparametric Methods Figure Illustration K-nearest-neighbour density estimation data set Figures parameter governs degree small value leads noisy density model large value smoothes bimodal nature true distribution green data set density allow radius sphere grow contains precisely data estimate density set volume resulting technique known nearest neighbours illustrated Figure various choices parameter data set used Figure Figure value governs degree smoothing optimum choice large Note model produced nearest neighbours true density model integral space close chapter showing K-nearest-neighbour technique density estimation extended problem apply K-nearest-neighbour density estimation technique class separately make use Let suppose data set comprising Nk points class Ck points Nk wish classify new point draw sphere centred containing precisely points irrespective Suppose sphere volume contains Kk points class provides estimate density associated class Kk NkV unconditional density NV class priors Nk combine theorem obtain posterior probability class membership Kk PROBABILITY DISTRIBUTIONS Figure K-nearestneighbour new shown black classified according majority class membership closest training data case nearest-neighbour approach resulting decision boundary composed hyperplanes form perpendicular bisectors pairs points different wish minimize probability assigning test point class having largest posterior corresponding largest value classify new identify nearest points training data set assign new point class having largest number representatives Ties broken particular case called nearest-neighbour test point simply assigned class nearest point training concepts illustrated Figure Figure results applying K-nearest-neighbour algorithm oil flow introduced Chapter various values controls degree small produces small regions large leads fewer larger Figure Plot data points oil data set showing values plotted blue points correspond shown classifications input space K-nearest-neighbour algorithm various values Exercises interesting property nearest-neighbour classifier limit error rate twice minimum achievable error rate optimal uses true class distributions discussed K-nearest-neighbour kernel density require entire training data set leading expensive computation data set effect expense additional one-off constructing tree-based search structures allow near neighbours efficiently doing exhaustive search data nonparametric methods severely seen simple parametric models restricted terms forms distribution need density models flexible complexity models controlled independently size training shall subsequent chapters achieve Exercises www Verify Bernoulli distribution satisfies following properties entropy Bernoulli distributed random binary variable form Bernoulli distribution symmetric values convenient use equivalent formulation case distribution written distribution evaluate www prove binomial distribution use definition number combinations identical objects chosen total PROBABILITY DISTRIBUTIONS Use result prove induction following result xm known binomial valid real values binomial distribution pulling factor summation making use binomial mean binomial distribution differentiate sides normalization condition respect rearrange obtain expression mean differentiating twice respect making use result mean binomial distribution prove result variance www prove beta correctly equivalent showing definition gamma dx Use expression prove bring integral inside integrand integral make change variable interchange order finally make change variable Make use result mode beta distribution respectively ab Exercises Consider binomial random variable prior distribution beta distribution suppose observed occurrences occurrences posterior mean value lies prior mean maximum likelihood estimate posterior mean written times prior mean plus times maximum likelihood illustrates concept posterior distribution compromise prior distribution maximum likelihood Consider variables joint distribution Prove following results Ey Ey vary denotes expectation conditional distribution similar notation conditional www prove normalization Dirichlet distribution shown Exercise beta special case Dirichlet assume Dirichlet distribution normalized variables prove normalized consider Dirichlet distribution account constraint eliminating Dirichlet written pM CM goal expression CM integrate taking care limits make change variable integral limits assuming correct result making use derive expression CM property gamma derive following results covariance Dirichlet distribution defined PROBABILITY DISTRIBUTIONS www expressing expectation Dirichlet distribution derivative respect da digamma uniform distribution continuous variable defined Verify distribution expressions mean Evaluate Kullback-Leibler divergence Gaussians www exercise demonstrates multivariate distribution maximum entropy distribution wish maximize distributions subject constraints normalized specific mean dx dx performing variational maximization Lagrange multipliers enforce constraints maximum likelihood distribution Gaussian entropy multivariate Gaussian dimensionality Exercises www Consider random variables having Gaussian distributions means precisions Derive expression differential entropy variable distribution relation dx2 completing square observe represents convolution Gaussian finally make use result entropy univariate www Consider multivariate Gaussian distribution writing precision matrix covariance sum symmetric anti-symmetric anti-symmetric term does appear exponent precision matrix taken symmetric loss inverse symmetric matrix symmetric Exercise follows covariance matrix chosen symmetric loss Consider symmetric matrix eigenvalue equation taking complex conjugate equation subtracting original forming inner product eigenvector eigenvalues use symmetry property eigenvectors ui uj orthogonal provided loss set eigenvectors chosen satisfy eigenvalues symmetric matrix having eigenvector equation expressed expansion coefficients form inverse matrix representation form www positive definite matrix defined quadratic form positive real value vector necessary sufficient condition positive definite eigenvalues defined symmetric matrix size independent www inverse symmetric matrix diagonalizing coordinate eigenvector expansion volume contained hyperellipsoid corresponding constant PROBABILITY DISTRIBUTIONS Mahalanobis distance VD volume unit sphere Mahalanobis distance defined www Prove identity multiplying sides making use definition Sections considered conditional marginal distributions multivariate consider partitioning components groups corresponding partitioning mean vector covariance matrix form making use results Section expression conditional distribution xc marginalized useful result linear algebra Woodbury matrix inversion formula multiplying sides prove correctness Let independent random mean sum sum means variable covariance matrix sum covariance matrices Confirm result agrees Exercise www Consider joint distribution variable mean covariance making use results marginal distribution making use results conditional distribution Exercises partitioned matrix inversion formula inverse precision matrix covariance matrix starting making use result verify result Consider multidimensional random vectors having Gaussian distributions sum Use results expression marginal distribution considering linear-Gaussian model comprising product marginal distribution conditional distribution www exercise provide practice manipulating quadratic forms arise linear-Gaussian giving independent check results derived main Consider joint distribution defined marginal conditional distributions examining quadratic form exponent joint technique discussed Section expressions mean covariance marginal distribution variable integrated make use Woodbury matrix inversion formula Verify results agree obtained results Chapter Consider joint distribution Exercise use technique completing square expressions mean covariance conditional distribution verify agree corresponding expressions www maximum likelihood solution covariance matrix multivariate need maximize log likelihood function respect noting covariance matrix symmetric positive proceed ignoring constraints doing straightforward results Appendix covariance matrix maximizes log likelihood function sample covariance note final result necessarily symmetric positive definite sample covariance Use result prove results xn denotes data point sampled Gaussian distribution mean covariance Inm denotes element identity prove result www analogous procedure used obtain derive expression sequential estimation variance univariate Gaussian PROBABILITY DISTRIBUTIONS starting maximum likelihood expression Verify substituting expression Gaussian distribution RobbinsMonro sequential estimation formula gives result obtain expression corresponding coefficients analogous procedure used obtain derive expression sequential estimation covariance multivariate Gaussian starting maximum likelihood expression Verify substituting expression Gaussian distribution Robbins-Monro sequential estimation formula gives result obtain expression corresponding coefficients Use technique completing square quadratic form exponent derive results Starting results posterior distribution mean Gaussian random dissect contributions data points obtain expressions sequential update derive results starting posterior distribution multiplying likelihood function completing square normalizing obtain posterior distribution www Consider D-dimensional Gaussian random variable distribution covariance known wish infer mean set observations prior distribution corresponding posterior distribution Use definition gamma function gamma distribution Evaluate mode gamma distribution following distribution exp generalization univariate Gaussian distribution normalized dx reduces Gaussian Consider regression model target variable random noise Exercises variable drawn distribution log likelihood function observed data set input vectors corresponding target variables const denotes terms independent Note function Lq error function considered Section Consider univariate Gaussian distribution having conjugate Gaussian-gamma prior data set posterior distribution Gaussian-gamma distribution functional form write expressions parameters posterior Verify Wishart distribution defined conjugate prior precision matrix multivariate www Verify evaluating integral leads result www limit t-distribution ignore normalization simply look dependence following analogous steps used derive univariate t-distribution verify result multivariate form marginalizing variable definition exchanging integration variables multivariate t-distribution correctly definition multivariate t-distribution convolution Gaussian gamma verify properties multivariate t-distribution defined limit multivariate t-distribution reduces Gaussian mean precision www various trigonometric identities used discussion periodic variables chapter proven easily relation cos sinA square root minus considering identity prove result identity PROBABILITY DISTRIBUTIONS denotes real prove denotes imaginary prove result large von Mises distribution sharply peaked mode defining making Taylor expansion cosine function cos von Mises distribution tends trigonometric identity solution computing second derivatives von Mises distribution maximum distribution occurs minimum occurs making use result trigonometric identity maximum likelihood solution mML concentration von Mises distribution satisfies radius mean observations viewed unit vectors two-dimensional Euclidean illustrated Figure www Express beta distribution gamma distribution von Mises distribution members exponential family identify natural Verify multivariate Gaussian distribution cast exponential family form derive expressions analogous result showed negative gradient exponential family expectation taking second derivatives changing variables density correctly provided correctly www Consider histogram-like density model space divided fixed regions density takes constant value hi ith volume region denoted Suppose set observations ni observations fall region Lagrange multiplier enforce normalization constraint derive expression maximum likelihood estimator K-nearest-neighbour density model defines improper distribution integral space 
Linear Models Regression focus far book unsupervised including topics density estimation data turn discussion supervised starting goal regression predict value continuous target variables value D-dimensional vector input encountered example regression problem considered polynomial curve fitting Chapter polynomial specific example broad class functions called linear regression share property linear functions adjustable form focus simplest form linear regression models linear functions input obtain useful class functions taking linear combinations fixed set nonlinear functions input known basis models linear functions gives simple analytical nonlinear respect input LINEAR MODELS REGRESSION training data set comprising observations corresponding target values goal predict value new value simplest directly constructing appropriate function values new inputs constitute predictions corresponding values probabilistic aim model predictive distribution expresses uncertainty value value conditional distribution make predictions new value way minimize expected value suitably chosen loss discussed Section common choice loss function real-valued variables squared optimal solution conditional expectation linear models significant limitations practical techniques pattern particularly problems involving input spaces high nice analytical properties form foundation sophisticated models discussed later Linear Basis Function Models simplest linear model regression involves linear combination input variables w1x1 wDxD simply known linear key property model linear function parameters linear function input variables imposes significant limitations extend class models considering linear combinations fixed nonlinear functions input form known basis denoting maximum value index total number parameters model parameter allows fixed offset data called bias parameter confused statistical convenient define additional dummy practical applications pattern apply form fixed Linear Basis Function Models feature original data original variables comprise vector features expressed terms basis functions nonlinear basis allow function nonlinear function input vector Functions form called linear function linear linearity parameters greatly simplify analysis class leads significant discuss Section example polynomial regression considered Chapter particular example model single input variable basis functions form powers xj limitation polynomial basis functions global functions input changes region input space affect resolved dividing input space regions fit different polynomial leading spline functions et possible choices basis example exp 2s2 govern locations basis functions input parameter governs spatial usually referred basis noted required probabilistic particular normalization coefficient unimportant basis functions multiplied adaptive parameters wj possibility sigmoidal basis function form logistic sigmoid function defined use function related logistic sigmoid general linear combination logistic sigmoid functions equivalent general linear combination various choices basis function illustrated Figure possible choice basis function Fourier leads expansion sinusoidal basis function represents specific frequency infinite spatial basis functions localized finite regions input space necessarily comprise spectrum different spatial signal processing consider basis functions localized space leading class functions known defined mutually simplify Wavelets applicable input values live LINEAR MODELS REGRESSION Figure Examples basis showing polynomials Gaussians form sigmoidal form regular successive time points temporal pixels Useful texts wavelets include Ogden Mallat Vidakovic discussion independent particular choice basis function discussion shall specify particular form basis purposes numerical discussion equally applicable situation vector basis functions simply identity order notation shall focus case single target variable Section consider briefly modifications needed deal multiple target Maximum likelihood squares Chapter fitted polynomial functions data sets minimizing sumof-squares error showed error function motivated maximum likelihood solution assumed Gaussian noise Let return discussion consider squares relation maximum assume target variable deterministic function additive Gaussian noise zero mean Gaussian random variable precision write Recall assume squared loss optimal new value conditional mean target theSection case Gaussian conditional distribution form conditional mean Linear Basis Function Models simply dt Note Gaussian noise assumption implies conditional distribution inappropriate extension mixtures conditional Gaussian permit multimodal conditional discussed Section consider data set inputs corresponding target values group target variables column vector denote typeface chosen distinguish single observation multivariate denoted Making assumption data points drawn independently distribution obtain following expression likelihood function adjustable parameters form used Note supervised learning problems regression seeking model distribution input appear set conditioning drop explicit expressions order notation Taking logarithm likelihood making use standard form univariate lnN sum-of-squares error function defined Having written likelihood use maximum likelihood determine Consider maximization respect observed Section maximization likelihood function conditional Gaussian noise distribution linear model equivalent minimizing sum-of-squares error function gradient log likelihood function takes form LINEAR MODELS REGRESSION Setting gradient zero gives Solving obtain wML known normal equations squares called design elements quantity known Moore-Penrose pseudo-inverse matrix Golub Van regarded generalization notion matrix inverse nonsquare square property gain insight role bias parameter make bias parameter error function Setting derivative respect equal solving obtain defined bias compensates difference averages training target values weighted sum averages basis function maximize log likelihood function respect noise precision parameter giving Linear Basis Function Models Figure Geometrical interpretation least-squares -dimensional space axes values least-squares regression function obtained finding orthogonal projection data vector subspace spanned basis functions basis function viewed vector length elements inverse noise precision residual variance target values regression Geometry squares instructive consider geometrical interpretation least-squares consider -dimensional space axes vector basis function evaluated data represented vector denoted illustrated Figure Note corresponds jth column corresponds nth row number basis functions smaller number data vectors span linear subspace dimensionality define -dimensional vector nth element arbitrary linear combination vectors live -dimensional sum-of-squares error equal factor squared Euclidean distance least-squares solution corresponds choice lies subspace closest Figure anticipate solution corresponds orthogonal projection subspace easily verified noting solution confirming takes form orthogonal direct solution normal equations lead numerical difficulties close basis vectors nearly resulting parameter values large near degeneracies uncommon dealing real data resulting numerical difficulties addressed technique singular value SVD et Bishop Note addition regularization term ensures matrix presence Sequential learning Batch maximum likelihood solution involve processing entire training set computationally costly large data discussed Chapter data set sufficiently worthwhile use sequential known on-line LINEAR MODELS REGRESSION data points considered model parameters updated Sequential learning appropriate realtime applications data observations arriving continuous predictions data points obtain sequential learning algorithm applying technique stochastic gradient known sequential gradient error function comprises sum data points presentation pattern stochastic gradient descent algorithm updates parameter vector denotes iteration learning rate shall discuss choice value value initialized starting vector case sum-of-squares error function gives known least-mean-squares LMS value needs chosen care ensure algorithm converges Regularized squares Section introduced idea adding regularization term error function order control total error function minimized takes form regularization coefficient controls relative importance data-dependent error regularization term EW simplest forms regularizer sum-of-squares weight vector elements EW consider sum-of-squares error function total error function particular choice regularizer known machine learning literature weight decay sequential learning encourages weight values decay unless supported provides example parameter shrinkage method shrinks parameter values Linear Basis Function Models Figure Contours regularization term various values parameter advantage error function remains quadratic function exact minimizer closed setting gradient respect solving obtain represents simple extension least-squares solution general regularizer regularized error takes form corresponds quadratic regularizer Figure shows contours regularization function different values case know lasso statistics literature property sufficiently coefficients wj driven leading sparse model corresponding basis functions play note minimizing equivalent minimizing unregularized sum-of-squares error subject constraintExercise appropriate value parameter approaches related Lagrange origin sparsity seen Figure shows minimum error subject constraint increasing number parameters driven Regularization allows complex models trained data sets limited size severe essentially limiting effective model problem determining optimal model complexity shifted finding appropriate number basis functions determining suitable value regularization coefficient shall return issue model complexity later LINEAR MODELS REGRESSION Figure Plot contours unregularized error function constraint region quadratic regularizer left lasso regularizer optimum value parameter vector denoted lasso gives sparse solution remainder chapter shall focus quadratic regularizer practical importance analytical Multiple outputs considered case single target variable wish predict target denote collectively target vector introducing different set basis functions component leading independent regression approach use set basis functions model components target vector K-dimensional column matrix -dimensional column vector elements Suppose conditional distribution target vector isotropic Gaussian form set observations combine matrix size nth row tTn combine input vectors matrix log likelihood function lnN NK Bias-Variance Decomposition maximize function respect giving WML examine result target variable wk tk -dimensional column vector components tnk solution regression problem decouples different target need compute single pseudo-inverse matrix shared vectors extension general Gaussian noise distributions having arbitrary covariance matrices leads decoupling inde-Exercise pendent regression result unsurprising parameters define mean Gaussian noise know Section maximum likelihood solution mean multivariate Gaussian independent shall consider single target variable Bias-Variance Decomposition far discussion linear models assumed form number basis functions seen Chapter use maximum equivalently lead severe over-fitting complex models trained data sets limited limiting number basis functions order avoid over-fitting effect limiting flexibility model capture interesting important trends introduction regularization terms control over-fitting models raises question determine suitable value regularization coefficient Seeking solution minimizes regularized error function respect weight vector regularization coefficient clearly right approach leads unregularized solution seen earlier phenomenon over-fitting really unfortunate property maximum likelihood does arise marginalize parameters Bayesian shall consider Bayesian view model complexity doing instructive consider frequentist viewpoint model complexity known biasvariance shall introduce concept context linear basis function easy illustrate ideas simple discussion general Section discussed decision theory regression considered various loss functions leads corresponding optimal prediction conditional distribution popular choice LINEAR MODELS REGRESSION squared loss optimal prediction conditional denote worth distinguishing squared loss function arising decision theory sum-of-squares error function arose maximum likelihood estimation model use sophisticated techniques example regularization fully Bayesian determine conditional distribution combined squared loss function purpose making showed Section expected squared loss written form dx Recall second independent arises intrinsic noise data represents minimum achievable value expected term depends choice function seek solution makes term smallest hope make term unlimited supply data unlimited computational principle regression function desired degree represent optimal choice practice data set containing finite number data consequently know regression function model parametric function governed parameter vector Bayesian perspective uncertainty model expressed posterior distribution frequentist involves making point estimate based data set tries instead interpret uncertainty estimate following thought Suppose large number data sets size drawn independently distribution data set run learning algorithm obtain prediction function Different data sets ensemble different functions consequently different values squared performance particular learning algorithm assessed taking average ensemble data Consider integrand term particular data set takes form quantity dependent particular data set average ensemble data add subtract quantity Bias-Variance Decomposition inside obtain expectation expression respect note final term giving ED ED variance expected squared difference regression function expressed sum called squared represents extent average prediction data sets differs desired regression second called measures extent solutions individual data sets vary measures extent function sensitive particular choice data shall provide intuition support definitions shortly consider simple considered single input value substitute expansion obtain following decomposition expected squared loss expected loss variance noise dx variance ED dx noise dxdt bias variance terms refer integrated goal minimize expected decomposed sum constant noise shall trade-off bias flexible models having low bias high relatively rigid models having high bias low model optimal predictive capability leads best balance bias illustrated considering sinusoidal data set Chapter generate data containing 25Appendix data independently sinusoidal curve data sets indexed data set LINEAR MODELS REGRESSION Figure Illustration dependence bias variance model governed regularization parameter sinusoidal data set Chapter data having data Gaussian basis functions model total number parameters including bias left column shows result fitting model data sets various values fits right column shows corresponding average fits sinusoidal function data sets generated Bias-Variance Decomposition Figure Plot squared bias corresponding results shown Figure shown average test set error test data set size minimum value variance occurs close value gives minimum error test variance variance test error fit model Gaussian basis functions minimizing regularized error function prediction function shown Figure row corresponds large value regularization coefficient gives low variance red curves left plot look high bias curves right plot Conversely large variance high variability red curves left low bias good fit average model fit original sinusoidal Note result averaging solutions complex model good fit regression suggests averaging beneficial weighted averaging multiple solutions lies heart Bayesian averaging respect posterior distribution respect multiple data examine bias-variance trade-off quantitatively average prediction estimated integrated squared bias integrated variance variance integral weighted distribution approximated finite sum data points drawn plotted function Figure small values allow model finely tuned noise individual LINEAR MODELS REGRESSION data set leading large large value pulls weight parameters zero leading large bias-variance decomposition provide interesting insights model complexity issue frequentist limited practical bias-variance decomposition based averages respect ensembles data practice single observed data large number independent training sets better combining single large training course reduce level over-fitting model turn section Bayesian treatment linear basis function provides powerful insights issues over-fitting leads practical techniques addressing question model Bayesian Linear Regression discussion maximum likelihood setting parameters linear regression seen effective model governed number basis needs controlled according size data Adding regularization term log likelihood function means effective model complexity controlled value regularization choice number form basis functions course important determining overall behaviour leaves issue deciding appropriate model complexity particular decided simply maximizing likelihood leads excessively complex models Independent hold-out data used determine model discussed Section computationally expensive wasteful valuable turn Bayesian treatment linear avoid over-fitting problem maximum lead automatic methods determining model complexity training data simplicity focus case single target variable Extension multiple target variables straightforward follows discussion Section Parameter distribution begin discussion Bayesian treatment linear regression introducing prior probability distribution model parameters shall treat noise precision parameter known note likelihood function defined exponential quadratic function corresponding conjugate prior Gaussian distribution form having mean covariance Bayesian Linear Regression compute posterior proportional product likelihood function choice conjugate Gaussian prior posterior evaluate distribution usual procedure completing square finding normalization coefficient standard result normalized necessary work deriving gen-Exercise eral result allows write posterior distribution directly form mN SN Tt Note posterior distribution mode coincides maximum posterior weight vector simply wMAP mN consider infinitely broad prior mean mN posterior distribution reduces maximum likelihood value wML posterior distribution reverts data points arrive posterior distribution stage acts prior distribution subsequent data new posterior distribution remainder shall consider particular form Gaussian prior order simplify consider zero-mean isotropic Gaussian governed single precision parameter corresponding posterior distribution mN log posterior distribution sum log likelihood log prior function takes form Tw Maximization posterior distribution respect equivalent minimization sum-of-squares error function addition quadratic regularization corresponding illustrate Bayesian learning linear basis function sequential update posterior simple example involving straight-line Consider single input variable single target variable LINEAR MODELS REGRESSION linear model form just adaptive plot prior posterior distributions directly parameter generate synthetic data function parameter values choosing values xn uniform distribution evaluating finally adding Gaussian noise standard deviation obtain target values goal recover values explore dependence size data assume noise variance known set precision parameter true value fix parameter shall shortly discuss strategies determining training Figure shows results Bayesian learning model size data set increased demonstrates sequential nature Bayesian learning current posterior distribution forms prior new data point worth taking time study figure illustrates important aspects Bayesian row figure corresponds situation data points observed shows plot prior distribution space samples function values drawn second situation observing single data location data point shown blue circle right-hand left-hand column plot likelihood function data point function Note likelihood function provides soft constraint line pass close data close determined noise precision true parameter values used generate data set shown white cross plots left column Figure multiply likelihood function prior obtain posterior distribution shown middle plot second Samples regression function obtained drawing samples posterior distribution shown right-hand Note sample lines pass close data row figure shows effect observing second data shown blue circle plot right-hand corresponding likelihood function second data point shown left multiply likelihood function posterior distribution second obtain posterior distribution shown middle plot Note exactly posterior distribution obtained combining original prior likelihood function data posterior influenced data points sufficient define line gives relatively compact posterior Samples posterior distribution rise functions shown red functions pass close data fourth row shows effect observing total data left-hand plot shows likelihood function 20th data point middle plot shows resulting posterior distribution absorbed information Note posterior sharper limit infinite number data Bayesian Linear Regression Figure Illustration sequential Bayesian learning simple linear model form detailed description figure LINEAR MODELS REGRESSION posterior distribution delta function centred true parameter shown white forms prior parameters generalize Gaussian prior exp corresponds Gaussian case prior conjugate likelihood function Finding maximum posterior distribution corresponds minimization regularized error function case Gaussian mode posterior distribution equal longer hold Predictive distribution usually interested value making predictions new values requires evaluate predictive distribution defined dw vector target values training omitted corresponding input vectors right-hand conditioning statements simplify conditional distribution target variable posterior weight distribution involves convolution Gaussian making use result Section predictive distribution takes formExercise variance predictive distribution term represents noise data second term reflects uncertainty associated parameters noise process distribution independent variances Note additional data points posterior distribution consequence shown et limit second term goes varianceExercise predictive distribution arises solely additive noise governed parameter illustration predictive distribution Bayesian linear regression let return synthetic sinusoidal data set Section Figure Bayesian Linear Regression Figure Examples predictive distribution model consisting Gaussian basis functions form synthetic sinusoidal data set Section text detailed fit model comprising linear combination Gaussian basis functions data sets various sizes look corresponding posterior green curves correspond function data points generated addition Gaussian Data sets size shown plots blue red curve shows mean corresponding Gaussian predictive red shaded region spans standard deviation Note predictive uncertainty depends smallest neighbourhood data note level uncertainty decreases data points plots Figure point-wise predictive variance function order gain insight covariance predictions different values draw samples posterior distribution plot corresponding functions shown Figure LINEAR MODELS REGRESSION Figure Plots function samples posterior distributions corresponding plots Figure used localized basis functions regions away basis function contribution second term predictive variance leaving noise contribution model confident predictions extrapolating outside region occupied basis generally undesirable problem avoided adopting alternative Bayesian approach regression known Gaussian Note treated introduce conjugate prior distribution discussion Section Gaussian-gamma distribution et theExercise predictive distribution Bayesian Linear Regression Figure equivalent kernel Gaussian basis functions Figure shown plot versus slices matrix corresponding different values data set used generate kernel comprised values equally spaced interval Equivalent kernel posterior mean solution linear basis function model interesting interpretation set stage kernel including Gaussian substitute expression predictiveChapter mean written form SN defined mean predictive distribution point linear combination training set target variables write function known smoother matrix equivalent Regression make predictions taking linear combinations training set target values known linear Note equivalent kernel depends input values xn data set appear definition SN equivalent kernel illustrated case Gaussian basis functions Figure kernel functions plotted function different values localized mean predictive distribution obtained forming weighted combination target values data points close higher weight points removed reasonable weight local evidence strongly distant Note localization property holds localized Gaussian basis functions nonlocal polynomial sigmoidal basis illustrated Figure LINEAR MODELS REGRESSION Figure Examples equivalent kernels plotted function corresponding polynomial basis functions sigmoidal basis functions shown Figure Note localized functions corresponding basis functions insight role equivalent kernel obtained considering covariance use form equivalent predictive mean nearby points highly distant pairs points correlation predictive distribution shown Figure allows visualize pointwise uncertainty governed drawing samples posterior distribution plotting corresponding model functions Figure visualizing joint uncertainty posterior distribution values governed equivalent formulation linear regression terms kernel function suggests alternative approach regression Instead introducing set basis implicitly determines equivalent instead define localized kernel directly use make predictions new input vectors observed training leads practical framework regression called Gaussian discussed Section seen effective kernel defines weights training set target values combined order make prediction new value shown weights sum words values intuitively pleasing result easily proven informallyExercise noting summation equivalent considering predictive mean set target data Provided basis functions linearly data points basis basis functions constant bias clear fit training data exactly predictive mean Bayesian Model Comparison simply obtain Note kernel function negative satisfies summation corresponding predictions necessarily convex combinations training set target note equivalent kernel satisfies important property shared kernel functions expressed form anChapter inner product respect vector nonlinear Bayesian Model Comparison Chapter highlighted problem over-fitting use crossvalidation technique setting values regularization parameters choosing alternative consider problem model selection Bayesian discussion Section shall ideas applied determination regularization parameters linear shall over-fitting associated maximum likelihood avoided marginalizing model parameters instead making point estimates Models compared directly training need validation allows available data used training avoids multiple training runs model associated allows multiple complexity parameters determined simultaneously training Chapter shall introduce relevance vector Bayesian model having complexity parameter training data Bayesian view model comparison simply involves use probabilities represent uncertainty choice consistent application sum product rules Suppose wish compare set models model refers probability distribution observed data case polynomial curve-fitting distribution defined set target values set input values assumed types model define joint distributions shall suppose data generated models weSection uncertain uncertainty expressed prior probability distribution training set wish evaluate posterior distribution prior allows express preference different Let simply assume models equal prior interesting term model evidence expresses preference shown data LINEAR MODELS REGRESSION different shall examine term model evidence called marginal likelihood viewed likelihood function space parameters marginalized ratio model evidences models known Bayes factor know posterior distribution predictive distribution sum product example mixture distribution overall predictive distribution obtained averaging predictive distributions individual weighted posterior probabilities models a-posteriori equally likely predicts narrow distribution predicts narrow distribution overall predictive distribution bimodal distribution modes single model simple approximation model averaging use single probable model make known model model governed set parameters model evidence sum product rules sampling marginal likelihood viewed proba-Chapter bility generating data set model parameters sampled random interesting note evidence precisely normalizing term appears denominator theorem evaluating posterior distribution parameters obtain insight model evidence making simple approximation integral Consider case model having single parameter posterior distribution parameters proportional omit dependence model Mi notation assume posterior distribution sharply peaked probable value width approximate integral value integrand maximum times width assume prior flat width dw Bayesian Model Comparison Figure obtain rough approximation model evidence assume posterior distribution parameters sharply peaked mode wMAP taking logs obtain approximation illustrated Figure term represents fit data probable parameter flat prior correspond log second term penalizes model according term increases magnitude ratio gets parameters finely tuned data posterior penalty term model having set make similar approximation parameter Assuming parameters ratio obtain simple size complexity penalty increases linearly number adaptive parameters increase complexity term typically complex model better able fit second term increase dependence optimal model determined maximum trade-off competing shall later develop refined version based Gaussian approximation posterior gain insight Bayesian model comparison understand marginal likelihood favour models intermediate complexity considering Figure horizontal axis one-dimensional representation space possible data point axis corresponds specific data consider models successively increasing Imagine running models generatively produce example data looking distribution data sets LINEAR MODELS REGRESSION Figure Schematic illustration distribution data sets models different simplest Note distributions particular observed data set model intermediate complexity largest DD0 model generate variety different data sets parameters governed prior probability choice parameters random noise target generate particular data set specific choose values parameters prior distribution parameter values sample data simple model based order little variability generate data sets fairly similar distribution confined relatively small region horizontal complex model ninth order generate great variety different data distribution spread large region space data distributions particular data set highest value evidence model intermediate simpler model fit data complex model spreads predictive probability broad range data sets assigns relatively small probability Implicit Bayesian model comparison framework assumption true distribution data generated contained set models Provided Bayesian model comparison average favour correct consider models truth corresponds finite data possible Bayes factor larger incorrect average Bayes factor distribution data obtain expected Bayes factor form dD average taken respect true distribution quantity example Kullback-Leibler divergence satisfies prop-Section erty positive unless distributions equal case average Bayes factor favour correct seen Bayesian framework avoids problem over-fitting allows models compared basis training data Evidence Approximation Bayesian like approach pattern needs make assumptions form invalid results Figure model evidence sensitive aspects behaviour evidence defined prior seen noting improper prior arbitrary scaling factor normalization coefficient defined distribution consider proper prior suitable limit order obtain improper prior Gaussian prior limit infinite evidence seen Figure possible consider evidence ratio models limit obtain meaningful practical wise aside independent test set data evaluate overall performance final Evidence Approximation fully Bayesian treatment linear basis function introduce prior distributions hyperparameters make predictions marginalizing respect hyperparameters respect parameters integrate analytically complete marginalization variables analytically discuss approximation set hyperparameters specific values determined maximizing marginal likelihood function obtained integrating parameters framework known statistics literature empirical Bayes Gelman et type maximum likelihood generalized maximum likelihood machine learning literature called evidence approximation introduce hyperpriors predictive distribution obtained marginalizing dw mN SN defined omitted dependence input variable notation posterior distribution sharply peaked values predictive distribution obtained simply marginalizing fixed values LINEAR MODELS REGRESSION posterior distribution prior relatively evidence framework values obtained maximizing marginal likelihood function shall proceed evaluating marginal likelihood linear basis function model finding allow determine values hyperparameters training data recourse Recall ratio analogous regularization aside worth noting define conjugate prior distributions marginalization hyperparameters performed analytically t-distribution Section resulting integral longer analytically thought approximating example Laplace approximation discussed based local Gaussian approximation centred mode posterior provide practical alternative evidence framework integrand function typically strongly skewed mode Laplace approximation fails capture bulk probability leading poorer results obtained maximizing evidence Returning evidence note approaches maximization log evaluate evidence function analytically set derivative equal zero obtain re-estimation equations shall Section Alternatively use technique called expectation maximization discussed Section shall approaches converge Evaluation evidence function marginal likelihood function obtained integrating weight parameters way evaluate integral make use result conditional distribution linear-Gaussian shall evaluateExercise integral instead completing square exponent making use standard form normalization coefficient write evidence function formExercise exp dw Evidence Approximation dimensionality defined recognize constant regularized sum-of-squares error function complete square wExercise giving introduced NmN Note corresponds matrix second derivatives error function known Hessian defined mN mN equivalent previous definition represents mean posterior integral evaluated simply appealing standard result normalization coefficient multivariate givingExercise exp dw exp dw write log marginal likelihood form required expression evidence Returning polynomial regression plot model evidence order shown Figure assumed prior form parameter fixed form plot Referring Figure polynomial poor fit data consequently gives relatively low value LINEAR MODELS REGRESSION Figure Plot model evidence versus order polynomial regression showing evidence favours model Going polynomial greatly improves data evidence significantly going data fit improved fact underlying sinusoidal function data generated odd function terms polynomial Figure shows residual data error reduced slightly going richer model suffers greater complexity evidence actually falls going obtain significant improvement data seen Figure evidence increased giving highest overall evidence increases value produce small improvements fit data suffer increasing complexity leading overall decrease evidence Looking Figure generalization error roughly constant difficult choose models basis plot evidence clear preference simplest model gives good explanation observed Maximizing evidence function Let consider maximization respect defining following eigenvector ui follows eigenvalues consider derivative term involving respect stationary points respect satisfy mTNmN Evidence Approximation Multiplying obtain terms sum quantity written interpretation quantity discussed value maximizes marginal likelihood satisfiesExercise mTNmN Note implicit solution depends mode mN posterior distribution depends choice adopt iterative procedure make initial choice use mN evaluate values used re-estimate process repeated Note matrix compute eigenvalues start simply multiply obtain emphasized value determined purely looking training contrast maximum likelihood independent data set required order optimize model similarly maximize log marginal likelihood respect note eigenvalues defined proportional giving stationary point marginal likelihood satisfies rearranging obtainExercise implicit solution solved choosing initial value calculate mN re-estimate repeating determined values re-estimated update LINEAR MODELS REGRESSION Figure Contours likelihood function prior axes parameter space rotated align eigenvectors ui mode posterior maximum likelihood solution nonzero mode wMAP mN direction eigenvalue defined small compared quantity close corresponding MAP value close direction eigenvalue large compared quantity close MAP value close maximum likelihood wMAP wML Effective number parameters result elegant interpretation provides insight Bayesian solution consider contours likelihood function prior illustrated Figure implicitly transformed rotated set axes parameter space aligned eigenvectors ui defined Contours likelihood function axis-aligned eigenvalues measure curvature likelihood Figure eigenvalue small compared smaller curvature corresponds greater elongation contours likelihood positive definite positive ratio lie quantity defined lie range directions corresponding parameter wi close maximum likelihood ratio close parameters called determined values tightly constrained directions corresponding parameters wi close ratios directions likelihood function relatively insensitive parameter value parameter set small value quantity defined measures effective total number determined obtain insight result re-estimating comparing corresponding maximum likelihood result formulae express variance inverse average squared differences targets model differ number data points denominator maximum likelihood result replaced Bayesian recall maximum likelihood estimate variance Gaussian distribution Evidence Approximation single variable estimate biased maximum likelihood solution mean fitted noise used degree freedom corresponding unbiased estimate takes form shall Section result obtained Bayesian treatment marginalize unknown factor denominator Bayesian result takes account fact degree freedom used fitting mean removes bias maximum consider corresponding results linear regression mean target distribution function contains parameters tuned effective number parameters determined data remaining parameters set small values reflected Bayesian result variance factor correcting bias maximum likelihood illustrate evidence framework setting hyperparameters sinusoidal synthetic data set Section Gaussian basis function model comprising basis total number parameters model including simplicity set true value used evidence framework determine shown Figure parameter controls magnitude parameters plotting individual parameters versus effective number shown Figure consider limit number data points large relation number parameters determined data involves implicit sum data eigenvalues increase size data re-estimation equations 2EW EW ED defined results used easy-to-compute approximation evidence re-estimation LINEAR MODELS REGRESSION Figure left plot shows versus sinusoidal synthetic data intersection curves defines optimum value evidence right plot shows corresponding graph log evidence versus showing peak coincides crossing point curves left shown test set error showing evidence maximum occurs close point best require evaluation eigenvalue spectrum Figure Plot parameters wi Gaussian basis function model versus effective number parameters hyperparameter varied range causing vary range wi Limitations Fixed Basis Functions focussed models comprising linear combination nonlinear basis seen assumption linearity parameters led range useful properties including closed-form solutions least-squares tractable Bayesian suitable choice basis model arbitrary nonlinearities Exercises mapping input variables shall study analogous class models linear models constitute general purpose framework solving problems pattern significant shortcomings linear cause turn later chapters complex models support vector machines neural difficulty stems assumption basis functions fixed training data set observed manifestation curse dimensionality discussed Section number basis functions needs grow dimensionality input properties real data sets exploit help alleviate data vectors typically lie close nonlinear manifold intrinsic dimensionality smaller input space result strong correlations input example consider images handwritten digits Chapter localized basis arrange scattered input space regions containing approach used radial basis function networks support vector relevance vector Neural network use adaptive basis functions having sigmoidal adapt parameters regions input space basis functions vary corresponds data second property target variables significant dependence small number possible directions data Neural networks exploit property choosing directions input space basis functions Exercises www function logistic sigmoid function related general linear combination logistic sigmoid functions form equivalent linear combination functions form uj tanh expressions relate new parameters original parameters LINEAR MODELS REGRESSION matrix takes vector projects space spanned columns Use result least-squares solution corresponds orthogonal projection vector manifold shown Figure Consider data set data point associated weighting factor rn sum-of-squares error function rn expression solution minimizes error alternative interpretations weighted sum-of-squares error function terms data dependent noise variance replicated data www Consider linear model form wixi sum-of-squares error function form suppose Gaussian noise zero mean variance added independently input variables making use minimizing ED averaged noise distribution equivalent minimizing sum-of-squares error noise-free input variables addition weight-decay regularization bias parameter omitted www technique Lagrange discussed Appendix minimization regularized error function equivalent minimizing unregularized sum-of-squares error subject constraint Discuss relationship parameters www Consider linear basis function regression model multivariate target variable having Gaussian distribution form Exercises training data set comprising input basis vectors corresponding target vectors maximum likelihood solution WML parameter matrix property column expression form solution isotropic noise Note independent covariance matrix maximum likelihood solution technique completing verify result posterior distribution parameters linear basis function model mN SN defined www Consider linear basis function model Section suppose observed data posterior distribution posterior regarded prior considering additional data point completing square resulting posterior distribution SN replaced mN replaced Repeat previous exercise instead completing square make use general result linear-Gaussian models www making use result evaluate integral verify predictive distribution Bayesian linear regression model input-dependent variance seen size data set uncertainty associated posterior distribution model parameters Make use matrix identity vvT uncertainty associated linear regression function satisfies saw Section conjugate prior Gaussian distribution unknown mean unknown precision normal-gamma property holds case conditional Gaussian distribution linear regression consider likelihood function conjugate prior LINEAR MODELS REGRESSION corresponding posterior distribution takes functional bN expressions posterior parameters mN SN bN predictive distribution model discussed Exercise t-distribution form obtain expressions explore properties equivalent kernel defined SN defined Suppose basis functions linearly independent number data points greater number basis let basis functions say taking suitable linear combinations basis construct new basis set spanning space Ijk Ijk defined equivalent kernel written Use result kernel satisfies summation constraint www Consider linear basis function model regression parameters set evidence function defined satisfies relation Derive result log evidence function linear regression model making use evaluate integral evidence function Bayesian linear regression model written form defined www completing square error function Bayesian linear regression written form integration Bayesian linear regression model gives result log marginal likelihood Exercises www Starting verify steps needed maximization log marginal likelihood function respect leads re-estimation equation alternative way derive result optimal value evidence framework make use identity Tr Prove identity considering eigenvalue expansion symmetric matrix making use standard results determinant trace expressed terms eigenvalues make use derive starting Starting verify steps needed maximization log marginal likelihood function respect leads re-estimation equation www marginal probability words model model described Exercise ba00 baNN marginalizing respect respect Repeat previous exercise use theorem form substitute prior posterior distributions likelihood function order derive result 
Linear Models Classification previous explored class regression models having particularly simple analytical computational discuss analogous class models solving classification goal classification input vector assign discrete classes Ck common classes taken input assigned input space divided decision regions boundaries called decision boundaries decision consider linear models mean decision surfaces linear functions input vector defined hyperplanes D-dimensional input Data sets classes separated exactly linear decision surfaces said linearly regression target variable simply vector real numbers values wish case various LINEAR MODELS CLASSIFICATION ways target values represent class probabilistic case two-class binary representation single target variable represents class represents class interpret value probability class values probability taking extreme values convenient use 1-of-K coding scheme vector length class elements tk zero element tj takes value pattern class target vector interpret value tk probability class nonprobabilistic alternative choices target variable representation prove Chapter identified distinct approaches classification simplest involves constructing discriminant function directly assigns vector specific powerful models conditional probability distribution inference subsequently uses distribution make optimal separating inference gain numerous discussed Section different approaches determining conditional probabilities technique model example representing parametric models optimizing parameters training adopt generative approach model class-conditional densities prior probabilities compute required posterior probabilities theorem shall discuss examples approaches linear regression models considered Chapter model prediction linear function parameters simplest model linear input variables takes form real classification wish predict discrete class generally posterior probabilities lie range achieve consider generalization model transform linear function nonlinear function wTx machine learning literature known activation inverse called link function statistics decision surfaces correspond wTx constant decision surfaces linear functions function class models described called generalized linear models Discriminant Functions contrast models used longer linear parameters presence nonlinear function lead complex analytical computational properties linear regression models relatively simple compared general nonlinear models studied subsequent algorithms discussed chapter equally applicable make fixed nonlinear transformation input variables vector basis functions did regression models Chapter begin considering classification directly original input space Section shall convenient switch notation involving basis functions consistency later Discriminant Functions discriminant function takes input vector assigns denoted shall restrict attention linear decision surfaces simplify consider case classes investigate extension classes simplest representation linear discriminant function obtained taking linear function input vector wTx called weight bias confused bias statistical negative bias called input vector assigned class class corresponding decision boundary defined relation corresponds hyperplane D-dimensional input Consider points xA xB lie decision vector orthogonal vector lying decision determines orientation decision point decision normal distance origin decision surface wTx bias parameter determines location decision properties illustrated case Figure note value gives signed measure perpendicular distance point decision consider LINEAR MODELS CLASSIFICATION Figure Illustration geometry linear discriminant function decision shown perpendicular displacement origin controlled bias parameter signed orthogonal distance general point decision surface arbitrary point let orthogonal projection decision Multiplying sides result wT adding making use wTx result illustrated Figure linear regression models Chapter convenient use compact notation introduce additional dummy value define decision surfaces D-dimensional hyperplanes passing origin 1-dimensional expanded input Multiple classes consider extension linear discriminants tempted build K-class discriminant combining number two-class discriminant leads difficulties Consider use classifiers solves two-class problem separating points particular class Ck points known one-versus-the-rest left-hand example Figure shows Discriminant Functions Figure Attempting construct class discriminant set class discriminants leads ambiguous shown left example involving use discriminants designed distinguish points class Ck points class right example involving discriminant functions used separate pair classes Ck example involving classes approach leads regions input space ambiguously alternative introduce binary discriminant possible pair known one-versus-one point classified according majority vote discriminant runs problem ambiguous illustrated right-hand diagram Figure avoid difficulties considering single K-class discriminant comprising linear functions form wTk wk0 assigning point class Ck decision boundary class Ck class corresponds hyperplane defined form decision boundary two-class case discussed Section analogous geometrical properties decision regions discriminant singly connected consider points xA xB lie inside decision region illustrated Figure point lies line connecting xA xB expressed form LINEAR MODELS CLASSIFICATION Figure Illustration decision regions multiclass linear decision boundaries shown points xA xB lie inside decision region point bx lies line connecting points lie decision region singly connected Ri Rj Rk xA xB linearity discriminant follows xA xB lie inside follows lies inside Rk singly connected Note employ formalism discussed based discriminant functions use simpler equivalent formulation described Section based single discriminant function explore approaches learning parameters linear discriminant based linear perceptron squares classification Chapter considered models linear functions saw minimization sum-of-squares error function led simple closed-form solution parameter tempting apply formalism classification Consider general classification problem 1-of-K binary coding scheme target vector justification squares context approximates conditional expectation target values input binary coding conditional expectation vector posterior class probabilities typically approximated approximations values outside range limited flexibility linear model shall class Ck described linear model wTk wk0 conveniently group vector notation Discriminant Functions matrix kth column comprises 1-dimensional vector corresponding augmented input vector dummy input representation discussed Section new input assigned class output yk determine parameter matrix minimizing sum-of-squares error did regression Chapter Consider training data set define matrix nth row vector tTn matrix nth row sum-of-squares error function written Tr Setting derivative respect obtain solution form pseudo-inverse matrix discussed Section obtain discriminant function form TT interesting property least-squares solutions multiple target variables target vector training set satisfies linear constraint aTtn constants model prediction value satisfy constraint thatExercise use 1-of-K coding scheme predictions model property elements sum value summation constraint sufficient allow model outputs interpreted probabilities constrained lie interval least-squares approach gives exact closed-form solution discriminant function discriminant function use make decisions directly dispense probabilistic suffers severe seen least-squares solutionsSection lack robustness applies equally classification illustrated Figure additional data points righthand figure produce significant change location decision point correctly classified original decision boundary left-hand sum-of-squares error function penalizes predictions lie long way correct decision LINEAR MODELS CLASSIFICATION Figure left plot shows data denoted red crosses blue decision boundary squares logistic regression model discussed later Section right-hand plot shows corresponding results obtained extra data points added left showing squares highly sensitive unlike logistic Section shall consider alternative error functions classification shall suffer problems squares severe simply lack illustrated Figure shows synthetic data set drawn classes two-dimensional input space having property linear decision boundaries excellent separation technique logistic described later gives satisfactory solution seen right-hand least-squares solution gives poor small region input space assigned green failure squares surprise recall corresponds maximum likelihood assumption Gaussian conditional binary target vectors clearly distribution far adopting appropriate probabilistic shall obtain classification techniques better properties continue explore alternative nonprobabilistic methods setting parameters linear classification linear discriminant way view linear classification model terms dimensionality Consider case suppose Discriminant Functions Figure Example synthetic data set comprising training data points denoted red green blue Lines denote decision background colours denote respective classes decision left result least-squares region input space assigned green class small points class right result logistic regressions described Section showing correct classification training dimensional input vector project dimension place threshold classify class class obtain standard linear classifier discussed previous projection dimension leads considerable loss classes separated original D-dimensional space strongly overlapping adjusting components weight vector select projection maximizes class begin consider two-class problem points class points class mean vectors classes simplest measure separation projected separation projected class suggests choose maximize mk wTmk LINEAR MODELS CLASSIFICATION Figure left plot shows samples classes red histograms resulting projection line joining class Note considerable class overlap projected right plot shows corresponding projection based Fisher linear showing greatly improved class mean projected data class expression arbitrarily large simply increasing magnitude solve constrain unit Lagrange multiplier perform constrained thatAppendix problem illustratedExercise Figure shows classes separated original twodimensional space considerable overlap projected line joining difficulty arises strongly nondiagonal covariances class idea proposed Fisher maximize function large separation projected class means giving small variance minimizing class projection formula transforms set labelled data points labelled set one-dimensional space within-class variance transformed data class Ck s2k yn define total within-class variance data set simply s21 Fisher criterion defined ratio between-class variance within-class variance s21 s22 make dependence explicit rewrite Fisher criterion formExercise Discriminant Functions wTSBw wTSWw SB between-class covariance matrix SB SW total within-class covariance SW Differentiating respect maximized SBw direction care magnitude drop scalar factors Multiplying sides obtain Note within-class covariance SW proportional unit proportional difference class discussed result known linear strictly discriminant specific choice direction projection data projected data subsequently used construct choosing threshold classify new point belonging classify belonging model class-conditional densities Gaussian distributions use techniques Section parameters Gaussian distributions maximum Having Gaussian approximations projected formalism Section gives expression optimal justification Gaussian assumption comes central limit theorem noting wTx sum set random Relation squares least-squares approach determination linear discriminant based goal making model predictions close possible set target Fisher criterion derived requiring maximum class separation output interesting relationship shall two-class Fisher criterion obtained special case far considered 1-of-K coding target adopt slightly different target coding least-squares solution LINEAR MODELS CLASSIFICATION weights equivalent Fisher solution shall targets class number patterns class total number target value approximates reciprocal prior probability class class shall targets number patterns class sum-of-squares error function written wTxn Setting derivatives respect obtain respectively wTxn wTxn xn making use choice target coding scheme obtain expression bias form used mean total data set xn straightforward making use choice second equation becomesExercise SW N1N2 SB SW defined SB defined substituted bias note SBw direction write ignored irrelevant scale weight vector coincides Fisher expression bias value tells new vector classified belonging class class Discriminant Functions discriminant multiple classes consider generalization Fisher discriminant shall assume dimensionality input space greater number introduce linear yk wTk feature values conveniently grouped form vector weight vectors considered columns matrix Note including bias parameters definition generalization within-class covariance matrix case classes follows SW Sk Sk mk Nk xn Nk number patterns class order generalization between-class covariance follow Duda Hart consider total covariance matrix ST mean total data set xn Nkmk Nk total number data total covariance matrix decomposed sum within-class covariance plus additional matrix identify measure between-class covariance ST SW SB SB LINEAR MODELS CLASSIFICATION covariance matrices defined original define similar matrices projected y-space sW sB Nk wish construct scalar large between-class covariance large within-class covariance possible choices criterion example Tr sB criterion rewritten explicit function projection matrix form Tr Maximization criteria somewhat discussed length Fukunaga weight values determined eigenvectors SB correspond largest important result common worth note SB composed sum outer product vectors rank matrices independent result constraint SB rank equal nonzero shows projection subspace spanned eigenvectors SB does alter value unable linear means perceptron algorithm example linear discriminant model perceptron Rosenblatt occupies important place history pattern recognition corresponds two-class model input vector transformed fixed nonlinear transformation feature vector used construct generalized linear model form Discriminant Functions nonlinear activation function step function form vector typically include bias component earlier discussions two-class classification focussed target coding scheme appropriate context probabilistic convenient use target values class class matches choice activation algorithm used determine parameters perceptron easily motivated error function natural choice error function total number misclassified does lead simple learning algorithm error piecewise constant function discontinuities change causes decision boundary data Methods based changing gradient error function gradient zero consider alternative error function known perceptron derive note seeking weight vector patterns xn class patterns xn class target coding scheme follows like patterns satisfy perceptron criterion associates zero error pattern correctly misclassified pattern xn tries minimize quantity perceptron criterion Frank Rosenblatt perceptron played important role history machine Rosenblatt simulated perceptron IBM Cornell early 1960s built special-purpose hardware provided parallel implementation perceptron ideas encapsulated Perceptrons Theory Brain published work criticized Marvin objections published book co-authored Seymour book widely misinterpreted time showing neural networks fatally flawed learn solutions linearly separable proved limitations case single-layer networks perceptron merely conjectured applied general network book contributed substantial decline research funding neural situation reversed applications neural networks widespread examples areas handwriting recognition information retrieval used routinely millions LINEAR MODELS CLASSIFICATION denotes set misclassified contribution error associated particular misclassified pattern linear function regions space pattern misclassified zero regions correctly total error function piecewise apply stochastic gradient descent algorithm error change weight vector learning rate parameter integer indexes steps perceptron function unchanged multiply set learning rate parameter equal Note weight vector evolves set patterns misclassified perceptron learning algorithm simple cycle training patterns pattern xn evaluate perceptron function pattern correctly weight vector remains incorrectly class add vector current estimate weight vector class subtract vector perceptron learning algorithm illustrated Figure consider effect single update perceptron learning contribution error misclassified pattern reduced set use does imply contribution error function misclassified patterns change weight vector caused previously correctly classified patterns perceptron learning rule guaranteed reduce total error function perceptron convergence theorem states exists exact solution training data set linearly perceptron learning algorithm guaranteed exact solution finite number Proofs theorem example Rosenblatt Block Nilsson Minsky Papert Hertz et Bishop number steps required achieve convergence convergence able distinguish nonseparable problem simply slow data set linearly depend initialization parameters order presentation data data sets linearly perceptron learning algorithm Discriminant Functions Figure Illustration convergence perceptron learning showing data points classes two-dimensional feature space left plot shows initial parameter vector shown black arrow corresponding decision boundary arrow points decision region classified belonging red data point circled green misclassified feature vector added current weight giving new decision boundary shown right left plot shows misclassified point indicated green feature vector added weight vector giving decision boundary shown right plot data points correctly LINEAR MODELS CLASSIFICATION Figure Illustration Mark perceptron photograph left shows inputs obtained simple camera input case printed illuminated powerful image focussed array cadmium sulphide giving primitive pixel perceptron patch shown middle allowed different configurations input features wired random demonstrate ability perceptron learn need precise contrast modern digital photograph right shows racks adaptive weight implemented rotary variable called driven electric motor allowing value weight adjusted automatically learning Aside difficulties learning perceptron does provide probabilistic does generalize readily important arises fact common models discussed chapter previous based linear combinations fixed basis detailed discussions limitations perceptrons Minsky Papert Bishop Analogue hardware implementations perceptron built based motor-driven variable resistors implement adaptive parameters wj illustrated Figure inputs obtained simple camera based array basis functions chosen variety example based simple fixed functions randomly chosen subsets pixels input Typical applications involved learning discriminate simple shapes time perceptron closely related called short linear explored Widrow functional form model different approach training adopted Widrow Probabilistic Generative Models turn probabilistic view classification models linear decision boundaries arise simple assumptions distribution Section discussed distinction discriminative generative approaches shall adopt generative Probabilistic Generative Models Figure Plot logistic sigmoid function defined shown scaled probit function shown dashed defined scaling factor chosen derivatives curves equal approach model class-conditional densities class priors use compute posterior probabilities Consider case posterior probability class written defined logistic sigmoid function defined plotted Figure term means type function called maps real axis finite logistic sigmoid encountered earlier chapters plays important role classification satisfies following symmetry property easily inverse logistic sigmoid known logit represents log ratio probabilities known log LINEAR MODELS CLASSIFICATION Note simply rewritten posterior probabilities equivalent appearance logistic sigmoid significance provided takes simple functional shall shortly consider situations linear function case posterior probability governed generalized linear case known normalized exponential regarded multiclass generalization logistic quantities ak defined ak normalized exponential known softmax represents smoothed version function ak aj investigate consequences choosing specific forms classconditional looking continuous input variables discussing briefly case discrete Continuous inputs Let assume class-conditional densities Gaussian explore resulting form posterior start shall assume classes share covariance density class Ck exp Consider case defined quadratic terms exponents Gaussian densities cancelled assumption common covariance leading linear function argument logistic result illustrated case two-dimensional input space Figure resulting Probabilistic Generative Models Figure left-hand plot shows class-conditional densities denoted red right corresponding posterior probability logistic sigmoid linear function surface right-hand plot coloured proportion red ink proportion blue ink decision boundaries correspond surfaces posterior probabilities constant linear functions decision boundaries linear input prior probabilities enter bias parameter changes priors effect making parallel shifts decision boundary generally parallel contours constant posterior general case classes wTk wk0 defined wk wk0 linear functions consequence cancellation quadratic terms shared resulting decision corresponding minimum misclassification occur posterior probabilities defined linear functions generalized linear relax assumption shared covariance matrix allow classconditional density covariance matrix earlier cancellations longer obtain quadratic functions giving rise quadratic linear quadratic decision boundaries illustrated Figure LINEAR MODELS CLASSIFICATION Figure left-hand plot shows class-conditional densities classes having Gaussian coloured red green classes covariance right-hand plot shows corresponding posterior RGB colour vector represents posterior probabilities respective decision boundaries Notice boundary red green covariance pairs classes Maximum likelihood solution specified parametric functional form class-conditional densities determine values prior class probabilities maximum requires data set comprising observations corresponding class Consider case having Gaussian class-conditional density shared covariance suppose data set denotes class denotes class denote prior class probability data point xn class Similarly class likelihood function convenient maximize log likelihood Consider maximization respect terms Probabilistic Generative Models log likelihood function depend Setting derivative respect equal zero obtain denotes total number data points class denotes total number data points class maximum likelihood estimate simply fraction points class result easily generalized multiclass case maximum likelihood estimate prior probability associated class Ck fraction training set points assigned consider maximization respect pick log likelihood function terms depend giving lnN Setting derivative respect zero obtain tnxn simply mean input vectors xn assigned class similar corresponding result mean input vectors xn assigned class consider maximum likelihood solution shared covariance matrix Picking terms log likelihood function depend Tr LINEAR MODELS CLASSIFICATION defined standard result maximum likelihood solution Gaussian represents weighted average covariance matrices associated classes result easily extended class problem obtain corresponding maximum likelihood solutions parameters class-conditional density Gaussian shared covariance Note approach fittingExercise Gaussian distributions classes robust maximum likelihood estimation Gaussian Discrete features Let consider case discrete feature values begin looking binary feature values xi discuss extension general discrete features general distribution correspond table 2D numbers containing 2D independent variables summation grows exponentially number seek restricted make naive Bayes assumption feature values areSection treated conditioned class class-conditional distributions form contain independent parameters Substituting gives linear functions input values case alternatively consider logistic sigmoid formulation Analogous results obtained discrete variables Exponential family Gaussian distributed discrete posterior class probabilities generalized linear models logistic sigmoid Probabilistic Discriminative Models softmax activation particular cases general result obtained assuming class-conditional densities members exponential family form members exponential distribution written form exp restrict attention subclass distributions make use introduce scaling parameter obtain restricted set exponential family class-conditional densities form exp Note allowing class parameter vector assuming classes share scale parameter two-class substitute expression class-conditional densities posterior class probability logistic sigmoid acting linear function K-class substitute class-conditional density expression linear function Probabilistic Discriminative Models two-class classification seen posterior probability class written logistic sigmoid acting linear function wide choice class-conditional distributions multiclass posterior probability class Ck softmax transformation linear function specific choices class-conditional densities used maximum likelihood determine parameters densities class priors used theorem posterior class alternative approach use functional form generalized linear model explicitly determine parameters directly maximum shall efficient algorithm finding solutions known iterative reweighted indirect approach finding parameters generalized linear fitting class-conditional densities class priors separately applying LINEAR MODELS CLASSIFICATION Figure Illustration role nonlinear basis functions linear classification left plot shows original input space data points classes labelled red basis functions defined space centres shown green crosses contours shown green right-hand plot shows corresponding feature space linear decision boundary obtained logistic regression model form discussed Section corresponds nonlinear decision boundary original input shown black curve left-hand represents example generative model generate synthetic data drawing values marginal distribution direct maximizing likelihood function defined conditional distribution represents form discriminative advantage discriminative approach typically fewer adaptive parameters shall lead improved predictive particularly class-conditional density assumptions poor approximation true Fixed basis functions far considered classification models work directly original input vector algorithms equally applicable make fixed nonlinear transformation inputs vector basis functions resulting decision boundaries linear feature space correspond nonlinear decision boundaries original illustrated Figure Classes linearly separable feature space need linearly separable original observation space Note discussion linear models Probabilistic Discriminative Models basis functions typically set say corresponding parameter plays role remainder shall include fixed basis function transformation highlight useful similarities regression models discussed Chapter problems practical significant overlap class-conditional densities corresponds posterior probabilities values optimal solution obtained modelling posterior probabilities accurately applying standard decision discussed Chapter Note nonlinear transformations remove class increase level create overlap existed original observation suitable choices nonlinearity make process modelling posterior probabilities fixed basis function models important beSection resolved later chapters allowing basis functions adapt Notwithstanding models fixed nonlinear basis functions play important role discussion models introduce key concepts needed understanding complex Logistic regression begin treatment generalized linear models considering problem two-class discussion generative approaches Section saw general posterior probability class written logistic sigmoid acting linear function feature vector logistic sigmoid function defined terminology model known logistic emphasized model classification -dimensional feature space model adjustable fitted Gaussian class conditional densities maximum used 2M parameters means parameters covariance class prior gives total grows quadratically contrast linear dependence number parameters logistic large values clear advantage working logistic regression model use maximum likelihood determine parameters logistic regression shall make use derivative logistic sigmoid conveniently expressed terms sigmoid function itselfExercise da LINEAR MODELS CLASSIFICATION data set likelihood function written ytnn yn define error function taking negative logarithm gives crossentropy error function form yn yn Taking gradient error function respect obtainExercise use factor involving derivative logistic sigmoid leading simplified form gradient log contribution gradient data point yn target value prediction times basis function vector comparison shows takes precisely form gradient sum-of-squares error function linear regression make use result sequential algorithm patterns presented weight vectors updated nth term worth noting maximum likelihood exhibit severe over-fitting data sets linearly arises maximum likelihood solution occurs hyperplane corresponding equivalent separates classes magnitude goes logistic sigmoid function infinitely steep feature corresponding Heaviside step training point class assigned posterior probability typically continuumExercise solutions separating hyperplane rise posterior probabilities training data seen later Figure Maximum likelihood provides way favour solution solution practice depend choice optimization algorithm parameter Note problem arise number data points large compared number parameters long training data set linearly singularity avoided inclusion prior finding MAP solution equivalently adding regularization term error Probabilistic Discriminative Models Iterative reweighted squares case linear regression models discussed Chapter maximum likelihood assumption Gaussian noise leads closed-form consequence quadratic dependence log likelihood function parameter vector logistic longer closed-form nonlinearity logistic sigmoid departure quadratic form error function shall unique error function minimized efficient iterative technique based Newton-Raphson iterative optimization uses local quadratic approximation log likelihood Newton-Raphson minimizing function takes form Bishop Hessian matrix elements comprise second derivatives respect components Let apply Newton-Raphson method linear regression model sum-of-squares error function gradient Hessian error function design nth row Newton-Section Raphson update takes form recognize standard least-squares Note error function case quadratic Newton-Raphson formula gives exact solution let apply Newton-Raphson update cross-entropy error function logistic regression gradient Hessian error function LINEAR MODELS CLASSIFICATION use introduced diagonal matrix elements Rnn Hessian longer constant depends weighting matrix corresponding fact error function longer property yn follows form logistic sigmoid uTHu arbitrary vector Hessian matrix positive follows error function concave function unique Newton-Raphson update formula logistic regression model -dimensional vector elements update formula takes form set normal equations weighted least-squares weighing matrix constant depends parameter vector apply normal equations time new weight vector compute revised weighing matrix algorithm known iterative reweighted IRLS weighted least-squares elements diagonal weighting matrix interpreted variances mean variance logistic regression model used property interpret IRLS solution linearized problem space variable quantity corresponds nth element simple interpretation effective target value space obtained making local linear approximation logistic sigmoid function current operating point dandyn Probabilistic Discriminative Models Multiclass logistic regression discussion generative models multiclass haveSection seen large class posterior probabilities softmax transformation linear functions feature ak ak wTk used maximum likelihood determine separately class-conditional densities class priors corresponding posterior probabilities implicitly determining parameters consider use maximum likelihood determine parameters model require derivatives yk respect activations aj byExercise Ikj elements identity write likelihood easily 1-of-K coding scheme target vector feature vector belonging class Ck binary vector elements zero element equals likelihood function ytnknk ynk matrix target variables elements Taking negative logarithm gives tnk ynk known cross-entropy error function multiclass classification gradient error function respect parameter vectors wj Making use result derivatives softmax obtainExercise LINEAR MODELS CLASSIFICATION use tnk form arising gradient sum-of-squares error function linear model cross-entropy error logistic regression product error times basis function use formulate sequential algorithm patterns presented weight vectors updated seen derivative log likelihood function linear regression model respect parameter vector data point took form yn times feature vector combination logistic sigmoid activation function cross-entropy error function softmax activation function multiclass cross-entropy error function obtain simple example general shall Section batch appeal Newton-Raphson update obtain corresponding IRLS algorithm multiclass requires evaluation Hessian matrix comprises blocks size block two-class Hessian matrix multiclass logistic regression model positive definite error function unique Practical details IRLS multiclass case Bishop Nabney Probit regression seen broad range class-conditional described exponential resulting posterior class probabilities logistic transformation acting linear function feature choices class-conditional density rise simple form posterior probabilities class-conditional densities modelled Gaussian suggests worth exploring types discriminative probabilistic purposes shall return two-class remain framework generalized linear models activation way motivate alternative choice link function consider noisy threshold input evaluate set target value according Probabilistic Discriminative Models Figure Schematic example probability density shown blue example mixture cumulative distribution function shown red Note value blue curve indicated vertical green corresponds slope red curve value red curve point corresponds area blue curve indicated shaded green stochastic threshold class label takes value value exceeds takes value equivalent activation function cumulative distribution function value drawn probability density corresponding activation function cumulative distribution function illustrated Figure specific suppose density zero unit variance corresponding cumulative distribution function known probit sigmoidal shape compared logistic sigmoid function Figure Note use general Gaussian distribution does change model equivalent re-scaling linear coefficients numerical packages provide evaluation closely related function defined known erf function error function confused error function machine learning related probit function byExercise generalized linear model based probit activation function known probit determine parameters model maximum straightforward extension ideas discussed results probit regression tend similar logistic LINEAR MODELS CLASSIFICATION use probit model discuss Bayesian treatments logistic regression Section issue occur practical applications arise instance errors measuring input vector mislabelling target value points lie long way wrong ideal decision seriously distort Note logistic probit regression models behave differently respect tails logistic sigmoid decay asymptotically like probit activation function decay like probit model significantly sensitive logistic probit models assume data correctly effect mislabelling easily incorporated probabilistic model introducing probability target value flipped wrong value leading target value distribution data point form activation function input vector set treated hyperparameter value inferred Canonical link functions linear regression model Gaussian noise error corresponding negative log derivative respect parameter vector contribution error function data point takes form yn times feature vector yn combination logistic sigmoid activation function cross-entropy error function softmax activation function multiclass cross-entropy error function obtain simple general result assuming conditional distribution target variable exponential corresponding choice activation function known canonical link make use restricted form exponential family Note applying assumption exponential family distribution target variable contrast Section applied input vector consider conditional distributions target variable form exp line argument led derivation result conditional mean denote Laplace Approximation denote relation Following Nelder Wedderburn define generalized linear model nonlinear function linear combination input variables known activation function machine learning known link function consider log likelihood function function const assuming observations share common scale parameter corresponds noise variance Gaussian distribution independent derivative log likelihood respect model parameters dyn dyn dan used yn result considerable simplification choose particular form link function gives gradient error function reduces Gaussian logistic model Laplace Approximation Section shall discuss Bayesian treatment logistic shall complex Bayesian treatment linear regression discussed Sections integrate exactly LINEAR MODELS CLASSIFICATION parameter vector posterior distribution longer necessary introduce form Later book shall consider range techniques based analytical approximationsChapter numerical introduce widely framework called Laplace aims Gaussian approximation probability density defined set continuous Consider case single continuous variable suppose distribution defined dz normalization shall suppose value Laplace method goal Gaussian approximation centred mode distribution step mode words point equivalently dz Gaussian distribution property logarithm quadratic function consider Taylor expansion centred mode dz2 Note first-order term Taylor expansion does appear local maximum Taking exponential obtain exp obtain normalized distribution making use standard result normalization exp Laplace approximation illustrated Figure Note Gaussian approximation defined precision words stationary point local second derivative point Laplace Approximation Figure Illustration Laplace approximation applied distribution logistic sigmoid function defined left plot shows normalized distribution Laplace approximation centred mode right plot shows negative logarithms corresponding extend Laplace method approximate distribution defined -dimensional space stationary point gradient Expanding stationary point Hessian matrix defined gradient Taking exponential sides obtain exp distribution proportional appropriate normalization coefficient standard result normalized multivariate giving exp denotes determinant Gaussian distribution defined provided precision positive implies stationary point local minimum saddle order apply Laplace approximation need mode evaluate Hessian matrix practice mode typically running form numerical optimization algorithm LINEAR MODELS CLASSIFICATION distributions encountered practice multimodal different Laplace approximations according mode Note normalization constant true distribution does need known order apply Laplace result central limit posterior distribution model expected increasingly better approximated Gaussian number observed data points expect Laplace approximation useful situations number data points relatively major weakness Laplace approximation based Gaussian directly applicable real cases possible apply Laplace approximation transformation instance consider Laplace approximation limitation Laplace based purely aspects true distribution specific value fail capture important global Chapter shall consider alternative approaches adopt global Model comparison BIC approximating distribution obtain approximation normalization constant approximation dz exp dz noted integrand Gaussian use standard result normalized Gaussian use result obtain approximation model evidence discussed Section plays central role Bayesian model Consider data set set models having parameters model define likelihood function introduce prior interested computing model evidence various omit conditioning Mi notation theorem model evidence Identifying applying result obtainExercise Occam factor Bayesian Logistic Regression value mode posterior Hessian matrix second derivatives negative log posterior term right hand represents log likelihood evaluated optimized remaining terms comprise penalizes model assume Gaussian prior distribution parameters Hessian approximate roughly usingExercise 12M lnN number data number parameters omitted additive known Bayesian Information Criterion Schwarz criterion Note compared AIC penalizes model complexity Complexity measures AIC BIC virtue easy misleading assumption Hessian matrix rank valid parameters use result obtain accurate estimateSection model evidence starting Laplace illustrate context neural networks Section Bayesian Logistic Regression turn Bayesian treatment logistic Exact Bayesian inference logistic regression evaluation posterior distribution require normalization product prior distribution likelihood function comprises product logistic sigmoid data Evaluation predictive distribution similarly consider application Laplace approximation problem Bayesian logistic regression Laplace approximation Recall Section Laplace approximation obtained finding mode posterior distribution fitting Gaussian centred requires evaluation second derivatives log equivalent finding Hessian seek Gaussian representation posterior natural begin Gaussian write general form LINEAR MODELS CLASSIFICATION fixed posterior distribution Taking log substituting prior distribution likelihood function obtain yn const yn obtain Gaussian approximation posterior maximize posterior distribution MAP solution defines mean covariance inverse matrix second derivatives negative log takes form SN Gaussian approximation posterior distribution takes form Having obtained Gaussian approximation posterior remains task marginalizing respect distribution order make Predictive distribution predictive distribution class new feature vector obtained marginalizing respect posterior distribution approximated Gaussian distribution dw dw corresponding probability class evaluate predictive note function depends projection Denoting da Dirac delta dw da Bayesian Logistic Regression evaluate noting delta function imposes linear constraint forms marginal distribution joint distribution integrating directions orthogonal know Section marginal distribution evaluate mean covariance distribution taking interchanging order integration used result variational posterior distribution Similarly da dw Note distribution takes form predictive distribution linear regression noise variance set variational approximation predictive distribution da result derived directly making use results marginal Gaussian distribution Section integral represents convolution Gaussian logistic evaluated obtain good approximation Barber making use close similarity logistic sigmoid function defined probit function defined order obtain best approximation logistic function need re-scale horizontal approximate suitable value requiring functions slope gives similarity logistic sigmoid probit thisExercise choice illustrated Figure advantage probit function convolution Gaussian expressed analytically terms probit Specifically thatExercise da LINEAR MODELS CLASSIFICATION apply approximation probit functions appearing sides leading following approximation convolution logistic sigmoid da defined Applying result obtain approximate predictive distribution form defined defined Note decision boundary corresponding decision boundary obtained MAP value decision criterion based minimizing misclassification equal prior marginalization complex decision criteria play important Marginalization logistic sigmoid model Gaussian approximation posterior distribution illustrated context variational inference Figure Exercises set data points define convex hull set points Consider second set points corresponding convex sets points linearly separable exists vector scalar convex hulls sets points linearly conversely linearly convex hulls www Consider minimization sum-of-squares error function suppose target vectors training set satisfy linear constraint aTtn corresponds nth row matrix consequence elements model prediction least-squares solution satisfy Exercises assume basis functions corresponding parameter plays role Extend result Exercise multiple linear constraints satisfied simultaneously target constraints satisfied least-squares prediction linear www maximization class separation criterion respect Lagrange multiplier enforce constraint wTw leads result making use Fisher criterion written form definitions between-class within-class covariance matrices choice target values described Section expression minimizes sum-of-squares error function written form www logistic sigmoid function satisfies property inverse derive result posterior class probability two-class generative model Gaussian verify results parameters www Consider generative classification model classes defined prior class probabilities general class-conditional densities input feature Suppose training data set binary target vector length uses 1-ofK coding components tnj Ijk pattern class Assuming data points drawn independently maximum-likelihood solution prior probabilities Nk Nk number data points assigned class Consider classification model Exercise suppose class-conditional densities Gaussian distributions shared covariance maximum likelihood solution mean Gaussian distribution class Ck Nk LINEAR MODELS CLASSIFICATION represents mean feature vectors assigned class maximum likelihood solution shared covariance matrix Nk Sk Sk Nk weighted average covariances data associated weighting coefficients prior probabilities Consider classification problem classes feature vector components discrete Let values components represented 1-of-L binary coding suppose conditioned class components class-conditional density factorizes respect feature vector quantities ak appear argument softmax function describing posterior class linear functions components Note represents example naive Bayes model discussed Section www Verify relation derivative logistic sigmoid function defined www making use result derivative logistic derivative error function logistic regression model linearly separable data maximum likelihood solution logistic regression model obtained finding vector decision boundary separates classes taking magnitude Hessian matrix logistic regression positive diagonal matrix elements yn output logistic regression model input vector error function concave function unique Consider binary classification problem observation xn known belong corresponding suppose procedure collecting training data training points data point instead having value class instead value representing probability probabilistic model write log likelihood function appropriate data Exercises www derivatives softmax activation function ak defined result derivatives softmax activation gradients cross-entropy error www Write expressions gradient log corresponding Hessian probit regression model defined Section quantities required train model Hessian matrix multiclass logistic regression defined positive Note Hessian matrix problem size MK number parameters number prove positive semidefinite consider product uTHu arbitrary vector length apply probit function erf function related result derive expression log model evidence Laplace www derive BIC result starting Laplace approximation model evidence prior parameters Gaussian form log model evidence Laplace approximation takes form const matrix second derivatives log likelihood evaluated assume prior broad small second term right-hand consider case identically distributed data sum terms data log model evidence written approximately form BIC expression Use results Section derive result marginalization logistic regression model respect Gaussian posterior distribution parameters Suppose wish approximate logistic sigmoid defined scaled probit function defined chosen derivatives functions equal LINEAR MODELS CLASSIFICATION prove relation convolution probit function Gaussian derivative lefthand respect equal derivative right-hand integrate sides respect constant integration Note differentiating left-hand convenient introduce change variable integral replaced integral differentiate left-hand relation obtain Gaussian integral evaluated 
Neural Networks Chapters considered models regression classification comprised linear combinations fixed basis saw models useful analytical computational properties practical applicability limited curse order apply models largescale necessary adapt basis functions Support vector machines discussed Chapter address defining basis functions centred training data points selecting subset advantage SVMs training involves nonlinear objective function solution optimization problem relatively number basis functions resulting models generally smaller number training relatively large typically increases size training relevance vector discussed Section chooses subset fixed set basis functions typically results NEURAL NETWORKS sparser Unlike SVM produces probabilistic expense nonconvex optimization alternative approach fix number basis functions advance allow words use parametric forms basis functions parameter values adapted successful model type context pattern recognition feed-forward neural known multilayer discussed really model comprises multiple layers logistic regression models continuous multiple perceptrons discontinuous resulting model significantly faster support vector machine having generalization price paid relevance vector likelihood forms basis network longer convex function model worth investing substantial computational resources training phase order obtain compact model fast processing new term origins attempts mathematical representations information processing biological systems Widrow Rumelhart et used broadly cover wide range different subject exaggerated claims regarding biological perspective practical applications pattern biological realism impose entirely unnecessary focus chapter neural networks efficient models statistical pattern shall restrict attention specific class neural networks proven greatest practical multilayer begin considering functional form network including specific parameterization basis discuss problem determining network parameters maximum likelihood involves solution nonlinear optimization requires evaluation derivatives log likelihood function respect network shall obtained efficiently technique error shall backpropagation framework extended allow derivatives Jacobian Hessian discuss various approaches regularization neural network training relationships consider extensions neural network particular general framework modelling conditional probability distributions known mixture density discuss use Bayesian treatments neural Additional background neural network models Bishop Feed-forward Network Functions Feed-forward Network Functions linear models regression classification discussed Chapters based linear combinations fixed nonlinear basis functions form nonlinear activation function case classification identity case goal extend model making basis functions depend parameters allow parameters coefficients ways construct parametric nonlinear basis Neural networks use basis functions follow form basis function nonlinear function linear combination coefficients linear combination adaptive leads basic neural network described series functional construct linear combinations input variables xD form aj ji xi superscript indicates corresponding parameters shall refer parameters weights parameters following nomenclature Chapter quantities aj known transformed nonlinear activation function zj quantities correspond outputs basis functions context neural called hidden nonlinear functions generally chosen sigmoidal functions logistic sigmoid Following values linearly combined outputExercise unit activations ak kj zj total number transformation corresponds second layer bias output unit activations transformed appropriate activation function set network outputs choice activation function determined nature data assumed distribution target variables NEURAL NETWORKS Figure Network diagram twolayer neural network corresponding output variables represented weight parameters represented links bias parameters denoted links coming additional input hidden variables Arrows denote direction information flow network forward xD zM yK MD KM hidden units inputs outputs follows considerations linear models discussed Chapters standard regression activation function identity yk multiple binary classification output unit activation transformed logistic sigmoid function yk multiclass softmax activation function form choice output unit activation function discussed Section combine various stages overall network function sigmoidal output unit activation takes form kj ji xi set weight bias parameters grouped vector neural network model simply nonlinear function set input variables set output variables controlled vector adjustable function represented form network diagram shown Figure process evaluating interpreted forward propagation information emphasized diagrams represent probabilistic graphical models kind considered Chapter internal nodes represent deterministic variables stochastic adopted slightly different graphical Feed-forward Network Functions notation kinds shall later probabilistic interpretation neural discussed Section bias parameters absorbed set weight parameters defining additional input variable value clamped takes form aj ji similarly absorb second-layer biases second-layer overall network function kj ji xi seen Figure neural network model comprises stages resembles perceptron model Section reason neural network known multilayer key difference compared neural network uses continuous sigmoidal nonlinearities hidden perceptron uses step-function means neural network function differentiable respect network property play central role network activation functions hidden units network taken network equivalent network hidden follows fact composition successive linear transformations linear number hidden units smaller number input output transformations network generate general possible linear transformations inputs outputs information lost dimensionality reduction hidden Section networks linear units rise principal component little multilayer networks linear network architecture shown Figure commonly used easily instance considering additional layers processing consisting weighted linear combination form followed element-wise transformation nonlinear activation Note confusion literature regarding terminology counting number layers network Figure described 3-layer network counts number layers treats inputs single-hidden-layer network counts number layers hidden recommend terminology Figure called two-layer number layers adaptive weights important determining network generalization network architecture include skip-layer associated corresponding adaptive NEURAL NETWORKS Figure Example neural network having general feed-forward Note hidden output unit associated bias parameter inputs outputs two-layer network directly inputs network sigmoidal hidden units mimic skip layer connections bounded input sufficiently small first-layer weight operating hidden unit effectively compensating large weight value hidden unit advantageous include skip-layer connections network possible connections layer shall example sparse network architecture consider convolutional neural networks Section direct correspondence network diagram mathematical develop general network mappings considering complex network restricted feed-forward words having closed directed ensure outputs deterministic functions illustrated simple example Figure unit network computes function zk wkjzj sum runs units send connections unit bias parameter included set values applied inputs successive application allows activations units network evaluated including output approximation properties feed-forward networks widely studied Hornik et Stinchecombe Neural networks said universal two-layer network linear outputs uniformly approximate continuous function compact input domain arbitrary accuracy provided network sufficiently large number hidden result holds wide range hidden unit activation excluding theorems key problem suitable parameter values set training later sections chapter Feed-forward Network Functions Figure Illustration capability multilayer perceptron approximate different functions comprising Heaviside step data shown blue sampled uniformly interval corresponding values data points used train twolayer network having hidden units activation functions linear output resulting network functions shown red outputs hidden units shown dashed exist effective solutions problem based maximum likelihood Bayesian capability two-layer network model broad range functions illustrated Figure figure shows individual hidden units work collaboratively approximate final role hidden units simple classification problem illustrated Figure synthetic classification data set described Appendix Weight-space symmetries property feed-forward play role consider Bayesian model multiple distinct choices weight vector rise mapping function inputs outputs et Consider two-layer network form shown Figure hidden units having activation functions connectivity change sign weights bias feeding particular hidden input sign activation hidden unit odd transformation exactly compensated changing sign weights leading hidden changing signs particular group weights mapping function represented network different weight vectors rise mapping hidden NEURAL NETWORKS Figure Example solution simple twoclass classification problem involving synthetic data neural network having hidden units activation single output having logistic sigmoid activation dashed blue lines contours hidden red line shows decision surface green line denotes optimal decision boundary computed distributions used generate weight vector set 2M equivalent weight vectors imagine interchange values weights leading particular hidden unit corresponding values weights associated different hidden clearly leaves network mapping function corresponds different choice weight hidden weight vector belong set equivalent weight vectors associated interchange corresponding different orderings hidden network overall weight-space symmetry factor networks layers total level symmetry product layer hidden turns factors account symmetries weight space possible accidental symmetries specific choices weight existence symmetries particular property function applies wide range activation functions symmetries weight space little practical Section shall encounter situation need Network Training viewed neural networks general class parametric nonlinear functions vector input variables vector output simple approach problem determining network parameters make analogy discussion polynomial curve fitting Section minimize sum-of-squares error training set comprising set input vectors corresponding set Network Training target vectors minimize error function provide general view network training giving probabilistic interpretation network seen advantages probabilistic predictions Section provide clearer motivation choice output unit nonlinearity choice error start discussing regression moment consider single target variable real Following discussions Section assume Gaussian distribution xdependent output neural precision Gaussian course somewhat restrictive Section shall extend approach allow general conditional conditional distribution sufficient output unit activation function network approximate continuous function data set identically distributed observations corresponding target values construct corresponding likelihood function Taking negative obtain error function used learn parameters Section shall discuss Bayesian treatment neural consider maximum likelihood Note neural networks usual consider minimization error function maximization shall follow Consider determination Maximizing likelihood function equivalent minimizing sum-of-squares error function NEURAL NETWORKS discarded additive multiplicative value minimizing denoted wML corresponds maximum likelihood nonlinearity network function causes error practice local maxima likelihood corresponding local minima error discussed Section Having value minimizing negative log likelihood Note evaluated iterative optimization required wML multiple target assume independent conditional shared noise precision conditional distribution target values Following argument single target maximum likelihood weights determined minimizing sum-of-squares error function noise precision byExercise NK number target assumption independence dropped expense slightly complex optimization Recall Section natural pairing error function negative log output unit activation regression view network having output activation function yk corresponding sum-of-squares error function property yk tk shall make use discussing error backpropagation Section consider case binary classification single target variable denotes class denotes class Following discussion canonical link functions Section consider network having single output activation function logistic sigmoid interpret conditional probability conditional distribution targets inputs Bernoulli distribution form Network Training consider training set independent error negative log cross-entropy error function form yn yn denotes Note analogue noise precision target values assumed correctly model easily extended allow labelling Simard et usingExercise cross-entropy error function instead sum-of-squares classification problem leads faster training improved separate binary classifications use network having outputs logistic sigmoid activation Associated output binary class label tk assume class labels input conditional distribution targets Taking negative logarithm corresponding likelihood function gives following error functionExercise ynk ynk denotes derivative error function respect activation particular output unit takes form just theExercise regression interesting contrast neural network solution problem corresponding approach based linear classification model kind discussed Chapter Suppose standard two-layer network kind shown Figure weight parameters layer network shared various linear model classification problem solved layer network viewed performing nonlinear feature sharing features different outputs save computation lead improved consider standard multiclass classification problem input assigned mutually exclusive binary target variables tk 1-of-K coding scheme indicating network outputs interpreted leading following error function tkn NEURAL NETWORKS Figure Geometrical view error function surface sitting weight Point wA local minimum wB global point wC local gradient error surface vector wA wB wC Following discussion Section output unit activation corresponds canonical softmax function satisfies yk yk Note unchanged constant added causing error function constant directions weight degeneracy removed appropriate regularization term added error derivative error function respect activation particular output unit takes familiar form natural choice output unit activation function matching error according type problem regression use linear outputs sum-of-squares binary classifications use logistic sigmoid outputs cross-entropy error multiclass classification use softmax outputs corresponding multiclass cross-entropy error classification problems involving use single logistic sigmoid alternatively use network outputs having softmax output activation Parameter optimization turn task finding weight vector minimizes chosen function useful geometrical picture error view surface sitting weight space shown Figure note make small step weight space change error function points direction greatest rate increase error error smooth continuous function smallest value occur Network Training point weight space gradient error function make small step direction reduce Points gradient vanishes called stationary classified saddle goal vector takes smallest error function typically highly nonlinear dependence weights bias points weight space gradient vanishes numerically discussion Section point local points weight space equivalent two-layer network kind shown Figure hidden point weight space member family equivalent typically multiple inequivalent stationary points particular multiple inequivalent minimum corresponds smallest value error function weight vector said global minima corresponding higher values error function said local successful application neural necessary global minimum general known global minimum necessary compare local minima order sufficiently good clearly hope finding analytical solution equation resort iterative numerical optimization continuous nonlinear functions widely studied problem exists extensive literature solve techniques involve choosing initial value weight vector moving weight space succession steps form labels iteration Different algorithms involve different choices weight vector update algorithms make use gradient information require value evaluated new weight vector order understand importance gradient useful consider local approximation error function based Taylor Local quadratic approximation Insight optimization various techniques solving obtained considering local quadratic approximation error Consider Taylor expansion point weight space NEURAL NETWORKS cubic higher terms defined gradient evaluated Hessian matrix elements corresponding local approximation gradient points sufficiently close expressions reasonable approximations error Consider particular case local quadratic approximation point minimum error case linear Hessian evaluated order interpret consider eigenvalue equation Hessian matrix Hui eigenvectors ui form complete orthonormal set uTi uj expand linear combination eigenvectors form regarded transformation coordinate origin translated point axes rotated align eigenvectors orthogonal matrix columns discussed Appendix Substituting allows error function written form matrix said positive definite vTHv Network Training Figure neighbourhood minimum error function approximated Contours constant error ellipses axes aligned eigenvectors ui Hessian lengths inversely proportional square roots corresponding eigenvectors eigenvectors form complete arbitrary vector written form vTHv c2i positive definite eigenvalues new coordinate basis vectors eigenvectors contours constant ellipses centred illustratedExercise Figure one-dimensional weight stationary point minimum corresponding result D-dimensions Hessian evaluated positive Use gradient information shall Section possible evaluate gradient error function efficiently means backpropagation use gradient information lead significant improvements speed minima error function quadratic approximation error error surface specified quantities contain total independent elements matrix theExercise dimensionality total number adaptive parameters location minimum quadratic approximation depends expect able locate minimum gathered independent pieces make use gradient expect perform function NEURAL NETWORKS require computational effort needed minimum approach compare algorithm makes use gradient evaluation brings items hope minimum function gradient shall error evaluation takes steps minimum use gradient information forms basis practical algorithms training neural Gradient descent optimization simplest approach gradient information choose weight update comprise small step direction negative parameter known learning gradient re-evaluated new weight vector process Note error function defined respect training step requires entire training set processed order evaluate Techniques use data set called batch step weight vector moved direction greatest rate decrease error approach known gradient descent steepest approach intuitively fact turns poor reasons discussed Bishop Nabney batch efficient conjugate gradients quasi-Newton robust faster simple gradient descent et Nocedal Unlike gradient algorithms property error function decreases iteration unless weight vector arrived local global order sufficiently good necessary run gradient-based algorithm multiple time different randomly chosen starting comparing resulting performance independent validation on-line version gradient descent proved useful practice training neural networks large data sets Cun et Error functions based maximum likelihood set independent observations comprise sum data point On-line gradient known sequential gradient descent stochastic gradient makes update weight vector based data point Error Backpropagation update repeated cycling data sequence selecting points random course intermediate scenarios updates based batches data advantage on-line methods compared batch methods handle redundancy data consider extreme example data set double size duplicating data Note simply multiplies error function factor equivalent original error Batch methods require double computational effort evaluate batch error function online methods property on-line gradient descent possibility escaping local stationary point respect error function data set generally stationary point data point Nonlinear optimization practical application neural network discussed Bishop Nabney Error Backpropagation goal section efficient technique evaluating gradient error function feed-forward neural shall achieved local message passing scheme information sent alternately forwards backwards network known error simply noted term backpropagation used neural computing literature mean variety different multilayer perceptron architecture called backpropagation term backpropagation used training multilayer perceptron gradient descent applied sum-of-squares error order clarify useful consider nature training process training algorithms involve iterative procedure minimization error adjustments weights sequence distinguish distinct derivatives error function respect weights shall important contribution backpropagation technique providing computationally efficient method evaluating stage errors propagated backwards shall use term backpropagation specifically evaluation second derivatives used compute adjustments simplest originally considered Rumelhart et involves gradient important recognize stages propagation errors backwards network order evaluate applied kinds network just multilayer applied error functions just simple eval242 NEURAL NETWORKS uation derivatives Jacobian Hessian shall later second stage weight adjustment calculated derivatives tackled variety optimization substantially powerful simple gradient Evaluation error-function derivatives derive backpropagation algorithm general network having arbitrary feed-forward arbitrary differentiable nonlinear activation broad class error resulting formulae illustrated simple layered network structure having single layer sigmoidal hidden units sum-of-squares error functions practical instance defined maximum likelihood set comprise sum data point training shall consider problem evaluating term error used directly sequential results accumulated training set case batch Consider simple linear model outputs yk linear combinations input variables xi yk wkixi error function particular input pattern takes form En ynk gradient error function respect weight wji interpreted computation involving product ynj tnj associated output end link wji variable xni associated input end Section saw similar formula arises logistic sigmoid activation function cross entropy error similarly softmax activation function matching cross-entropy error shall simple result extends complex setting multilayer feed-forward general feed-forward unit computes weighted sum inputs form aj wjizi Error Backpropagation zi activation sends connection unit wji weight associated Section saw biases included sum introducing extra activation fixed need deal biases sum transformed nonlinear activation function activation zj unit form zj Note variables zi sum unit pattern training shall suppose supplied corresponding input vector network calculated activations hidden output units network successive application process called forward propagation regarded forward flow information consider evaluation derivative En respect weight outputs various units depend particular input pattern order notation shall omit subscript network note En depends weight wji summed input aj unit apply chain rule partial derivatives introduce useful notation referred errors reasons shall write Substituting obtain Equation tells required derivative obtained simply multiplying value unit output end weight value unit input end weight case Note takes form simple linear model considered start order evaluate need calculate value hidden output unit apply seen output yk tk NEURAL NETWORKS Figure Illustration calculation hidden unit backpropagation units unit sends blue arrow denotes direction information flow forward red arrows indicate backward propagation error zi zj wji wkj provided canonical link output-unit activation evaluate hidden make use chain rule partial sum runs units unit sends arrangement units weights illustrated Figure Note units labelled include hidden units output writing making use fact variations aj rise variations error function variations variables substitute definition make use obtain following backpropagation formula tells value particular hidden unit obtained propagating backwards units higher illustrated Figure Note summation taken index wkj backward propagation information forward propagation equation taken second know values output follows recursively applying evaluate hidden units feed-forward regardless backpropagation procedure summarized Error Backpropagation Apply input vector xn network forward propagate network activations hidden output Evaluate output units Backpropagate obtain hidden unit Use evaluate required Error Backpropagation batch derivative total error obtained repeating steps pattern training set summing derivation implicitly assumed hidden output unit network activation function derivation easily allow different units individual activation simply keeping track form goes simple example derivation backpropagation procedure allowed general forms error activation network order illustrate application shall consider particular chosen simplicity practical applications neural networks reported literature make use type shall consider two-layer network form illustrated Figure sum-of-squares output units linear activation yk hidden units logistic sigmoid activation functions ea ea useful feature function derivative expressed particularly simple consider standard sum-of-squares error pattern error En yk activation output unit tk corresponding particular input pattern pattern training set perform forward propagation aj ji xi zj yk kj zj NEURAL NETWORKS compute output unit yk backpropagate obtain hidden units z2j derivatives respect first-layer second-layer weights ji kj Efficiency backpropagation important aspects backpropagation computational understand let examine number operations required evaluate derivatives error function scales total number weights biases single evaluation error function input require sufficiently large follows fact network sparse number weights typically greater number bulk computational effort forward propagation concerned evaluating sums evaluation activation functions representing small term sum requires multiplication leading overall computational cost alternative approach backpropagation computing derivatives error function use finite perturbing weight approximating derivatives expression software accuracy approximation derivatives improved making numerical roundoff problems accuracy finite differences method improved significantly symmetrical central differences form corrections verified Taylor expansion onExercise right-hand residual corrections number computational steps roughly doubled compared main problem numerical differentiation highly desirable scaling forward propagation requires Error Backpropagation Figure Illustration modular pattern recognition Jacobian matrix used backpropagate error signals outputs earlier modules weights network perturbed overall scaling numerical differentiation plays important role comparison derivatives calculated backpropagation obtained central differences provides powerful check correctness software implementation backpropagation training networks derivatives evaluated gives greatest accuracy numerical results compared numerical differentiation test cases order check correctness Jacobian matrix seen derivatives error function respect weights obtained propagation errors backwards technique backpropagation applied calculation consider evaluation Jacobian elements derivatives network outputs respect inputs Jki derivative evaluated inputs held Jacobian matrices play useful role systems built number distinct illustrated Figure module comprise fixed adaptive linear long Suppose wish minimize error function respect parameter Figure derivative error function Jacobian matrix red module Figure appears middle Jacobian matrix provides measure local sensitivity outputs changes input allows known errors NEURAL NETWORKS associated inputs propagated trained network order estimate contribution errors relation valid provided network mapping represented trained neural network elements Jacobian matrix constants depend particular input vector valid small perturbations Jacobian re-evaluated new input Jacobian matrix evaluated backpropagation procedure similar derived earlier evaluating derivatives error function respect start writing element Jki form Jki wji use sum runs units input unit sends connections units hidden layer layered topology considered write recursive backpropagation formula determine derivatives wlj sum runs units unit sends connections index use backpropagation starts output units required derivatives directly functional form output-unit activation individual sigmoidal activation functions output softmax outputs ykyj summarize procedure evaluating Jacobian matrix Apply input vector corresponding point input space Jacobian matrix forward propagate usual way obtain Hessian Matrix activations hidden output units row Jacobian corresponding output unit backpropagate recursive relation starting hidden units use backpropagation Jacobian evaluated alternative forward propagation derived analogous way backpropagation approach implementation algorithms checked numerical differentiation form involves 2D forward propagations network having Hessian Matrix shown technique backpropagation used obtain derivatives error function respect weights Backpropagation used evaluate second derivatives Note convenient consider weight bias parameters elements wi single denoted case second derivatives form elements Hij Hessian matrix total number weights Hessian plays important role aspects neural including nonlinear optimization algorithms used training neural networks based considerations second-order properties error controlled Hessian matrix Hessian forms basis fast procedure re-training feed-forward network following small change training data inverse Hessian used identify significant weights network network algorithms Cun et Hessian plays central role Laplace approximation Bayesian neural network Section inverse used determine predictive distribution trained eigenvalues determine values determinant used evaluate model Various approximation schemes used evaluate Hessian matrix neural Hessian calculated exactly extension backpropagation NEURAL NETWORKS important consideration applications Hessian efficiency parameters Hessian matrix dimensions computational effort needed evaluate Hessian scale like pattern data shall efficient methods evaluating Hessian scaling Diagonal approximation applications Hessian matrix discussed require inverse Hessian diagonal approximation words simply replaces off-diagonal elements inverse trivial shall consider error function consists sum pattern data Hessian obtained considering pattern summing results diagonal elements pattern written z2i second derivatives right-hand recursively chain rule differential calculus backpropagation equation form wkj neglect off-diagonal elements second-derivative obtain Le Le Cun et w2kj wkj Note number computational steps required evaluate approximation total number weight bias parameters compared Ricotti et used diagonal approximation retained terms evaluation obtained exact expressions diagonal Note longer major problem diagonal practice Hessian typically strongly driven mainly computational treated Hessian Matrix Outer product approximation neural networks applied regression common use sum-of-squares error function form considered case single output order notation simple extension outputs write theExercise Hessian matrix form network trained data outputs yn happen close target values second term small appropriate neglect term following Recall Section optimal function minimizes sum-of-squares loss conditional average target quantity random variable zero assume value uncorrelated value second derivative term right-hand term average zero summation neglecting second term arrive approximation outer product approximation Hessian matrix built sum outer products bnbTn bn activation function output units simply Evaluation outer product approximation Hessian straightforward involves derivatives error evaluated efficiently steps standard elements matrix steps simple important emphasize approximation likely valid network trained general network mapping second derivative terms right-hand typically case cross-entropy error function network logistic sigmoid output-unit activation corresponding approximation byExercise analogous result obtained multiclass networks having softmax outputunit activation NEURAL NETWORKS Inverse Hessian use outer-product approximation develop computationally efficient procedure approximating inverse Hessian write outer-product approximation matrix notation HN bnbTn bn contribution gradient output unit activation arising data point derive sequential procedure building Hessian including data points Suppose obtained inverse Hessian data separating contribution data point obtain HL order evaluate inverse consider matrix vvT unit simply special case Woodbury identity identify HL obtain data points sequentially absorbed data set result represents procedure evaluating inverse Hessian single pass data initial matrix chosen small algorithm actually finds inverse results particularly sensitive precise value Extension algorithm networks having output note Hessian matrix calculated indirectly network training quasi-Newton nonlinear optimization algorithms gradually build approximation inverse Hessian algorithms discussed Bishop Nabney Finite differences case derivatives error second derivatives finite accuracy limited numerical perturb possible pair weights obtain wlk wlk wlk wlk Hessian Matrix symmetrical central differences ensure residual errors elements Hessian evaluation element requires forward propagations needing operations approach require operations evaluate complete poor scaling practice useful check software implementation backpropagation efficient version numerical differentiation applying central differences derivatives error calculated gives weights gradients evaluated method gives Hessian Exact evaluation Hessian considered various approximation schemes evaluating Hessian matrix Hessian evaluated network arbitrary feed-forward extension technique backpropagation used evaluate shares desirable features including computational efficiency applied differentiable error function expressed function network outputs networks having arbitrary differentiable activation number computational steps needed evaluate Hessian scales like Similar algorithms considered Buntine Weigend consider specific case network having layers required equations easily shall use indices denote indices denoted hidden indices denote define 2En En contribution error data point Hessian matrix network considered separate blocks weights second kj NEURAL NETWORKS weights ji kj weight ji zj element identity weights bias corresponding expressions obtained simply setting appropriate Inclusion skip-layer connections Fast multiplication Hessian applications quantity Hessian matrix product vector seen evaluation Hessian takes requires storage vector vTH wish instead computing Hessian intermediate instead try efficient approach evaluating vTH directly way requires note vTH denotes gradient operator weight write standard forward-propagation backpropagation equations evaluation apply equations set forward-propagation backpropagation equations evaluation vTH corresponds acting original forward-propagation backpropagation equations differential operator Pearlmutter used notation denote operator shall follow analysis straightforward makes use usual rules differential result technique best illustrated simple choose two-layer network form shown Figure linear output units sum-of-squares error consider contribution error function pattern data required vector obtained Hessian Matrix usual summing contributions patterns two-layer forward-propagation equations aj wjixi zj yk wkjzj act equations operator obtain set forward propagation equations form vjixi vkjzj vji element vector corresponds weight Quantities form regarded new variables values considering sum-of-squares error following standard backpropagation yk tk act equations operator obtain set backpropagation equations form usual equations derivatives error NEURAL NETWORKS acting obtain expressions elements vector vTH implementation algorithm involves introduction additional variables hidden units output input values quantities elements vTH elegant aspect technique equations evaluating vTH mirror closely standard forward backward extension existing software compute product typically technique used evaluate Hessian matrix choosing vector successively series unit vectors form picks column leads formalism analytically equivalent backpropagation procedure Bishop described Section loss efficiency redundant Regularization Neural Networks number input outputs units neural network generally determined dimensionality data number hidden units free parameter adjusted best predictive Note controls number parameters expect maximum likelihood setting optimum value gives best generalization corresponding optimum balance under-fitting Figure shows example effect different values sinusoidal regression generalization simple function presence local minima error illustrated Figure effect choosing multiple random initializations weight vector range values overall best validation set performance case occurred particular solution having approach choosing fact plot graph kind shown Figure choose specific solution having smallest validation set ways control complexity neural network model order avoid discussion polynomial curve fitting Chapter alternative approach choose relatively large value control complexity addition regularization term error simplest regularizer giving regularized error Regularization Neural Networks Figure Examples two-layer networks trained data points drawn sinusoidal data graphs result fitting networks having hidden minimizing sum-of-squares error function scaled conjugate-gradient form regularizer known weight decay discussed length Chapter effective model complexity determined choice regularization coefficient seen regularizer interpreted negative logarithm zero-mean Gaussian prior distribution weight vector Consistent Gaussian priors limitations simple weight decay form inconsistent certain scaling properties network illustrate consider multilayer perceptron network having layers weights linear output performs mapping set input variables set output variables activations hidden units hidden layer Figure Plot sum-of-squares test-set error polynomial data set versus number hidden units random starts network showing effect local new weight vector initialized sampling isotropic Gaussian distribution having mean zero variance NEURAL NETWORKS form zj wjixi wj0 activations output units yk wkjzj Suppose perform linear transformation input data form xi axi arrange mapping performed network unchanged making corresponding linear transformation weights biases inputs units hidden layer formExercise wji wji wj0 wj0 linear transformation output variables network form yk cyk achieved making transformation second-layer weights biases wkj cwkj wk0 cwk0 train network original data network data input target variables transformed linear consistency requires obtain equivalent networks differ linear transformation weights regularizer consistent arbitrarily favours solution equivalent simple weight decay treats weights biases equal does satisfy look regularizer invariant linear transformations require regularizer invariant re-scaling weights shifts regularizer denotes set weights denotes set weights second biases excluded regularizer Regularization Neural Networks remain unchanged weight transformations provided regularization parameters re-scaled regularizer corresponds prior form exp Note priors form improper bias parameters use improper priors lead difficulties selecting regularization coefficients model comparison Bayesian corresponding evidence common include separate priors biases break shift having illustrate effect resulting hyperparameters drawing samples prior plotting corresponding network shown Figure consider priors weights divided number groups Wk exp w2j special case choose groups correspond sets weights associated input optimize marginal likelihood respect corresponding parameters obtain automatic relevance determination discussed Section Early stopping alternative regularization way controlling effective complexity network procedure early training nonlinear network models corresponds iterative reduction error function defined respect set training optimization algorithms used network conjugate error nonincreasing function iteration error measured respect independent generally called validation shows decrease followed increase network starts Training stopped point smallest error respect validation data indicated Figure order obtain network having good generalization behaviour network case explained qualitatively terms effective number degrees freedom number starts small grows training corresponding steady increase effective complexity Halting training NEURAL NETWORKS Figure Illustration effect hyperparameters governing prior distribution weights biases two-layer network having single single linear hidden units having activation priors governed hyperparameters represent precisions Gaussian distributions first-layer first-layer second-layer second-layer parameter governs vertical scale functions different vertical axis ranges governs horizontal scale variations function governs horizontal range variations parameter effect illustrated governs range vertical offsets minimum training error reached represents way limiting effective network case quadratic error verify early stopping exhibit similar behaviour regularization simple weight-decay understood Figure axes weight space rotated parallel eigenvectors Hessian absence weight weight vector starts origin proceeds training path follows local negative gradient weight vector initially parallel axis point corresponding roughly minimum error function follows shape error surface widely differing eigenvalues Stopping point near similar weight relationship early stopping weight decay showing quantity iteration learning rate plays role reciprocal regularization Regularization Neural Networks Figure illustration behaviour training set error validation set error typical training function iteration sinusoidal data goal achieving best generalization performance suggests training stopped point shown vertical dashed corresponding minimum validation set parameter effective number parameters network grows course Invariances applications pattern known predictions transformations input classification objects two-dimensional handwritten particular object assigned classification irrespective position image size transformations produce significant changes raw expressed terms intensities pixels rise output classification Similarly speech small levels nonlinear warping time preserve temporal change interpretation sufficiently large numbers training patterns adaptive model neural network learn involves including training set sufficiently large number examples effects various translation invariance training set include examples objects different approach number training examples invariants number combinations transformations grows exponentially number seek alternative approaches encouraging adaptive model exhibit required broadly divided training set augmented replicas training transformed according desired digit recognition make multiple copies example NEURAL NETWORKS Figure schematic illustration early stopping similar results weight decay case quadratic error ellipse shows contour constant wML denotes minimum error weight vector starts origin moves according local negative gradient follow path shown stopping training weight vector ew qualitatively similar obtained simple weight-decay regularizer training minimum regularized seen comparing Figure wML digit shifted different position regularization term added error function penalizes changes model output input leads technique tangent discussed Section Invariance built pre-processing extracting features invariant required subsequent regression classification uses features inputs necessarily respect final option build invariance properties structure neural network definition kernel function case techniques relevance vector way achieve use local receptive fields shared discussed context convolutional neural networks Section Approach relatively easy implement used encourage complex invariances illustrated Figure sequential training transforming input pattern presented model patterns different transformation appropriate added batch similar effect achieved replicating data point number times transforming copy use augmented data lead significant improvements generalization et computationally Approach leaves data set unchanged modifies error function addition Section shall approach closely related approach Regularization Neural Networks Figure Illustration synthetic warping handwritten original image shown row shows examples warped corresponding displacement fields shown displacement fields generated sampling random displacements pixel smoothing convolution Gaussians width advantage approach correctly extrapolate range transformations included training difficult hand-crafted features required invariances discard information useful Tangent propagation use regularization encourage models invariant transformations input technique tangent propagation et Consider effect transformation particular input vector Provided transformation continuous translation mirror reflection transformed pattern sweep manifold D-dimensional input illustrated Figure case Suppose transformation governed single parameter rotation angle subspace swept xn Figure Illustration two-dimensional input space showing effect continuous transformation particular input vector onedimensional parameterized continuous variable applied xn causes sweep one-dimensional manifold effect transformation approximated tangent vector xn NEURAL NETWORKS parameterized Let vector results acting xn transformation denoted defined tangent curve directional derivative tangent vector point xn transformation input network output vector derivative output respect Jki element Jacobian matrix discussed Section result used modify standard error encourage local invariance neighbourhood data addition original error function regularization function total error function form regularization coefficient regularization function zero network mapping function invariant transformation neighbourhood pattern value parameter determines balance fitting training data learning invariance practical tangent vector approximated finite subtracting original vector xn corresponding vector transformation small value dividing illustrated Figure regularization function depends network weights Jacobian backpropagation formalism computing derivatives regularizer respect network weights easily obtained extension theExercise techniques introduced Section transformation governed parameters case translations combined in-plane rotations two-dimensional manifold dimensionality corresponding regularizer sum terms form transformations considered network mapping invariant invariant combinations transformations et Regularization Neural Networks Figure Illustration showing original image handwritten tangent vector corresponding infinitesimal clockwise result adding small contribution tangent vector original image giving true image rotated related called tangent used build invariance properties distance-based methods nearest-neighbour classifiers et Training transformed data seen way encourage invariance model set transformations expand training set transformed versions original input approach closely related technique tangent propagation Section shall consider transformation governed single parameter described function shall consider sum-of-squares error error function untransformed inputs written infinite data set form dxdt discussed Section considered network having single order notation consider infinite number copies data perturbed transformation NEURAL NETWORKS parameter drawn distribution error function defined expanded data set written assume distribution zero mean small considering small transformations original input expand transformation function Taylor series powers denotes second derivative respect evaluated allows expand model function Substituting mean error function dxdt dxdt dxdt distribution transformations zero mean shall denote Omitting terms average error function original sum-of-squares regularization term takes form dx performed integration Regularization Neural Networks simplify regularization term Section saw function minimizes sum-of-squares error conditional average target values regularized error equal unregularized sum-of-squares plus terms network function minimizes total error form leading order term regularizer vanishes left dx equivalent tangent propagation regularizer consider special case transformation inputs simply consists addition random regularizer takes formExercise dx known Tikhonov regularization Derivatives regularizer respect network weights extended backpropagation algorithm small noise Tikhonov regularization related addition random noise shown improve generalization appropriate circumstances Convolutional networks approach creating models invariant certain transformation inputs build invariance properties structure neural basis convolutional neural network Cun et LeCun et widely applied image Consider specific task recognizing handwritten input image comprises set pixel intensity desired output posterior probability distribution digit know identity digit invariant translations scaling network exhibit invariance subtle transformations elastic deformations kind illustrated Figure simple approach treat image input fully connected kind shown Figure sufficiently large training network principle yield good solution problem learn appropriate invariances approach ignores key property nearby pixels strongly correlated distant modern approaches vision exploit property extracting local features depend small subregions Information features merged later stages processing order detect higher-order features NEURAL NETWORKS Input image Convolutional layer Sub-sampling layer Figure Diagram illustrating convolutional neural showing layer convolutional units followed layer subsampling successive pairs layers ultimately yield information image local features useful region image likely useful regions instance object notions incorporated convolutional neural networks local receptive weight structure convolutional network illustrated Figure convolutional layer units organized called feature Units feature map inputs small subregion units feature map constrained share weight feature map consist units arranged unit taking inputs pixel patch feature map adjustable weight parameters plus adjustable bias Input values patch linearly combined weights result transformed sigmoidal nonlinearity think units feature units feature map detect pattern different locations input weight evaluation activations units equivalent convolution image pixel intensities comprising weight input image activations feature map shifted provides basis invariance Regularization Neural Networks network outputs translations distortions input typically need detect multiple features order build effective generally multiple feature maps convolutional having set weight bias outputs convolutional units form inputs subsampling layer feature map convolutional plane units subsampling layer unit takes inputs small receptive field corresponding feature map convolutional units perform subsampling unit inputs unit region corresponding feature map compute average multiplied adaptive weight addition adaptive bias transformed sigmoidal nonlinear activation receptive fields chosen contiguous nonoverlapping half number rows columns subsampling layer compared convolutional response unit subsampling layer relatively insensitive small shifts image corresponding regions input practical pairs convolutional subsampling stage larger degree invariance input transformations compared previous feature maps convolutional layer plane units previous subsampling gradual reduction spatial resolution compensated increasing number final layer network typically fully fully adaptive softmax output nonlinearity case multiclass network trained error minimization backpropagation evaluate gradient error involves slight modification usual backpropagation algorithm ensure shared-weight constraints use local receptive number weights inExercise network smaller network fully number independent parameters learned data smaller substantial numbers constraints Soft weight sharing way reduce effective complexity network large number weights constrain weights certain groups technique weight sharing discussed Section way building translation invariance networks used image particular problems form constraints specified consider form soft weight sharing hard constraint equal weights replaced form regularization groups weights encouraged similar division weights mean weight value spread values groups determined learning NEURAL NETWORKS Recall simple weight decay viewed negative log Gaussian prior distribution encourage weight values form just considering instead probability distribution mixture centresSection variances Gaussian mixing considered adjustable parameters determined learning probability density form mixing Taking negative logarithm leads regularization function form total error function regularization error minimized respect weights wi respect parameters mixture weights parameters mixture model determined EM algorithm discussed Chapter distribution weights evolving learning avoid numerical joint optimization performed simultaneously weights mixture-model standard optimization algorithm conjugate gradients quasi-Newton order minimize total error necessary able evaluate derivatives respect various adjustable convenient regard prior probabilities introduce corresponding posterior probabilities following theorem form derivatives total error function respect weights byExercise Regularization Neural Networks effect regularization term pull weight centre jth force proportional posterior probability Gaussian precisely kind effect Derivatives error respect centres Gaussians easily computed giveExercise simple intuitive pushes average weight weighted posterior probabilities respective weight parameters generated component derivatives respect variances byExercise drives weighted average squared deviations weights corresponding centre weighting coefficients posterior probability weight generated component Note practical new variables defined minimization performed respect ensures parameters remain effect discouraging pathological solutions goes corresponding Gaussian component collapsing weight parameter solutions discussed context Gaussian mixture models Section derivatives respect mixing coefficients need account constraints follow interpretation prior expressing mixing coefficients terms set auxiliary variables softmax function derivatives regularized error function respect formExercise NEURAL NETWORKS Figure left figure shows two-link robot Cartesian coordinates end effector determined uniquely joint angles lengths know forward kinematics joint angles rise desired end effector position shown right inverse kinematics solutions corresponding elbow elbow driven average posterior probability component Mixture Density Networks goal supervised learning model conditional distribution simple regression problems chosen practical machine learning problems significantly non-Gaussian inverse problems distribution case Gaussian assumption lead poor simple example inverse consider kinematics robot illustrated Figure forward problem involves finding end ef-Exercise fector position joint angles unique practice wish end effector robot specific set appropriate joint need solve inverse solutions seen Figure Forward problems corresponds causality physical generally unique specific pattern symptoms human body caused presence particular pattern typically solve inverse trying predict presence disease set forward problem involves many-to-one inverse problem multiple different diseases result robotics kinematics defined geometrical multimodality readily machine learning problems presence particularly problems involving spaces high tutorial shall consider simple toy problem easily visualize Data problem generated sampling variable uniformly interval set values corresponding target values obtained Mixture Density Networks Figure left data set simple red curve shows result fitting two-layer neural network minimizing sum-of-squares error corresponding inverse shown obtained exchanging roles network trained minimizing sum-of-squares error function gives poor fit data multimodality data computing function xn adding uniform noise interval inverse problem obtained keeping data points exchanging roles Figure shows data sets forward inverse results fitting two-layer neural networks having hidden units single linear output unit minimizing sumof-squares error squares corresponds maximum likelihood Gaussian leads poor model highly non-Gaussian inverse seek general framework modelling conditional probability achieved mixture model mixing coefficients component densities flexible functions input vector giving rise mixture density value mixture model provides general formalism modelling arbitrary conditional density function Provided consider sufficiently flexible framework approximating arbitrary conditional shall develop model explicitly Gaussian example heteroscedastic model noise variance data function input vector Instead use distributions Bernoulli distributions target variables binary specialized case isotropic covariances mixture density network readily extended allow general covariance matrices representing covariances Cholesky factorization isotropic conditional distribution does assume factorization respect components contrast standard sum-of-squares regression consequence mixture various parameters mixture mixing coefficients means variances governed NEURAL NETWORKS xD Figure mixture density network represent general conditional probability densities considering parametric mixture model distribution parameters determined outputs neural network takes input outputs conventional neural network takes structure mixture density network illustrated Figure mixture density network closely related mixture experts discussed Section principle difference mixture density network function used predict parameters component densities mixing nonlinear hidden units shared input-dependent neural network Figure two-layer network having sigmoidal hidden components mixture model network output unit activations denoted determine mixing coefficients outputs denoted determine kernel widths outputs denoted determine components kernel centres total number network outputs compared usual outputs simply predicts conditional means target mixing coefficients satisfy constraints achieved set softmax outputs variances satisfy represented terms exponentials corresponding network activations means real represented Mixture Density Networks directly network output activations kj adaptive parameters mixture density network comprise vector weights biases neural set maximum equivalently minimizing error function defined negative logarithm independent error function takes form dependencies order minimize error need calculate derivatives error respect components evaluated standard backpropagation provided obtain suitable expressions derivatives error respect output-unit represent error signals pattern output backpropagated hidden units error function derivatives evaluated usual error function composed sum training data consider derivatives particular pattern derivatives summing dealing mixture convenient view mixing coefficients x-dependent prior probabilities introduce corresponding posterior probabilities Nnk denotes derivatives respect network output activations governing mixing coefficients byExercise derivatives respect output activations controlling component means byExercise tl derivatives respect output activations controlling component variances byExercise NEURAL NETWORKS Figure Plot mixing coefficients function kernel functions mixture density network trained data shown Figure model Gaussian uses two-layer multilayer perceptron sigmoidal units hidden outputs means variances Gaussian components mixing small large values conditional probability density target data kernels high value prior intermediate values conditional density mixing coefficients comparable Plots means colour coding mixing Plot contours corresponding conditional probability density target data mixture density Plot approximate conditional shown red conditional illustrate use mixture density network returning toy example inverse problem shown Figure Plots mixing coefficients means conditional density contours corresponding shown Figure outputs neural parameters mixture necessarily continuous single-valued functions input Figure model able produce conditional density unimodal values trimodal values modulating amplitudes mixing components mixture density network predict conditional density function target data value input conditional density represents complete description generator far problem predicting value output vector density function calculate specific quantities different simplest corresponding conditional average target dt Bayesian Neural Networks used standard network trained squares approximating conditional mixture density network reproduce conventional least-squares result special multimodal distribution conditional mean limited similarly evaluate variance density function conditional giveExercise used general corresponding least-squares result variance function seen multimodal conditional mean poor representation controlling simple robot arm shown Figure need pick possible joint angle settings order achieve desired end-effector average solutions conditional mode conditional mode mixture density network does simple analytical require numerical simple alternative mean probable component largest mixing value shown toy data set Figure Bayesian Neural Networks discussion neural networks focussed use maximum likelihood determine network parameters Regularized maximum likelihood interpreted MAP approach regularizer viewed logarithm prior parameter Bayesian treatment need marginalize distribution parameters order make Section developed Bayesian solution simple linear regression model assumption Gaussian saw posterior evaluated exactly predictive distribution closed case multilayered highly nonlinear dependence network function parameter values means exact Bayesian treatment longer log posterior distribution corresponding multiple local minima error technique variational discussed Chapter applied Bayesian neural networks factorized Gaussian approximation NEURAL NETWORKS posterior distribution van fullcovariance Gaussian Barber complete based Laplace approximation forms basis discussion approximate posterior distribution centred mode true shall assume covariance Gaussian small network function approximately linear respect parameters region parameter space posterior probability significantly obtain models analogous linear regression classification models discussed earlier chapters exploit results obtained make use evidence framework provide point estimates hyperparameters compare alternative models networks having different numbers hidden start shall discuss regression case later consider modifications needed solving classification Posterior parameter distribution Consider problem predicting single continuous target variable vector inputs extension multiple targets shall suppose conditional distribution x-dependent mean output neural network model precision shall choose prior distribution weights Gaussian form data set observations corresponding set target values likelihood function resulting posterior distribution consequence nonlinear dependence Gaussian approximation posterior distribution Laplace maximum iterative numerical convenient maximize logarithm written Bayesian Neural Networks form wTw const corresponds regularized sum-of-squares error Assuming moment maximum denote standard nonlinear optimization algorithms conjugate error backpropagation evaluate required Having mode build local Gaussian approximation evaluating matrix second derivatives negative log posterior Hessian matrix comprising second derivatives sum-ofsquares error function respect components Algorithms computing approximating Hessian discussed Section corresponding Gaussian approximation posterior predictive distribution obtained marginalizing respect posterior distribution Gaussian approximation integration analytically intractable nonlinearity network function function make assume posterior distribution small variance compared characteristic scales allows make Taylor series expansion network function wMAP retain linear terms defined linear-Gaussian model Gaussian distribution Gaussian mean linear function form make use general result marginal giveExercise NEURAL NETWORKS input-dependent variance predictive distribution Gaussian mean network function parameter set MAP variance arises intrinsic noise target second x-dependent term expresses uncertainty interpolant uncertainty model parameters compared corresponding predictive distribution linear regression Hyperparameter optimization assumed hyperparameters fixed make use evidence discussed Section Gaussian approximation posterior obtained Laplace obtain practical procedure choosing values marginal hyperparameters obtained integrating network weights easily evaluated making use Laplace approximation result Taking logarithms gives total number parameters regularized error function defined takes form corresponding result linear regression evidence make point estimates maximizing Consider maximization respect analogy linear regression case discussed Section define eigenvalue equation Hessian matrix comprising second derivatives sum-ofsquares error evaluated analogy obtain wTMAPwMAP Bayesian Neural Networks represents effective number parameters defined bySection Note result exact linear regression nonlinear neural ignores fact changes cause changes Hessian turn change implicitly ignored terms involving derivatives respect maximizing evidence respect gives re-estimation formula linear need alternate re-estimation hyperparameters updating posterior situation neural network model multimodality posterior solution wMAP maximizing log posterior depend initialization Solutions differ consequence interchange sign reversal symmetries hidden unitsSection identical far predictions irrelevant equivalent solutions inequivalent solutions generally yield different values optimized order compare different example neural networks having different numbers hidden need evaluate model evidence approximated taking substituting values obtained iterative optimization careful evaluation obtained marginalizing making Gaussian approximation necessary evaluate determinant Hessian problematic practice unlike sensitive small eigenvalues difficult determine Laplace approximation based local quadratic expansion mode posterior distribution seen Section mode two-layer network member set equivalent modes differ interchange sign-change number hidden comparing networks having different numbers hidden taken account multiplying evidence factor Bayesian neural networks classification used Laplace approximation develop Bayesian treatment neural network regression discuss modifications NEURAL NETWORKS framework arise applied shall consider network having single logistic sigmoid output corresponding two-class classification extension networks multiclass softmax outputs shall build extensively analogous results linearExercise classification models discussed Section encourage reader familiarize material studying log likelihood function model 1N yn target yn Note hyperparameter data points assumed correctly prior taken isotropic Gaussian form stage applying Laplace framework model initialize hyperparameter determine parameter vector maximizing log posterior equivalent minimizing regularized error function wTw achieved error backpropagation combined standard optimization discussed Section Having solution wMAP weight step evaluate Hessian matrix comprising second derivatives negative log likelihood exact method Section outer product approximation second derivatives negative log posterior written form Gaussian approximation posterior optimize hyperparameter maximize marginal easily shown formExercise const regularized error function defined yn MAPwMAP yn Maximizing evidence function respect leads re-estimation equation use evidence procedure determine illustrated Figure synthetic two-dimensional data discussed Appendix need predictive defined integration intractable nonlinearity network Bayesian Neural Networks Figure Illustration evidence framework applied synthetic two-class data green curve shows optimal decision black curve shows result fitting two-layer network hidden units maximum red curve shows result including regularizer optimized evidence starting initial value Note evidence procedure greatly reduces over-fitting simplest approximation assume posterior distribution narrow make approximation improve taking account variance posterior linear approximation network used case inappropriate logistic sigmoid outputunit activation function constrains output lie range make linear approximation output unit activation form vector Gaussian approximation posterior distribution model linear function appeal results Section distribution output unit activation induced distribution network dw Gaussian approximation posterior distribution Section distribution Gaussian mean aMAP variance obtain predictive marginalize NEURAL NETWORKS Figure illustration Laplace approximation Bayesian neural network having hidden units activation functions single logistic-sigmoid output weight parameters scaled conjugate hyperparameter optimized evidence left result simple approximation based point estimate wMAP green curve shows decision contours correspond output probabilities right corresponding result obtained Note effect marginalization spread contours make predictions input point posterior probabilities shifted contour convolution Gaussian logistic sigmoid apply approximation giving defined Recall functions Figure shows example framework applied synthetic classification data set described Appendix Exercises Consider two-layer network function form hiddenunit nonlinear activation functions logistic sigmoid functions form exists equivalent computes exactly hidden unit activation functions tanh function defined relation parameters networks differ linear www maximizing likelihood function conditional distribution multioutput neural network equivalent minimizing sum-of-squares error function Exercises Consider regression problem involving multiple target variables assumed distribution conditioned input vector Gaussian form output neural network input vector weight vector covariance assumed Gaussian noise set independent observations write error function minimized order maximum likelihood solution assume fixed assume determined write expression maximum likelihood solution Note optimizations contrast case independent target variables discussed Section Consider binary classification problem target values network output represents suppose probability class label training data point incorrectly Assuming independent identically distributed write error function corresponding negative log Verify error function obtained Note error function makes model robust incorrectly labelled contrast usual error www maximizing likelihood multiclass neural network model network outputs interpretation equivalent minimization cross-entropy error function www derivative error function respect activation ak output unit having logistic sigmoid activation function satisfies derivative error function respect activation ak output units having softmax activation function satisfies saw derivative logistic sigmoid activation function expressed terms function value Derive corresponding result activation function defined www error function binary classification problems derived network having logistic-sigmoid output activation data having target values Derive corresponding error function consider network having output target values class class appropriate choice output unit activation www Consider Hessian matrix eigenvector equation setting vector equal eigenvectors ui positive definite eigenvalues NEURAL NETWORKS www Consider quadratic error function defined Hessian matrix eigenvalue equation contours constant error ellipses axes aligned eigenvectors lengths inversely proportional square root corresponding eigenvalues www considering local Taylor expansion error function stationary point necessary sufficient condition stationary point local minimum error function Hessian matrix defined positive consequence symmetry Hessian matrix number independent elements quadratic error function making Taylor verify terms cancel right-hand Section derived procedure evaluating Jacobian matrix neural network backpropagation Derive alternative formalism finding Jacobian based forward propagation outer product approximation Hessian matrix neural network sum-of-squares error function Extend result case multiple Consider squared loss function form dxdt parametric function neural result shows function minimizes error conditional expectation Use result second derivative respect elements wr ws vector Note finite sample obtain Consider two-layer network form shown Figure addition extra parameters corresponding skip-layer connections directly inputs extending discussion Section write equations derivatives error function respect additional www Derive expression outer product approximation Hessian matrix network having single output logistic sigmoid output-unit activation function cross-entropy error corresponding result sum-of-squares error Exercises Derive expression outer product approximation Hessian matrix network having outputs softmax output-unit activation function cross-entropy error corresponding result sum-ofsquares error Extend expression outer product approximation Hessian matrix case output derive recursive expression analogous incrementing number patterns similar expression incrementing number Use identity sequential update expressions analogous finding inverse Hessian incrementally including extra patterns extra Derive results elements Hessian matrix two-layer feed-forward network application chain rule Extend results Section exact Hessian two-layer network include skip-layer connections directly inputs Verify network function defined invariant transformation applied provided weights biases simultaneously transformed network outputs transformed according applying transformation second-layer weights www Consider quadratic error function form represents Hessian matrix positive definite Suppose initial weight vector chosen origin updated simple gradient descent denotes step learning rate assumed components weight vector parallel eigenvectors written wj wTuj uj eigenvectors Huj gives provided suppose training halted finite number NEURAL NETWORKS components weight vector parallel eigenvectors Hessian satisfy Compare result discussion Section regularization simple weight analogous regularization parameter results effective number parameters defined grows training Consider multilayer perceptron arbitrary feed-forward trained minimizing tangent propagation error function regularizing function regularization term written sum patterns terms form differential operator defined acting forward propagation equations zj aj wjizi operator evaluated forward propagation following defined new variables Gzj Gaj derivatives respect weight wrs network written form defined Write backpropagation equations derive set backpropagation equations evaluation Exercises www Consider framework training transformed data special case transformation consists simply addition random noise Gaussian distribution zero mean unit following argument analogous Section resulting regularizer reduces Tikhonov form www Consider neural convolutional network discussed Section multiple weights constrained Discuss standard backpropagation algorithm modified order ensure constraints satisfied evaluating derivatives error function respect adjustable parameters www Verify result Verify result Verify result derivatives mixing coefficients defined respect auxiliary parameters making use constraint derive result Write pair equations express Cartesian coordinates robot arm shown Figure terms joint angles lengths Assume origin coordinate attachment point lower equations define robot www Derive result derivative error function respect network output activations controlling mixing coefficients mixture density Derive result derivative error function respect network output activations controlling component means mixture density Derive result derivative error function respect network output activations controlling component variances mixture density Verify results conditional mean variance mixture density network general result derive predictive distribution Laplace approximation Bayesian neural network NEURAL NETWORKS www Make use Laplace approximation result evidence function hyperparameters Bayesian neural network model approximated www Outline modifications needed framework Bayesian neural discussed Section handle multiclass problems networks having softmax output-unit activation following analogous steps Section regression derive result marginal likelihood case network having cross-entropy error function logistic-sigmoid output-unit activation 
Kernel Methods Chapters considered linear parametric models regression classification form mapping input output governed vector adaptive learning set training data used obtain point estimate parameter vector determine posterior distribution training data predictions new inputs based purely learned parameter vector approach used nonlinear parametric models neural class pattern recognition training data subset kept used prediction Parzen probability density model comprised linear combinationSection functions centred training data Section introduced simple technique classification called nearest involved assigning new test vector label KERNEL METHODS closest example training examples memory-based methods involve storing entire training set order make predictions future data typically require metric defined measures similarity vectors input generally fast slow making predictions test data linear parametric models re-cast equivalent predictions based linear combinations kernel function evaluated training data shall models based fixed nonlinear feature space mapping kernel function relation kernel symmetric function arguments kernel concept introduced field pattern recognition Aizerman et context method potential so-called analogy neglected re-introduced machine learning context largemargin classifiers Boser et giving rise technique support vector considerable bothChapter terms theory significant developments extension kernels handle symbolic greatly expanding range problems simplest example kernel function obtained considering identity mapping feature space case shall refer linear concept kernel formulated inner product feature space allows build interesting extensions well-known algorithms making use kernel known kernel general idea algorithm formulated way input vector enters form scalar replace scalar product choice technique kernel substitution applied principal component analysis order develop nonlinear variant PCA et examples kernel substitution include nearest-neighbour classifiers kernel Fisher discriminant et Roth Baudat numerous forms kernel functions common shall encounter examples property function difference known stationary kernels invariant translations input specialization involves homogeneous known radial basis depend magnitude distance arguments recent textbooks kernel Smola Herbrich Shawe-Taylor Cristianini Dual Representations Dual Representations linear models regression classification reformulated terms dual representation kernel function arises concept play important role consider support vector machines consider linear regression model parameters determined minimizing regularized sum-of-squares error function wTw set gradient respect equal solution takes form linear combination vectors coefficients functions form design nth row vector defined Instead working parameter vector reformulate leastsquares algorithm terms parameter vector giving rise dual substitute obtain tTt define Gram matrix symmetric matrix elements Knm introduced kernel function defined terms Gram sum-of-squares error function written aTKt tTt Setting gradient respect obtain following solution KERNEL METHODS substitute linear regression obtain following prediction new input defined vector elements dual formulation allows solution least-squares problem expressed entirely terms kernel function known dual formulation noting solution expressed linear combination elements recover original formulation terms parameter vector Note prediction linear combinationExercise target values training obtained slightly different Section dual determine parameter vector inverting original parameter space formulation invert matrix order determine typically larger dual formulation does particularly advantage dual shall expressed entirely terms kernel function work directly terms kernels avoid explicit introduction feature vector allows implicitly use feature spaces existence dual representation based Gram matrix property linear including Section develop dual-Exercise ity probabilistic linear models regression technique Gaussian Duality play important role discuss support vector machines Chapter Constructing Kernels order exploit kernel need able construct valid kernel approach choose feature space mapping use corresponding illustrated Figure kernel function defined one-dimensional input space basis alternative approach construct kernel functions ensure function choose valid words corresponds scalar product infinite feature simple consider kernel function xTz Constructing Kernels Figure Illustration construction kernel functions starting corresponding set basis column lower plot shows kernel function defined plotted function upper plot shows corresponding basis functions polynomials logistic sigmoids particular case two-dimensional input space expand terms identify corresponding nonlinear feature mapping xTz x21z 2x1z1x2z2 2z feature mapping takes form comprises possible second order specific weighting need simple way test function constitutes valid kernel having construct function necessary sufficient condition function valid kernel Gram matrix elements positive semidefinite possible choices set Note positive semidefinite matrix thing matrix elements powerful technique constructing new kernels build simpler kernels building following KERNEL METHODS Techniques Constructing New valid kernels following new kernels exp polynomial nonnegative function RM valid kernel symmetric positive semidefinite xa xb variables necessarily ka kb valid kernel functions respective Equipped embark construction complex kernels appropriate specific require kernel symmetric positive semidefinite expresses appropriate form similarity according intended consider common examples kernel extensive discussion Shawe-Taylor Cristianini saw simple polynomial kernel contains terms degree consider slightly generalized kernel corresponding feature mapping contains constant linear terms terms order contains monomials order kernel represents particular weighted sum possible products pixels image pixels second similarly generalized include terms degree considering results combining kernels valid kernel commonly used kernel takes form exp called context interpreted probability normalization coefficient Constructing Kernels valid kernel expanding square xTx exp exp exp making use validity linear kernel Note feature vector corresponds Gaussian kernel infinite Gaussian kernel restricted use Euclidean use kernel substitution replace nonlinear kernel obtain exp important contribution arise kernel viewpoint extension inputs simply vectors real Kernel functions defined objects diverse text fixed set define nonvectorial space consisting possible subsets subsets simple choice kernel denotes intersection sets denotes number subsets valid kernel function shown correspond inner product feature powerful approach construction kernels starts probabilistic generative model allows apply generative models discriminative Generative models deal naturally missing data case hidden Markov models handle sequences varying discriminative models generally better performance discriminative tasks generative combine approaches et way combine use generative model define use kernel discriminative generative model define kernel clearly valid kernel function interpret inner product one-dimensional feature space defined mapping says inputs similar high use extend class kernels considering sums products different probability positive weighting coefficients form KERNEL METHODS overall multiplicative mixture distribution components index playing role inputs large value kernel andSection appear significant probability range different Taking limit infinite consider kernels form dz continuous latent suppose data consists ordered sequences length observation popular generative model sequences hidden Markov expresses distribution aSection marginalization corresponding sequence hidden states use approach define kernel function measuring similarity sequences extending mixture representation observed sequences generated hidden sequence model easily extended allow sequences differing length alternative technique generative models define kernel functions known Fisher kernel Consider parametric generative model denotes vector goal kernel measures similarity input vectors induced generative Jaakkola Haussler consider gradient respect defines vector space having dimensionality consider Fisher score Fisher kernel defined Fisher information expectation respect distribution motivated perspective information geometry considers differential geometry space model simply note presence Fisher information matrix causes kernel invariant nonlinear re-parameterization density model infeasible evaluate Fisher information approach simply replace expectation definition Fisher information sample giving Radial Basis Function Networks covariance matrix Fisher Fisher kernel corresponds whitening just omit FisherSection information matrix altogether use noninvariant kernel application Fisher kernels document retrieval Hofmann final example kernel function sigmoidal kernel tanh Gram matrix general positive form kernel used practice possibly gives kernel expansions support vector machine superficial resemblance neural network shall limit infinite number basis Bayesian neural network appropriate prior reduces Gaussian providing deeper link neural networks kernel Radial Basis Function Networks Chapter discussed regression models based linear combinations fixed basis did discuss form basis functions choice widely used radial basis property basis function depends radial distance centre radial basis functions introduced purpose exact function interpolation set input vectors corresponding target values goal smooth function fits target value achieved expressing linear combination radial basis centred data point values coefficients number coefficients result function fits target value pattern recognition target values generally exact interpolation undesirable corresponds over-fitted Expansions radial basis functions arise regularization theory sum-of-squares error function regularizer defined terms differential optimal solution expansion functions operator analogous eigenvectors discrete basis function centred data KERNEL METHODS differential operator isotropic functions depend radial distance corresponding data presence solution longer interpolates training data motivation radial basis functions comes consideration interpolation problem input variables noisy noise input variable described variable having distribution sum-of-squares error function calculus optimize respect function giveExercise basis functions basis function centred data known Nadaraya-Watson model derived different perspective Section noise distribution function basis functions Note basis functions value effect normalization shown Figure Normalization used practice avoids having regions input space basis functions small necessarily lead predictions regions small controlled purely bias situation expansions normalized radial basis functions arise application kernel density estimation problem shall discuss Section basis function associated data corresponding model computationally costly evaluate making predictions new data Models proposed Moody Poggio retain expansion radial basis functions number basis functions smaller number data number basis locations determined based input data basis functions kept fixed coefficients determined squares solving usual set linear discussed Section Radial Basis Function Networks Figure Plot set Gaussian basis functions corresponding normalized basis functions simplest ways choosing basis function centres use randomly chosen subset data systematic approach called orthogonal squares et sequential selection process step data point chosen basis function centre corresponds gives greatest reduction sum-of-squares Values expansion coefficients determined Clustering algorithms K-means set basis function centres thatSection longer coincide training data Nadaraya-Watson model Section saw prediction linear regression model new input takes form linear combination training set target values coefficients equivalent kernel satisfies summation constraint motivate kernel regression model different starting kernel density Suppose training set use Parzen density estimator model joint distribution thatSection component density component centred data expression regression function corresponding conditional average target variable conditioned KERNEL METHODS input dt dt dt assume simplicity component density functions zero mean dt values simple change obtain kernel function defined result known Nadaraya-Watson kernel regression localized kernel property giving weight data points xn close Note kernel satisfies summation constraint Gaussian Processes Figure Illustration Nadaraya-Watson kernel regression model isotropic Gaussian sinusoidal data original sine function shown green data points shown centre isotropic Gaussian resulting regression conditional shown red twostandard-deviation region conditional distribution shown red blue ellipse data point shows standard deviation contour corresponding appear noncircular different scales horizontal vertical model defines conditional expectation conditional distribution dt dt expectations illustration consider case single input variable zero-mean isotropic Gaussian variable variance corresponding conditional distribution Gaussian conditional sinusoidalExercise synthetic data set Figure obvious extension model allow flexible forms Gaussian instance having different variance parameters input target model joint distribution Gaussian mixture trained techniques discussed Chapter corresponding conditional distribution case longer representation terms kernel functions evaluated training set data number components mixture model smaller number training set resulting model faster evaluate test data accepted increased computational cost training phase order model faster making Gaussian Processes Section introduced kernels applying concept duality nonprobabilistic model extend role kernels probabilis304 KERNEL METHODS tic discriminative leading framework Gaussian shall kernels arise naturally Bayesian Chapter considered linear regression models form vector parameters vector fixed nonlinear basis functions depend input vector showed prior distribution induced corresponding prior distribution functions training data evaluated posterior distribution obtained corresponding posterior distribution regression turn addition implies predictive distribution new input vectors Gaussian process dispense parametric model instead define prior probability distribution functions difficult work distribution uncountably infinite space shall finite training set need consider values function discrete set input values xn corresponding training set test set data practice work finite Models equivalent Gaussian processes widely studied different geostatistics literature Gaussian process regression known kriging ARMA moving Kalman radial basis function networks viewed forms Gaussian process Reviews Gaussian processes machine learning perspective MacKay Williams MacKay comparison Gaussian process models alternative approaches Rasmussen Rasmussen Williams recent textbook Gaussian Linear regression revisited order motivate Gaussian process let return linear regression example re-derive predictive distribution working terms distributions functions provide specific example Gaussian Consider model defined terms linear combination fixed basis functions elements vector input vector -dimensional weight consider prior distribution isotropic Gaussian form governed hyperparameter represents precision value definition defines particular function probability distribution defined induces probability distribution functions wish evaluate function specific values example training data points Gaussian Processes interested joint distribution function values denote vector elements yn vector design matrix elements probability distribution note linear combination Gaussian distributed variables elements need mean fromExercise yyT wwT Gram matrix elements Knm kernel model provides particular example Gaussian Gaussian process defined probability distribution functions set values evaluated arbitrary set points jointly Gaussian cases input vector known Gaussian random stochastic process specified giving joint probability distribution finite set values consistent key point Gaussian stochastic processes joint distribution variables yN specified completely second-order mean prior knowledge mean symmetry equivalent choosing mean prior weight values zero basis function specification Gaussian process completed giving covariance evaluated values kernel function specific case Gaussian process defined linear regression model weight prior kernel function define kernel function indirectly choice basis Figure shows samples functions drawn Gaussian processes different choices kernel kernel form second exponential kernel exp corresponds Ornstein-Uhlenbeck process originally introduced Uhlenbeck Ornstein Brownian KERNEL METHODS Figure Samples Gaussian processes kernel exponential kernel Gaussian processes regression order apply Gaussian process models problem need account noise observed target yn yn random noise variable value chosen independently observation shall consider noise processes Gaussian hyperparameter representing precision noise independent data joint distribution target values conditioned values yN isotropic Gaussian form denotes unit definition Gaussian marginal distribution Gaussian mean zero covariance defined Gram matrix kernel function determines typically chosen express property points xn xm corresponding values strongly correlated dissimilar notion similarity depend order marginal distribution conditioned input values need integrate making use results Section linear-Gaussian marginal distribution dy Gaussian Processes covariance matrix elements result reflects fact Gaussian sources associated associated independent covariances simply widely used kernel function Gaussian process regression exponential quadratic addition constant linear terms exp Note term involving corresponds parametric model linear function input Samples prior plotted various values parameters Figure Figure shows set points sampled joint distribution corresponding values defined used Gaussian process viewpoint build model joint distribution sets data goal make predictions target variables new set training Let suppose corresponding input values comprise observed training goal predict target variable new input vector requires evaluate predictive distribution Note distribution conditioned variables notation simple conditioning variables conditional distribution begin writing joint distribution denotes vector apply results Section obtain required conditional illustrated Figure joint distribution covariance matrix elements joint distribution apply results Section conditional Gaussian partition covariance matrix follows kT covariance matrix elements vector elements scalar KERNEL METHODS Figure Samples Gaussian process prior defined covariance function title plot denotes results conditional distribution Gaussian distribution mean covariance key results define Gaussian process vector function test point input value predictive distribution Gaussian mean variance depend example Gaussian process regression shown Figure restriction kernel function covariance matrix positive eigenvalue corresponding eigenvalue sufficient kernel matrix positive semidefinite pair points xn eigenvalue zero rise positive eigenvalue restriction kernel function discussed exploit techniques Section construct Gaussian Processes Figure Illustration sampling data points Gaussian blue curve shows sample function Gaussian process prior red points values yn obtained evaluating function set input values corresponding values shown obtained adding independent Gaussian noise suitable Note mean predictive distribution function form nth component kernel function depends distance obtain expansion radial basis results define predictive distribution Gaussian process regression arbitrary kernel function particular case kernel function defined terms finite set basis derive results obtained previously Section linear regression starting Gaussian process obtain predictive distribution taking parameter space viewpoint linear regression result taking function space viewpoint Gaussian process central computational operation Gaussian processes involve inversion matrix size standard methods require basis function model invert matrix SN size computational Note matrix inversion performed training new test methods require vector-matrix cost Gaussian process case linear basis function number basis functions smaller number data computationally efficient work basis function KERNEL METHODS Figure Illustration mechanism Gaussian process regression case training point test red ellipses contours joint distribution training data conditioning value corresponding vertical blue obtain shown function green advantage Gaussian processes viewpoint consider covariance functions expressed terms infinite number basis large training data direct application Gaussian process methods range approximation schemes developed better scaling training set size exact approach Smola Williams Seeger et Practical issues application Gaussian processes discussed Bishop Nabney introduced Gaussian process regression case single target extension formalism multiple target known co-kriging Various extensions Gaus-Exercise Figure Illustration Gaussian process regression applied sinusoidal data set Figure right-most data points green curve shows sinusoidal function data shown obtained sampling addition Gaussian red line shows mean Gaussian process predictive shaded region corresponds plus minus standard Notice uncertainty increases region right data Gaussian Processes sian process regression purposes modelling distribution low-dimensional manifolds unsupervised learning et solution stochastic differential equations Learning hyperparameters predictions Gaussian process model choice covariance fixing covariance prefer use parametric family functions infer parameter values parameters govern things length scale correlations precision noise correspond hyperparameters standard parametric Techniques learning hyperparameters based evaluation likelihood function denotes hyperparameters Gaussian process simplest approach make point estimate maximizing log likelihood represents set hyperparameters regression viewed analogous type maximum likelihood procedure linear regression Maximization log likelihoodSection efficient gradient-based optimization algorithms conjugate gradients Nocedal Bishop log likelihood function Gaussian process regression model easily evaluated standard form multivariate Gaussian giving nonlinear need gradient log likelihood function respect parameter vector shall assume evaluation derivatives case covariance functions considered Making use result derivative result derivative obtain Tr general nonconvex multiple straightforward introduce prior maximize log posterior gradient-based fully Bayesian need evaluate marginals weighted product prior likelihood function exact marginalization resort Gaussian process regression model gives predictive distribution mean variance functions input vector assumed contribution predictive variance arising additive governed parameter known noise variance depend model extend KERNEL METHODS Figure Samples ARD prior Gaussian kernel function left plot corresponds right plot corresponds Gaussian process framework introducing second Gaussian process represent dependence input et use Gaussian process model Automatic relevance determination previous saw maximum likelihood used determine value correlation length-scale parameter Gaussian technique usefully extended incorporating separate parameter input variable shall optimization parameters maximum likelihood allows relative importance different inputs inferred represents example Gaussian process context automatic relevance originally formulated framework neural networks mechanism appropriate inputs preferred discussed Section Consider Gaussian process two-dimensional input space having kernel function form exp Samples resulting prior functions shown different settings precision parameters Figure particular parameter function relatively insensitive corresponding input variable adapting parameters data set maximum possible detect input variables little effect predictive corresponding values useful practice allows inputs ARD illustrated simple synthetic data set having inputs Figure target variable generated sampling values evaluating function adding Gaussian Processes Figure Illustration automatic relevance determination Gaussian process synthetic problem having inputs curves corresponding values hyperparameters function number iterations optimizing marginal Details Note logarithmic scale vertical Gaussian Values copying corresponding values adding values sampled independent Gaussian good predictor noisy predictor chance correlations marginal likelihood Gaussian process ARD parameters optimized scaled conjugate gradients Figure converges relatively large converges smaller small indicating irrelevant predicting ARD framework easily incorporated exponential-quadratic kernel following form kernel useful applications Gaussian processes range regression problems exp xnixmi dimensionality input Gaussian processes classification probabilistic approach goal model posterior probabilities target variable new input set training probabilities lie interval Gaussian process model makes predictions lie entire real easily adapt Gaussian processes classification problems transforming output Gaussian process appropriate nonlinear activation Consider two-class problem target variable define Gaussian process function transform function logistic sigmoid obtain non-Gaussian stochastic process functions illustrated case one-dimensional input space Figure probability distri314 KERNEL METHODS Figure left plot shows sample Gaussian process prior functions right plot shows result transforming sample logistic sigmoid bution target variable Bernoulli distribution denote training set inputs corresponding observed target variables consider single test point target value goal determine predictive distribution left conditioning input variables introduce Gaussian process prior vector components turn defines non-Gaussian process conditioning training data obtain required predictive Gaussian process prior takes form Unlike regression covariance matrix longer includes noise term assume training data points correctly numerical reasons convenient introduce noise-like term governed parameter ensures covariance matrix positive covariance matrix elements positive semidefinite kernel function kind considered Section value typically fixed shall assume kernel function governed vector shall later discuss learned training two-class sufficient predict value required Gaussian Processes predictive distribution integral analytically approximated sampling methods consider techniques based analytical Section derived approximate formula convolution logistic sigmoid Gaussian use result evaluate integral provided Gaussian approximation posterior distribution usual justification Gaussian approximation posterior distribution true posterior tend Gaussian number data points increases consequence central limit case Gaussian number variables grows withSection number data argument does apply consider increasing number data points falling fixed region corresponding uncertainty function leading asymptotically Gaussian different approaches obtaining Gaussian approximation technique based variational inference makes use local variational bound logistic allows product sigmoid functions approximated product Gaussians allowing marginalization performed approach yields lower bound likelihood function variational framework Gaussian process classification extended multiclass problems Gaussian approximation softmax function second approach uses expectation propagation true posterior distribution shall expectation propagation approach good Laplace approximation approach Gaussian process classification based Laplace consider order evaluate predictiveSection distribution seek Gaussian approximation posterior distribution daN daN daN daN KERNEL METHODS used conditional distribution obtained invoking results Gaussian process evaluate integral finding Laplace approximation posterior distribution standard result convolution Gaussian prior zero-mean Gaussian process covariance matrix data term independence data obtain Laplace approximation Taylor expanding logarithm additive normalization constant quantity aTNC tTNaN need mode posterior requires evaluate gradient vector elements simply mode setting gradient depends nonlinearly resort iterative scheme based Newton-Raphson gives rise iterative reweighted squares requires secondSection derivatives require Laplace approximation WN diagonal matrix elements used result derivative logistic sigmoid Note diagonal elements lie range WN positive definite positive definite sum positive definite matrices positive seeExercise Hessian matrix positive definite posterior distribution log convex single mode global Gaussian Processes posterior distribution Hessian function Newton-Raphson formula iterative update equation byExercise anewN WNCN equations iterated converge mode denote gradient satisfy mode evaluate Hessian matrix WN elements WN evaluated defines Gaussian approximation posterior distribution combine evaluate integral corresponds linear-Gaussian use general result giveExercise Gaussian distribution approximate integral result Bayesian logistic regression model Section interested decision boundary corresponding need consider mean ignore effect need determine parameters covariance approach maximize likelihood function need expressions log likelihood suitable regularization terms leading penalized maximum likelihood likelihood function defined daN integral analytically make use Laplace result obtain following approximation log likelihood function KERNEL METHODS need evaluate gradient respect parameter vector Note changes cause changes leading additional terms differentiate respect obtain sets arising dependence covariance matrix rest arising dependence terms arising explicit dependence results Tr CNWN compute terms arising dependence note Laplace approximation constructed zero gradient gives contribution gradient result dependence leaves following contribution derivative respect component CNWN nn used result definition WN evaluate derivative respect differentiating relation respect Rearranging gives WNCN Combining evaluate gradient log likelihood used standard nonlinear optimization algorithms order determine value illustrate application Laplace approximation Gaussian processes synthetic two-class data set shown Figure Extension theAppendix Laplace approximation Gaussian processes involving softmax activation straightforward Gaussian Processes Figure Illustration use Gaussian process showing data left optimal decision boundary true distribution decision boundary Gaussian process classifier right predicted posterior probability blue red classes Gaussian process decision Connection neural networks seen range functions represented neural network governed number hidden sufficiently large two-layer network approximate function arbitrary framework maximum number hidden units needs limited level dependent size training order avoid Bayesian perspective makes little sense limit number parameters network according size training Bayesian neural prior distribution parameter vector conjunction network function produces prior distribution functions vector network Neal shown broad class prior distributions distribution functions generated neural network tend Gaussian process limit limit output variables neural network great merits neural networks outputs share hidden units statistical weights associated hidden unit influenced output variables just property lost Gaussian process seen Gaussian process determined covariance Williams explicit forms covariance case specific choices hidden unit activation function kernel functions expressed function difference consequence Gaussian weight prior centred zero breaks translation invariance weight KERNEL METHODS working directly covariance function implicitly marginalized distribution weight prior governed values determine length scales distribution understood studying examples Figure case finite number hidden Note marginalize hyperparameters instead resort techniques kind discussed Section Exercises www Consider dual formulation squares linear regression problem Section solution components vector expressed linear combination elements vector Denoting coefficients vector dual dual formulation original representation terms parameter vector develop dual formulation perceptron learning perceptron learning rule learned weight vector written linear combination vectors Denote coefficients linear combination derive formulation perceptron learning predictive function terms feature vector enters form kernel function nearest-neighbour classifier assigns new input vector class nearest input vector xn training simplest distance defined Euclidean metric expressing rule terms scalar products making use kernel formulate nearest-neighbour classifier general nonlinear Appendix example matrix positive elements negative eigenvalue positive example converse matrix positive eigenvalues negative www Verify results constructing valid Verify results constructing valid www Verify results constructing valid Verify results constructing valid Verify results constructing valid excellent choice kernel learning function showing linear learning machine based kernel solution proportional Exercises making use expansion expanding middle factor power Gaussian kernel expressed inner product infinite-dimensional feature www Consider space possible subsets fixed set kernel function corresponds inner product feature space dimensionality defined mapping subset element indexed subset denotes subset equal Fisher defined remains invariant make nonlinear transformation parameter vector function invertible www Write form Fisher defined case distribution Gaussian mean fixed covariance considering determinant Gram positivedefinite kernel function satisfies Cauchy-Schwartz inequality Consider parametric model governed parameter vector data set input values nonlinear feature mapping Suppose dependence error function takes form monotonically increasing writing form value minimizes takes form linear combination basis functions www Consider sum-of-squares error function data having noisy distribution Use calculus variations minimize error function respect function optimal solution expansion form basis functions KERNEL METHODS Consider Nadaraya-Watson model input variable target variable having Gaussian components isotropic covariance matrix unit Write expressions conditional density conditional mean variance terms kernel function viewpoint kernel regression comes consideration regression problems input variables target variables corrupted additive Suppose target value generated usual taking function evaluated point adding Gaussian value directly noise corrupted version xn random variable governed distribution Consider set observations corresponding sum-of-squares error function defined averaging distribution input noise minimizing respect function calculus variations optimal solution Nadaraya-Watson kernel regression solution form kernel form www Verify results www Consider Gaussian process regression model kernel function defined terms fixed set nonlinear basis predictive distribution identical result obtained Section Bayesian linear regression note models Gaussian predictive necessary conditional mean variance make use matrix identity make use matrix identity Consider regression problem training set input vectors test set input vectors suppose define Gaussian process prior functions Derive expression joint predictive distribution values marginal distribution test observations tj usual Gaussian process regression result www Consider Gaussian process regression model target variable dimensionality Write conditional distribution test input vector training set input vectors corresponding target observations diagonal matrix elements satisfy Wii positive sum positive definite matrices positive Exercises www Newton-Raphson formula derive iterative update formula finding mode posterior distribution Gaussian process classification result derive expressions mean variance posterior distribution Gaussian process classification Derive result log likelihood function Laplace approximation framework Gaussian process derive results terms gradient log 
Sparse Kernel Machines previous explored variety learning algorithms based nonlinear significant limitations algorithms kernel function evaluated possible pairs xn xm training computationally infeasible training lead excessive computation times making predictions new data chapter shall look kernel-based algorithms sparse predictions new inputs depend kernel function evaluated subset training data begin looking support vector machine popular years ago solving problems novelty important property support vector machines determination model parameters corresponds convex optimization local solution global discussion support vector machines makes extensive use Lagrange reader SPARSE KERNEL MACHINES encouraged review key concepts covered Appendix Additional information support vector machines Vapnik Burges Cristianini Shawe-Taylor et Smola Herbrich SVM decision machine does provide posterior discussed benefits determining probabilities Section alternative sparse kernel known relevance vector machine based Bayesian formulation provides posterior proba-Section bilistic having typically sparser solutions Maximum Margin Classifiers begin discussion support vector machines returning two-class classification problem linear models form denotes fixed feature-space bias parameter Note shall shortly introduce dual representation expressed terms kernel avoids having work explicitly feature training data set comprises input vectors corresponding target values new data points classified according sign shall assume moment training data set linearly separable feature definition exists choice parameters function form satisfies points having points having training data course exist solutions separate classes Section described perceptron algorithm guaranteed solution finite number solution dependent initial values chosen order data points multiple solutions classify training data set try smallest generalization support vector machine approaches problem concept defined smallest distance decision boundary illustrated Figure support vector machines decision boundary chosen margin maximum margin solution motivated computational learning known statistical learning How-Section simple insight origins maximum margin Tong Koller consider framework classification based hybrid generative discriminative model distribution input vectors class Parzen density estimator Gaussian kernels Maximum Margin Classifiers margin Figure margin defined perpendicular distance decision boundary closest data shown left Maximizing margin leads particular choice decision shown location boundary determined subset data known support indicated having common parameter class defines optimal misclassification-rate decision instead optimal determine best hyperplane minimizing probability error relative learned density limit optimal hyperplane shown having maximum intuition result hyperplane increasingly dominated nearby data points relative distant hyperplane independent data points support shall Figure marginalization respect prior distribution parameters Bayesian approach simple linearly separable data set leads decision boundary lies middle region separating data large margin solution similar Recall Figure perpendicular distance point hyperplane defined takes form interested solutions data points correctly distance point xn decision surface margin perpendicular distance closest point xn data wish optimize parameters order maximize maximum margin solution solving arg max minn taken factor outside optimization SPARSE KERNEL MACHINES does depend Direct solution optimization problem shall convert equivalent problem easier note make rescaling distance point xn decision use freedom set point closest data points satisfy constraints known canonical representation decision case data points equality constraints said remainder said active closest margin maximized active optimization problem simply requires maximize equivalent minimizing solve optimization problem arg min subject constraints factor included later example quadratic programming problem trying minimize quadratic function subject set linear inequality appears bias parameter disappeared determined implicitly require changes compensated changes shall works order solve constrained optimization introduce Lagrange multipliers multiplier constraints givingAppendix Lagrangian function Note minus sign Lagrange multiplier minimizing respect maximizing respect Setting derivatives respect equal obtain following conditions Maximum Margin Classifiers Eliminating conditions gives dual representation maximum margin problem maximize respect subject constraints antn kernel function defined takes form quadratic programming problem optimize quadratic function subject set inequality shall discuss techniques solving quadratic programming problems Section solution quadratic programming problem variables general computational complexity going dual formulation turned original optimization involved minimizing dual problem fixed set basis functions number smaller number data dual problem appears allows model reformulated maximum margin classifier applied efficiently feature spaces dimensionality exceeds number data including infinite feature kernel formulation makes clear role constraint kernel function positive ensures Lagrangian function bounded giving rise welldefined optimization order classify new data points trained evaluate sign defined expressed terms parameters kernel function substituting Joseph-Louis Lagrange widely considered French Lagrange born Turin age important contributions mathematics appointed Professor Royal Artillery School Euler worked hard persuade Lagrange eventually did succeeded Euler Director Mathematics Berlin Later moved narrowly escaping life French revolution thanks personal intervention Lavoisier French chemist discovered later executed Lagrange key contributions calculus variations foundations SPARSE KERNEL MACHINES Appendix constrained optimization form satisfies Karush-Kuhn-Tucker case require following properties hold data data point appear sum plays role making predictions new data remaining data points called support satisfy correspond points lie maximum margin hyperplanes feature illustrated Figure property central practical applicability support vector model significant proportion data points discarded support vectors Having solved quadratic programming problem value determine value threshold parameter noting support vector xn satisfies gives denotes set indices support solve equation arbitrarily chosen support vector numerically stable solution obtained multiplying making use t2n averaging equations support vectors solving NS NS total number support later comparison alternative express maximummargin classifier terms minimization error simple quadratic form function zero ensures constraints Note long regularization parameter satisfies precise value plays Figure shows example classification resulting training support vector machine simple synthetic data set Gaussian kernel Maximum Margin Classifiers Figure Example synthetic data classes dimensions showing contours constant obtained support vector machine having Gaussian kernel shown decision margin support form data set linearly separable two-dimensional data space linearly separable nonlinear feature space defined implicitly nonlinear kernel training data points perfectly separated original data example provides geometrical insight origin sparsity maximum margin hyperplane defined location support data points moved freely long remain outside margin changing decision solution independent data Overlapping class distributions assumed training data points linearly separable feature space resulting support vector machine exact separation training data original input space corresponding decision boundary class-conditional distributions case exact separation training data lead poor need way modify support vector machine allow training points case separable implicitly used error function gave infinite error data point misclassified zero error classified optimized model parameters maximize modify approach data points allowed margin penalty increases distance subsequent optimization convenient make penalty linear function introduce slack slack variable training data point Cortes defined data points inside correct margin boundary data point decision boundary points SPARSE KERNEL MACHINES Figure Illustration slack variables Data points circles support exact classification constraints replaced slack variables constrained satisfy Data points correctly classified margin correct Points lie inside correct decision data points lie wrong decision boundary illustrated Figure described relaxing hard margin constraint soft margin allows training set data points Note slack variables allow overlapping class framework sensitive outliers penalty misclassification increases linearly goal maximize margin softly penalizing points lie wrong margin minimize parameter controls trade-off slack variable penalty point misclassified follows upper bound number misclassified parameter analogous inverse regularization coefficient controls trade-off minimizing training errors controlling model limit recover earlier support vector machine separable wish minimize subject constraints corresponding Lagrangian Maximum Margin Classifiers Lagrange corresponding set KKT conditions byAppendix optimize making use definition antn results eliminate obtain dual Lagrangian form identical separable constraints somewhat constraints note required Lagrange implies minimize respect dual variables subject antn known box represents quadratic programming substitute predictions new data points interpret resulting subset data points case contribute predictive SPARSE KERNEL MACHINES model remaining data points constitute support satisfy implies requires points lie Points lie inside margin correctly classified misclassified determine parameter note support vectors satisfy numerically stable solution obtained averaging NM denotes set indices data points having equivalent formulation support vector known proposed et involves maximizing subject constraints antn approach advantage parameter replaces interpreted upper bound fraction margin errors lie wrong margin boundary lower bound fraction support example applied synthetic data set shown Figure Gaussian kernels form exp predictions new inputs support training phase determination parameters makes use data important efficient algorithms solving Maximum Margin Classifiers Figure Illustration applied nonseparable data set support vectors indicated quadratic programming note objective function quadratic local optimum global optimum provided constraints define convex region consequence Direct solution quadratic programming problem traditional techniques infeasible demanding computation memory practical approaches need technique chunking exploits fact value Lagrangian unchanged remove rows columns kernel matrix corresponding Lagrange multipliers value allows quadratic programming problem broken series smaller goal eventually identify nonzero Lagrange multipliers discard Chunking implemented protected conjugate gradients chunking reduces size matrix quadratic function number data points squared approximately number nonzero Lagrange multipliers big fit memory large-scale Decomposition methods et solve series smaller quadratic programming problems designed fixed technique applied arbitrarily large data involves numerical solution quadratic programming subproblems problematic popular approaches training support vector machines called sequential minimal SMO takes concept chunking extreme limit considers just Lagrange multipliers subproblem solved avoiding numerical quadratic programming Heuristics choosing pair Lagrange multipliers considered SMO scaling number data points linear quadratic depending particular seen kernel functions correspond inner products feature spaces working directly terms kernel introducing feature space support vector machines manage avoid curse di336 SPARSE KERNEL MACHINES constraints amongstSection feature values restrict effective dimensionality feature consider simple second-order polynomial kernel expand terms components xTz x1z1 2x1z1 2x2z2 x21z 2x1z1x2z2 2z z21 kernel function represents inner product feature space having mapping input space feature space described vector function coefficients weighting different features constrained specific set points original two-dimensional space constrained lie exactly two-dimensional nonlinear manifold embedded six-dimensional feature highlighted fact support vector machine does provide probabilistic outputs instead makes classification decisions new input Veropoulos et discuss modifications SVM allow trade-off false positive false negative errors wish use SVM module larger probabilistic probabilistic predictions class label new inputs address Platt proposed fitting logistic sigmoid outputs previously trained support vector required conditional probability assumed form defined Values parameters minimizing cross-entropy error function defined training set consisting pairs values data used fit sigmoid needs independent used train original SVM order avoid severe twostage approach equivalent assuming output support vector machine represents log-odds belonging class SVM training procedure specifically intended encourage SVM poor approximation posterior probabilities Relation logistic regression separable re-cast SVM nonseparable distributions terms minimization regularized error allow highlight compared logistic regression seen data points correct margin satisfy yntn Maximum Margin Classifiers Figure Plot error function used support vector shown error function logistic rescaled factor passes point shown shown misclassification error black squared error remaining points objective function written overall multiplicative form hinge error function defined denotes positive hinge error so-called plotted Figure viewed approximation misclassification error function ideally like shown Figure considered logistic regression model Section convenient work target variable comparison support vector reformulate maximum likelihood logistic regression target variable note logistic sigmoid function defined follows used properties logistic sigmoid write construct error function taking negative logarithm likelihood function quadratic takes formExercise SPARSE KERNEL MACHINES comparison error divide error function passes point rescaled error function plotted Figure similar form support vector error key difference flat region leads sparse logistic error hinge loss viewed continuous approximations misclassification continuous error function used solve classification problems squared plotted Figure placing increasing emphasis data points correctly classified long way decision boundary correct points strongly weighted expense misclassified objective minimize misclassification monotonically decreasing error function better Multiclass SVMs support vector machine fundamentally two-class tackle problems involving Various methods proposed combining multiple two-class SVMs order build multiclass commonly used approach construct separate kth model trained data class Ck positive examples data remaining classes negative known one-versus-the-rest Figure saw decisions individual classifiers lead inconsistent results input assigned multiple classes problem addressed making predictions new inputs max heuristic approach suffers problem different classifiers trained different guarantee realvalued quantities different classifiers appropriate problem one-versus-the-rest approach training sets classes equal numbers training data individual classifiers trained data sets comprising negative examples positive symmetry original problem variant one-versus-the-rest scheme proposed Lee et modify target values positive class target negative class target Weston Watkins define single objective function training SVMs based maximizing margin remaining result slower training instead solving separate optimization problems data points overall cost single optimization problem size solved giving overall cost Maximum Margin Classifiers approach train different 2-class SVMs possible pairs classify test points according class highest number approach called saw Figure lead ambiguities resulting large approach requires significantly training time one-versus-the-rest evaluate test significantly computation problem alleviated organizing pairwise classifiers directed acyclic graph confused probabilistic graphical leading DAGSVM et DAGSVM total classify new test point pairwise classifiers need particular classifiers used depending path graph different approach multiclass based error-correcting output developed Dietterich Bakiri applied support vector machines Allwein et viewed generalization voting scheme one-versus-one approach general partitions classes used train individual classes represented particular sets responses two-class classifiers suitable decoding gives robustness errors ambiguity outputs individual application SVMs multiclass classification problems remains open practice one-versus-the-rest approach widely used spite ad-hoc formulation practical single-class support vector solve unsupervised learning problem related probability density Instead modelling density methods aim smooth boundary enclosing region high boundary chosen represent quantile probability data point drawn distribution land inside region fixed number specified restricted problem estimating density sufficient specific approaches problem support vector machines algorithm et tries hyperplane separates fixed fraction training data origin time maximizing distance hyperplane Tax Duin look smallest sphere feature space contains fraction data kernels functions algorithms SVMs regression extend support vector machines regression problems time preserving property simple linear weSection SPARSE KERNEL MACHINES Figure Plot -insensitive error function error increases linearly distance insensitive shown comparison quadratic error function minimize regularized error function obtain sparse quadratic error function replaced error function gives zero error absolute difference prediction target simple example error having linear cost associated errors outside insensitive illustrated Figure minimize regularized error function convention regularization denoted appears error re-express optimization problem introducing slack data point need slack variables corresponds point corresponds point illustrated Figure condition target point lie inside yn yn Introducing slack variables allows points lie outside tube provided slack variables corresponding conditions Maximum Margin Classifiers Figure Illustration SVM showing regression curve insensitive shown examples slack variables Points -tube points -tube points inside -tube error function support vector regression written minimized subject constraints achieved introducing Lagrange multipliers optimizing Lagrangian yn yn substitute set derivatives Lagrangian respect giving results eliminate corresponding variables dual problem involves maximizingExercise SPARSE KERNEL MACHINES respect introduced kernel constrained constraints note required Lagrange require box constraints condition Substituting predictions new inputs expressed terms kernel corresponding Karush-Kuhn-Tucker state solution product dual variables constraints yn yn obtain useful note coefficient nonzero yn implies data point lies upper boundary lies upper boundary nonzero value implies yn points lie lower boundary constraints easily seen adding noting nonnegative strictly data point support vectors data points contribute predictions words points lie boundary outside points tube Maximum Margin Classifiers sparse terms evaluated predictive model involve support parameter considering data point satisfy yn solving obtain used obtain analogous result considering point better average estimates classification alternative formulation SVM regression parameter governing complexity intuitive interpretation et instead fixing width insensitive fix instead parameter bounds fraction points lying outside involves maximizing subject constraints shown data points falling outside insensitive data points support vectors lie tube outside use support vector machine solve regression problem illustrated sinusoidal data set Figure parameters beenAppendix chosen values typically determined SPARSE KERNEL MACHINES Figure Illustration regression applied sinusoidal synthetic data set Gaussian predicted regression curve shown red -insensitive tube corresponds shaded data points shown support vectors indicated blue Computational learning theory support vector machines largely motivated analysed theoretical framework known computational learning called statistical learning theory Kearns origins Valiant formulated probably approximately learning goal PAC framework understand large data set needs order good gives bounds computational cost consider Suppose data set size drawn joint distribution input variable represents class restrict attention situations class labels determined deterministic function PAC learning say function drawn space functions basis training set good generalization expected error rate pre-specified threshold indicator expectation respect distribution quantity left-hand random depends training set PAC framework requires probability greater data set drawn randomly pre-specified terminology approximately comes requirement high probability error rate small choice model space parameters PAC learning aims provide bounds minimum size data set needed meet key quantity PAC learning Vapnik-Chervonenkis VC provides measure complexity space allows PAC framework extended spaces containing infinite number bounds derived PAC framework described Relevance Vector Machines apply choice distribution long training test examples drawn choice function long belongs toF real-world applications machine deal distributions significant example large regions input space carry class consequence lack assumptions form PAC bounds words strongly over-estimate size data sets required achieve generalization PAC bounds practical attempt improve tightness PAC bounds PAC-Bayesian framework considers distribution space somewhat analogous prior Bayesian considers possible choice bounds Relevance Vector Machines Support vector machines used variety classification regression suffer number highlighted outputs SVM represent decisions posterior SVM originally formulated extension classes complexity parameter parameter case hold-out method predictions expressed linear combinations kernel functions centred training data points required positive relevance vector machine RVM Bayesian sparse kernel technique regression classification shares characteristics SVM whilst avoiding principal typically leads sparser models resulting correspondingly faster performance test data whilst maintaining comparable generalization contrast SVM shall convenient introduce regression form RVM consider extension classification RVM regression relevance vector machine regression linear model form studied Chapter modified prior results sparse model defines conditional distribution real-valued target variable input vector takes form SPARSE KERNEL MACHINES noise precision noise mean linear model form fixed nonlinear basis functions typically include constant term corresponding weight parameter represents relevance vector machine specific instance intended mirror structure support vector basis functions kernel associated data points training general expression takes SVM-like form bias number parameters case form predictive model coefficients denoted emphasized subsequent analysis valid arbitrary choices basis generality shall work form contrast restriction positivedefinite basis functions tied number location training data Suppose set observations input vector denote collectively data matrix nth row xTn corresponding target values likelihood function introduce prior distribution parameter vector Chapter shall consider zero-mean Gaussian key difference RVM introduce separate hyperparameter weight parameters wi instead single shared weight prior takes form represents precision corresponding parameter denotes shall maximize evidence respect significant proportion corresponding weight parameters posterior distributions concentrated basis functions associated parameters play role Relevance Vector Machines predictions model effectively pruned resulting sparse result linear regression posterior distribution weights Gaussian takes form mean covariance design matrix elements Note specific case model symmetric kernel matrix elements values determined type-2 maximum known evidence maximize marginal likeli-Section hood function obtained integrating weight parameters represents convolution readily evaluated toExercise log marginal likelihood form lnN defined matrix goal maximize respect hyperparameters requires small modification results obtained Section evidence approximation linear regression identify simply set required derivatives marginal likelihood zero obtain following re-estimation equationsExercise m2i mi ith component posterior mean defined quantity measures corresponding parameter wi determined data defined bySection SPARSE KERNEL MACHINES ith diagonal component posterior covariance Learning proceeds choosing initial values evaluating mean covariance posterior alternately re-estimating re-estimating posterior mean suitable convergence criterion second approach use EM discussed Section approaches finding values hyperparameters maximize evidence formally foundExercise direct optimization approach corresponding gives somewhat faster convergence result proportion hyperparameters driven large principle weight parametersSection wi corresponding hyperparameters posterior distributions mean variance corresponding basis functions removed model play role making predictions new case models form inputs xn corresponding remaining nonzero weights called relevance identified mechanism automatic relevance analogous support vectors worth mechanism achieving sparsity probabilistic models automatic relevance determination quite general applied model expressed adaptive linear combination basis Having values hyperparameters maximize marginal evaluate predictive distribution new input byExercise dw predictive mean set equal posterior mean variance predictive distribution set optimized values just familiar result obtained context linear Recall localized basis predictive variance linear regression models small regions input space basis case RVM basis functions centred data model increasingly certain predictions extrapolating outside domain data course predictive distribution Gaussian process regression does notSection Relevance Vector Machines Figure Illustration RVM regression data Gaussian kernel used Figure regression mean predictive distribution RVM shown red standarddeviation predictive distribution shown shaded data points shown relevance vectors indicated blue Note relevance vectors compared support vectors Figure suffer computational cost making predictions Gaussian processes typically higher Figure shows example RVM applied sinusoidal regression data noise precision parameter determined evidence number relevance vectors RVM significantly smaller number support vectors used wide range regression classification RVM models typically order magnitude compact corresponding support vector resulting significant improvement speed processing test greater sparsity achieved little reduction generalization error compared corresponding principal disadvantage RVM compared SVM training involves optimizing nonconvex training times longer comparable model basis RVM requires inversion matrix size general requires specific case SVM-like model techniques training SVMs cost roughly quadratic case RVM option starting smaller number basis functions relevance vector machine parameters governing complexity noise variance determined automatically single training support vector machine parameters generally involves multiple training section shall derive alternative procedure training relevance vector machine improves training speed Analysis sparsity noted earlier mechanism automatic relevance determination causes subset parameters driven examine SPARSE KERNEL MACHINES Figure Illustration mechanism sparsity Bayesian linear regression showing training set vector target values indicated model basis vector poorly aligned target data vector left model having isotropic corresponding set probable right model finite value case red ellipse corresponds unit Mahalanobis taking value dashed green circle shows contrition arising noise term finite value reduces probability observed probable solution basis vector mechanism sparsity context relevance vector arrive significantly faster procedure optimizing hyperparameters compared direct techniques proceeding mathematical informal insight origin sparsity Bayesian linear Consider data set comprising observations model having single basis function hyperparameter isotropic noise having precision marginal likelihood covariance matrix takes form denotes -dimensional vector similarly Notice just zero-mean Gaussian process model covariance particular observation goal maximizing marginal Figure poor alignment direction training data vector corresponding hyperparameter driven basis vector pruned arises finite value assign lower probability decreasing value density provided set optimal finite value cause distribution elongated direction away increasing probability mass regions away observed data reducing value density target data vector general case Relevance Vector Machines basis vectors similar intuition particular basis vector poorly aligned data vector likely pruned investigate mechanism sparsity mathematical general case involving basis motivate analysis note result re-estimating parameter terms right-hand functions results represent implicit iteration required determine single suggests different approach solving optimization problem make explicit dependence marginal likelihood particular determine stationary points explicitly Tipping pull contribution matrix defined denotes ith column words -dimensional vector elements contrast denotes nth row matrix represents matrix contribution basis function matrix identities determinant inverse written write log marginal likelihood function formExercise simply log marginal likelihood basis function quantity defined si contains dependence introduced quantities si qi si called sparsity qi known quality shall large value si relative value qi means basis function SPARSE KERNEL MACHINES Figure Plots log marginal likelihood versus showing single maximum finite q2i si q2i maximum q2i si q2i likely pruned measures extent basis function overlaps basis vectors represents measure alignment basis vector error training set values vector predictions result model vector excluded stationary points marginal likelihood respect occur derivative equal possible forms Recalling q2i provides q2i solve obtain s2i q2i si solutions illustrated Figure relative size quality sparsity terms determines particular basis vector pruned model complete analysis based second derivatives marginal confirms solutions unique maxima Note approach yielded closed-form solution values providing insight origin sparsity analysis leads practical algorithm optimizing hyperparameters significant speed uses fixed set candidate basis cycles turn decide vector included model resulting sequential sparse Bayesian learning algorithm described Sequential Sparse Bayesian Learning Algorithm solving regression initialize Initialize basis function hyperparameter set remaining hyperparameters initialized included Relevance Vector Machines Evaluate qi si basis Select candidate basis function q2i basis vector included update q2i add evaluate hyperparameter q2i remove basis function set solving regression update converged Note q2i si basis function excluded model action convenient evaluate quantities Qi Si quality sparseness variables expressed form qi Si si Si Note qi Qi si writeExercise Qi Si involve basis vectors correspond finite hyperparameters stage required computations scale like number active basis vectors model typically smaller number training RVM classification extend relevance vector machine framework classification problems applying ARD prior weights probabilistic linear classification model kind studied Chapter start consider two-class problems binary target variable model takes form linear combination basis functions transformed logistic sigmoid function SPARSE KERNEL MACHINES logistic sigmoid function defined introduce Gaussian prior weight vector obtain model considered Chapter difference model uses ARD prior separate precision hyperparameter associated weight contrast regression longer integrate analytically parameter vector follow Tipping use Laplace applied closely related problem Bayesian logisticSection regression Section begin initializing hyperparameter vector value build Gaussian approximation posterior distribution obtain approximation marginal Maximization approximate marginal likelihood leads re-estimated value process repeated Let consider Laplace approximation model fixed value mode posterior distribution obtained maximizing yn 12w TAw const iterative reweighted squares discussed Section need gradient vector Hessian matrix log posterior byExercise diagonal matrix elements bn vector yN design matrix elements used property derivative logistic sigmoid convergence IRLS negative Hessian represents inverse covariance matrix Gaussian approximation posterior mode resulting approximation posterior corresponding mean Gaussian obtained setting giving mean covariance Laplace approximation form use Laplace approximation evaluate marginal general result integral evaluated Laplace Relevance Vector Machines dw substitute set derivative marginal likelihood respect equal obtainExercise Defining rearranging gives identical re-estimation formula obtained regression define write approximate log marginal likelihood form takes form regression apply analysis sparsity obtain fast learning algorithm fully optimize single hyperparameter Figure shows relevance vector machine applied synthetic classification data relevance vectors tend lie region theAppendix decision contrast support vector consistent earlier discussion sparsity basis function centred data point near boundary vector poorly aligned training data vector potential advantages relevance vector machine compared SVM makes probabilistic allows RVM used help construct emission density nonlinear extension linear dynamical tracking faces video sequences et considered RVM binary classification make use probabilistic approach Section linear models form ak wTk SPARSE KERNEL MACHINES Figure Example relevance vector machine applied synthetic data left-hand plot shows decision boundary data relevance vectors indicated Comparison results shown Figure corresponding support vector machine shows RVM gives sparser right-hand plot shows posterior probability RVM output proportion red ink indicates probability point belonging red combined softmax function outputs log likelihood function ytnknk target values tnk 1-of-K coding data point matrix elements Laplace approximation used optimize hyperparameters model Hessian gives principled approach multiclass classification pairwise method used support vector machine provides probabilistic predictions new data principal disadvantage Hessian matrix size number active basis gives additional factor computational cost training compared two-class principal disadvantage relevance vector machine relatively long training times compared avoidance cross-validation runs set model complexity yields sparser computation time test usually important consideration typically Exercises Exercises www Suppose data set input vectors corresponding target values suppose model density input vectors class separately Parzen kernel density estimator Section kernel Write minimum misclassification-rate decision rule assuming classes equal prior kernel chosen classification rule reduces simply assigning new input vector class having closest kernel takes form classification based closest mean feature space right-hand constraint replaced arbitrary constant solution maximum margin hyperplane irrespective dimensionality data data set consisting just data sufficient determine location maximum-margin www value margin maximum-margin hyperplane maximizing subject constraints values previous exercise satisfy defined Consider logistic regression model target variable define negative log addition quadratic regularization takes form Consider Lagrangian regression support vector setting derivatives Lagrangian respect zero substituting eliminate corresponding dual Lagrangian SPARSE KERNEL MACHINES www regression support vector machine considered Section training data points similarly points Verify results mean covariance posterior distribution weights regression www Derive result marginal likelihood function regression performing Gaussian integral technique completing square Repeat time make use general result www direct maximization log marginal likelihood regression relevance vector machine leads re-estimation equations defined evidence framework RVM obtained re-estimation formulae maximizing marginal likelihood Extend approach inclusion hyperpriors gamma distributions form obtain corresponding re-estimation formulae maximizing corresponding posterior probability respect Derive result predictive distribution relevance vector machine predictive variance www results marginal likelihood written form defined sparsity quality factors defined taking second derivative log marginal likelihood regression RVM respect hyperparameter stationary point maximum marginal matrix identity quantities Sn Qn defined written form www gradient vector Hessian matrix log posterior distribution classification relevance vector machine Verify maximization approximate log marginal likelihood function classification relevance vector machine leads result re-estimation 
Graphical Models Probabilities play central role modern pattern seen Chapter probability theory expressed terms simple equations corresponding sum rule product probabilistic inference learning manipulations discussed matter repeated application proceed formulate solve complicated probabilistic models purely algebraic shall highly advantageous augment analysis diagrammatic representations probability called probabilistic graphical offer useful provide simple way visualize structure probabilistic model used design motivate new Insights properties including conditional independence obtained inspection GRAPHICAL MODELS Complex required perform inference learning sophisticated expressed terms graphical underlying mathematical expressions carried graph comprises nodes called connected links known edges probabilistic graphical node represents random variable group random links express probabilistic relationships graph captures way joint distribution random variables decomposed product factors depending subset shall begin discussing Bayesian known directed graphical links graphs particular directionality indicated major class graphical models Markov random known undirected graphical links carry arrows directional Directed graphs useful expressing causal relationships random undirected graphs better suited expressing soft constraints random purposes solving inference convenient convert directed undirected graphs different representation called factor shall focus key aspects graphical models needed applications pattern recognition machine general treatments graphical models books Whittaker Lauritzen Jensen Castillo et Jordan Cowell et Jordan Bayesian Networks order motivate use directed graphs probability consider arbitrary joint distribution variables Note need specify discrete powerful aspects graphical models specific graph make probabilistic statements broad class application product rule probability write joint distribution form second application product time second term righthand gives Note decomposition holds choice joint represent right-hand terms simple graphical model introduce node random variables associate node corresponding conditional distribution right-hand Bayesian Networks Figure directed graphical model representing joint probability distribution variables corresponding decomposition right-hand conditional distribution add directed links graph nodes corresponding variables distribution factor links nodes node factor incoming result graph shown Figure link going node node say node parent node say node child node Note shall make formal distinction node variable corresponds simply use symbol refer interesting point note left-hand symmetrical respect variables right-hand making decomposition implicitly chosen particular chosen different ordering obtained different decomposition different graphical shall return point moment let extend example Figure considering joint distribution variables repeated application product rule joint distribution written product conditional variables choice represent directed graph having conditional distribution right-hand node having incoming links lower numbered say graph fully connected link pair worked completely general joint representations fully connected applicable choice shall absence links graph conveys interesting information properties class distributions graph Consider graph shown Figure fully connected graph link shall graph corresponding representation joint probability distribution written terms product set conditional node conditional distribution conditioned parents corresponding node conditioned joint distribution variables GRAPHICAL MODELS Figure Example directed acyclic graph describing joint distribution variables corresponding decomposition joint distribution reader moment study carefully correspondence Figure state general terms relationship directed graph corresponding distribution joint distribution defined graph nodes conditional distribution node conditioned variables corresponding parents node graph joint distribution pak denotes set parents key equation expresses factorization properties joint distribution directed graphical considered node correspond single equally associate sets variables vector-valued variables nodes easy representation righthand correctly normalized provided individual conditional distributions directed graphs considering subject important restriction directed words closed paths graph node node links following direction arrows end starting graphs called directed acyclic equivalent statement thatExercise exists ordering nodes links node lower numbered Polynomial regression illustration use directed graphs probability consider Bayesian polynomial regression model introduced Bayesian Networks Figure Directed graphical model representing joint distribution corresponding Bayesian polynomial regression model introduced Section tion random variables model vector polynomial coefficients observed data model contains input data xN noise variance hyperparameter representing precision Gaussian prior parameters model random Focussing just random variables joint distribution product prior conditional distributions joint distribution represented graphical model shown Figure start deal complex models later shall inconvenient write multiple nodes form explicitly Figure introduce graphical notation allows multiple nodes expressed draw single representative node surround called labelled indicating nodes Re-writing graph Figure obtain graph shown Figure shall helpful make parameters stochastic make explicit graphical shall adopt convention random variables denoted open deterministic parameters denoted smaller solid graph Figure include deterministic obtain graph shown Figure apply graphical model problem machine learning pattern typically set random variables specific observed Figure representation graph shown Figure introduced plate box labelled represents nodes single example shown GRAPHICAL MODELS Figure shows model Figure deterministic parameters shown explicitly smaller solid xn example variables training set case polynomial curve graphical denote observed variables shading corresponding graph corresponding Figure variables observed shown Figure Note value example latent known hidden variables play crucial role probabilistic models form focus Chapters Having observed values evaluate posterior distribution polynomial coefficients discussed Section note involves straightforward application theorem omitted deterministic parameters order notation model parameters little direct ultimate goal make predictions new input Suppose new input value wish corresponding probability distribution conditioned observed graphical model describes problem shown Figure corresponding joint distribution random variables conditioned deterministic Figure Figure nodes shaded indicate corresponding random variables set observed xn Bayesian Networks Figure polynomial regression corresponding Figure showing new input value bx corresponding model prediction xn required predictive distribution sum rule integrating model parameters dw implicitly setting random variables specific values observed data details calculation discussed Chapter Generative models situations wish draw samples probability shall devote Chapter detailed discussion sampling instructive outline called ancestral particularly relevant graphical Consider joint distribution variables factorizes according corresponding directed acyclic shall suppose variables ordered links node lower numbered words node higher number goal draw sample joint start lowest-numbered node draw sample distribution work nodes node draw sample conditional distribution parent variables set sampled Note parent values available correspond lowernumbered nodes Techniques sampling specific distributions discussed Chapter sampled final variable xK achieved objective obtaining sample joint obtain sample marginal distribution corresponding subset simply sampled values required nodes ignore sampled values remaining draw sample distribution simply sample joint distribution retain values discard remaining values GRAPHICAL MODELS Figure graphical model representing process images objects identity object discrete position orientation object independent prior image vector pixel probability distribution dependent identity object position Image Object OrientationPosition practical applications probabilistic typically highernumbered variables corresponding terminal nodes graph represent lower-numbered nodes corresponding latent primary role latent variables allow complicated distribution observed variables represented terms model constructed simpler exponential conditional interpret models expressing processes observed data consider object recognition task observed data point corresponds image vector pixel latent variables interpretation position orientation particular observed goal posterior distribution integrate possible positions represent problem graphical model form Figure graphical model captures causal process observed data models called generative polynomial regression model described Figure generative probability distribution associated input variable possible generate synthetic data points make generative introducing suitable prior distribution expense complex hidden variables probabilistic model need explicit physical interpretation introduced simply allow complex joint distribution constructed simpler technique ancestral sampling applied generative model mimics creation observed data rise data probability distribution model perfect representation observed producing synthetic observations generative model prove informative understanding form probability distribution represented Discrete variables discussed importance probability distributions members exponential seen family includes well-Section known distributions particular distributions relatively form useful building blocks constructing complex probability Bayesian Networks Figure fully-connected graph describes general distribution K-state discrete variables having total dropping link number parameters reduced framework graphical models useful expressing way building blocks linked models particularly nice properties choose relationship parent-child pair directed graph shall explore examples cases particularly worthy parent child node correspond discrete variables correspond Gaussian cases relationship extended hierarchically construct arbitrarily complex directed acyclic begin examining discrete probability distribution single discrete variable having possible states 1-of-K governed parameters values need specified order define suppose discrete wish model joint denote probability observing x1k x2l parameter x1k denotes kth component similarly joint distribution written parameters subject constraint distribution governed easily seen total number parameters specified arbitrary joint distribution variables KM grows exponentially number product factor joint distribution form corresponds two-node graph link going node node shown Figure marginal distribution governed conditional distribution requires specification parameters possible values total number parameters specified joint distribution suppose variables corresponding graphical model shown Figure variable described GRAPHICAL MODELS Figure chain discrete having requires specification grows linearly length fully connected graph nodes KM grows exponentially xM separate multinomial total number parameters distribution independent discrete having total number parameters grows linearly number graphical reduced number parameters dropping links expense having restricted class discrete variables model joint distribution directed graph variable corresponding conditional distribution node set nonnegative parameters subject usual normalization graph fully connected completely general distribution having KM links graph joint distribution factorizes product total number parameters Graphs having intermediate levels connectivity allow general distributions fully factorized requiring fewer parameters general joint consider chain nodes shown Figure marginal distribution requires conditional distributions requires gives total parameter count quadratic grows linearly length alternative way reduce number independent parameters model sharing parameters known tying chain example Figure arrange conditional distributions governed set parameters governing distribution gives total parameters specified order define joint turn graph discrete variables Bayesian model introducing Dirichlet priors graphical point node acquires additional parent representing Dirichlet distribution parameters associated corresponding discrete illustrated chain model Figure corresponding model tie parameters governing conditional distributions shown Figure way controlling exponential growth number parameters models discrete variables use parameterized models conditional distributions instead complete tables conditional probability illustrate consider graph Figure nodes represent binary parent variables xi governed single Bayesian Networks Figure extension model Figure include Dirichlet priors parameters governing discrete xM Figure Figure single set parameters shared conditional distributions xM ter representing probability giving parameters total parent conditional distribution xM require 2M parameters representing probability 2M possible settings parent general number parameters required specify conditional distribution grow exponentially obtain parsimonious form conditional distribution logistic sigmoid function acting linear combination parent givingSection xM wixi logistic xM vector parent states augmented additional variable value clamped wM vector restricted form conditional distribution general case governed number parameters grows linearly analogous choice restrictive form covariance matrix diagonal multivariate Gaussian motivation logistic sigmoid representation discussed Section Figure graph comprising parents xM single child used illustrate idea parameterized conditional distributions discrete xM GRAPHICAL MODELS Linear-Gaussian models previous saw construct joint probability distributions set discrete variables expressing variables nodes directed acyclic multivariate Gaussian expressed directed graph corresponding linear-Gaussian model component allows impose interesting structure general Gaussian diagonal covariance Gaussian representing opposite widely used techniques examples linear-Gaussian probabilistic principal component factor linear dynamical systems shall make extensive use results section later chapters consider techniques Consider arbitrary directed acyclic graph variables node represents single continuous random variable xi having Gaussian mean distribution taken linear combination states parent nodes pai node wijxj vi wij bi parameters governing vi variance conditional distribution log joint distribution log product conditionals nodes graph takes form 2vi wijxj bi const denotes terms independent quadratic function components joint distribution multivariate determine mean covariance joint distribution recursively variable xi states Gaussian distribution form xi wijxj bi zero unit variance Gaussian random variable satisfying Iij Iij element identity Taking expectation Bayesian Networks Figure directed graph Gaussian missing components starting lowest numbered node working recursively graph assume nodes numbered node higher number use obtain element covariance matrix form recursion relation xj Iijvj covariance similarly evaluated recursively starting lowest numbered Let consider extreme suppose links comprises isolated parameters wij just parameters bi parameters recursion relations mean covariance matrix diagonal form joint distribution total 2D parameters represents set independent univariate Gaussian consider fully connected graph node lower numbered nodes matrix wij entries ith row lower triangular matrix entries leading total number parameters wij obtained taking number elements subtracting account absence elements leading dividing matrix elements giving total total number independent parameters covariance matrix corresponding general symmetric covariance Graphs having intermediate level complexity correspond joint Gaussian distributions partially constrained covariance Consider example graph shown Figure link missing variables recursion relations mean covariance joint distribution byExercise w32b2 w21v1 w32w21v1 w21v1 w221v1 w32w21v1 GRAPHICAL MODELS readily extend linear-Gaussian graphical model case nodes graph represent multivariate Gaussian write conditional distribution node form Wijxj Wij matrix nonsquare xi xj different easy verify joint distribution variables Note encountered specific example linear-Gaussian relationship saw conjugate prior mean GaussianSection variable Gaussian distribution joint distribution corresponds simple two-node graph node representing parent node representing mean distribution parameter controlling viewed value hyperparameter treat Bayesian perspective introducing prior called Gaussian type construction extended principle level illustration hierarchical Bayesian shall encounter examples later Conditional Independence important concept probability distributions multiple variables conditional independence Consider variables suppose conditional distribution does depend value say conditionally independent expressed slightly different way consider joint distribution conditioned write form used product rule probability conditioned joint distribution factorizes product marginal distribution marginal distribution conditioned says variables statistically Note definition conditional independence require Conditional Independence Figure examples graphs variables used discuss conditional independence properties directed graphical equivalently hold possible value just shall use shorthand notation conditional independence denotes conditionally independent equivalent Conditional independence properties play important role probabilistic models pattern recognition simplifying structure model computations needed perform inference learning shall examples expression joint distribution set variables terms product conditional distributions mathematical representation underlying directed principle test potential conditional independence property holds repeated application sum product rules approach time important elegant feature graphical models conditional independence properties joint distribution read directly graph having perform analytical general framework achieving called stands shall motivate concept d-separation general statement d-separation formal proof Lauritzen example graphs begin discussion conditional independence properties directed graphs considering simple examples involving graphs having just motivate illustrate key concepts examples shown Figure joint distribution corresponding graph easily written general result variables investigate independent marginalizing sides respect does factorize product GRAPHICAL MODELS Figure Figure conditioned value variable denotes symbol means conditional independence property does hold hold particular distribution virtue specific numerical values associated various conditional does follow general structure suppose condition variable represented graph Figure easily write conditional distribution form obtain conditional independence property provide simple graphical interpretation result considering path node node node said tail-to-tail respect path node connected tails presence path connecting nodes causes nodes condition node Figure conditioned node path causes similarly consider graph shown Figure joint distribution corresponding graph obtained general formula suppose variables test independent marginalizing Figure second examples 3-node graphs used motivate conditional independence framework directed graphical Conditional Independence Figure Figure conditioning node general does factorize suppose condition node shown Figure obtain obtain conditional independence property interpret results node said head-to-tail respect path node node path connects nodes renders observe Figure observation path obtain conditional independence property consider 3-node shown graph Figure shall subtle behaviour previous joint distribution written general result Consider case variables Marginalizing sides obtain Figure examples 3-node graphs used explore conditional independence properties graphical graph different properties previous GRAPHICAL MODELS Figure Figure conditioning value node act conditioning induces dependence independent variables contrast previous write result suppose condition indicated Figure conditional distribution general does factorize product example opposite behaviour say node head-to-head respect path connects heads node variables conditioning path renders subtlety associated example need introduce say node descendant node path step path follows directions shown head-to-head path unblocked tail-to-tail node head-to-tail node leaves path unblocked unless observed case blocks head-to-head node blocks path observed path worth spending moment understand unusual behaviour graph Figure Consider particular instance graph corresponding problem binary random variables relating fuel shown Figure variables called representing state battery charged flat representing state fuel tank fuel state electric fuel gauge indicates Conditional Independence Figure example 3-node graph used illustrate phenomenon nodes represent state battery state fuel tank reading electric fuel gauge text battery charged independently fuel tank prior probabilities state fuel tank fuel gauge reads probabilities unreliable fuel remaining probabilities determined requirement probabilities sum complete specification probabilistic observe prior probability fuel tank suppose observe fuel gauge discover reads corresponding middle graph Figure use theorem evaluate posterior probability fuel tank evaluate denominator theorem similarly evaluate results GRAPHICAL MODELS observing gauge reads makes likely tank intuitively suppose check state battery observed states fuel gauge shown right-hand graph Figure posterior probability fuel tank observations fuel gauge battery state prior probability cancelled numerator probability tank decreased result observation state accords intuition finding battery flat explains away observation fuel gauge reads state fuel tank battery dependent result observing reading fuel case instead observing fuel gauge observed state descendant Note probability greater prior probability observation fuel gauge reads zero provides evidence favour fuel D-separation general statement d-separation property directed Consider general directed graph arbitrary nonintersecting sets nodes union smaller complete set nodes wish ascertain particular conditional independence statement implied directed acyclic consider possible paths node node path said blocked includes node arrows path meet head-to-tail tail-to-tail node set arrows meet head-to-head set paths said d-separated joint distribution variables graph satisfy concept d-separation illustrated Figure graph path blocked node tail-to-tail node path blocked node head-to-head descendant conditioning conditional independence statement does follow graph path blocked node tail-to-tail node conditional independence property Conditional Independence Figure Illustration concept text satisfied distribution factorizes according Note path blocked node head-to-head node descendant conditioning purposes parameters Figure indicated small filled behave observed marginal distributions associated Consequently parameter nodes parents paths nodes tail-to-tail Consequently play role example conditional independence d-separation provided concept identically data introduced Section Consider problem finding posterior distribution mean univariate Gaussian represented directed graphSection shown Figure joint distribution defined prior set conditional distributions observe goal infer condition consider joint distribution note unique path xi xj path tail-to-tail respect observed node path blocked observations independent Figure Directed graph corresponding problem inferring mean univariate Gaussian distribution observations xN graph drawn plate xN xn GRAPHICAL MODELS Figure graphical representation model Conditioned class label components observed vector assumed xD integrate observations general longer independent latent value example model representing data graph Figure corresponding Bayesian polynomial stochastic nodes correspond node tail-to-tail respect path nodes following conditional independence property conditioned polynomial coefficients predictive distribution independent training data use training data determine posterior distribution coefficients discard training data use posterior distribution make predictions new input observations related graphical structure arises approach classification called naive Bayes use conditional independence assumptions simplify model Suppose observed variable consists D-dimensional vector wish assign observed values 1-of-K encoding represent classes Kdimensional binary vector define generative model introducing multinomial prior class kth component prior probability class conditional distribution observed vector key assumption naive Bayes model conditioned class distributions input variables xD graphical representation model shown Figure observation blocks path xi xj paths tail-to-tail node xi xj conditionally independent marginalize tail-to-tail path xi xj longer tells general marginal density factorize respect components encountered simple application naive Bayes model context fusing data different sources medical diagnosis Section labelled training comprising inputs class fit naive Bayes model training data Conditional Independence maximum likelihood assuming data drawn independently solution obtained fitting model class separately correspondingly labelled suppose probability density class chosen naive Bayes assumption implies covariance matrix Gaussian contours constant density class axis-aligned marginal superposition diagonal Gaussians weighting coefficients class longer factorize respect naive Bayes assumption helpful dimensionality input space making density estimation D-dimensional space useful input vector contains discrete continuous represented separately appropriate models Bernoulli distributions binary observations Gaussians real-valued conditional independence assumption model clearly strong lead poor representations class-conditional assumption precisely model good classification performance practice decision boundaries insensitive details class-conditional illustrated Figure seen particular directed graph represents specific decomposition joint probability distribution product conditional graph expresses set conditional independence statements obtained d-separation d-separation theorem really expression equivalence order make helpful think directed graph Suppose consider particular joint probability distribution variables corresponding nodes filter allow distribution pass expressed terms factorization implied present filter set possible distributions set variables subset distributions passed filter denoted DF directed illustrated Figure use graph different kind filter listing conditional independence properties obtained applying d-separation criterion allowing distribution pass satisfies present possible distributions second kind d-separation theorem tells set distributions allowed precisely set DF emphasized conditional independence properties obtained d-separation apply probabilistic model described particular directed variables discrete continuous combination particular graph describing family probability extreme fully connected graph exhibits conditional independence properties represent possible joint probability distribution set DF contain possible distribu382 GRAPHICAL MODELS DF Figure view graphical model case directed filter probability distribution allowed filter satisfies directed factorization property set possible probability distributions pass filter denoted DF alternatively use graph filter distributions according respect conditional independencies implied d-separation properties d-separation theorem says set distributions DF allowed second kind tions fully disconnected having links corresponds joint distributions factorize product marginal distributions variables comprising nodes Note set distributions DF include distributions additional independence properties described fully factorized distribution passed filter implied graph corresponding set end discussion conditional independence properties exploring concept Markov blanket Markov Consider joint distribution represented directed graph having consider conditional distribution particular node variables xi conditioned remaining variables xj factorization property express conditional distribution form dxi dxi integral replaced summation case discrete observe factor does functional dependence xi taken outside integral cancel numerator factors remain conditional distribution node xi conditional distributions nodes xk node xi conditioning set words xi parent conditional depend parents node conditionals depend children Markov Random Fields Figure Markov blanket node xi comprises set children co-parents property conditional distribution conditioned remaining variables dependent variables Markov xi xi words variables corresponding parents node xk node set nodes comprising children co-parents called Markov blanket illustrated Figure think Markov blanket node xi minimal set nodes isolates xi rest Note sufficient include parents children node xi phenomenon explaining away means observations child nodes block paths observe co-parent nodes Markov Random Fields seen directed graphical models specify factorization joint distribution set variables product local conditional define set conditional independence properties satisfied distribution factorizes according turn second major class graphical models described undirected graphs specify factorization set conditional independence Markov random known Markov network undirected graphical model set nodes corresponds variable group set links connects pair links carry case undirected convenient begin discussion conditional independence Conditional independence properties case directed saw possible test par-Section ticular conditional independence property holds applying graphical test called involved testing paths connecting sets nodes definition somewhat subtle presence paths having head-to-head ask possible define alternative graphical semantics probability distributions conditional independence determined simple graph case corresponds undirected graphical removing GRAPHICAL MODELS Figure example undirected graph path node set node set passes node set Consequently conditional independence property holds probability distribution described directionality links asymmetry parent child nodes subtleties associated head-to-head nodes longer Suppose undirected graph identify sets denoted consider conditional independence property test property satisfied probability distribution defined graph consider possible paths connect nodes set nodes set paths pass nodes set paths conditional independence property path property does necessarily precisely exist distributions corresponding graph satisfy conditional independence illustrated example Figure Note exactly d-separation criterion Testing conditional independence undirected graphs simpler directed alternative way view conditional independence test imagine removing nodes set graph links connect ask exists path connects node node conditional independence property Markov blanket undirected graph takes particularly simple node conditionally independent nodes conditioned neighbouring illustrated Figure Factorization properties seek factorization rule undirected graphs correspond conditional independence involve expressing joint distribution product functions defined sets variables local need decide appropriate notion locality Markov Random Fields Figure undirected Markov blanket node xi consists set neighbouring property conditional distribution conditioned remaining variables dependent variables Markov consider nodes xi xj connected variables conditionally independent nodes follows fact direct path paths pass nodes paths conditional independence property expressed xj denotes set variables xi xj factorization joint distribution xi xj appear factor order conditional independence property hold possible distributions belonging leads consider graphical concept called defined subset nodes graph exists link pairs nodes set nodes clique fully maximal clique clique possible include nodes graph set ceasing concepts illustrated undirected graph variables shown Figure graph cliques nodes maximal cliques set clique missing link define factors decomposition joint distribution functions variables consider functions maximal loss cliques subsets maximal maximal clique define arbitrary function including factor defined subset variables Let denote clique set variables clique xC Figure four-node undirected graph showing clique maximal clique GRAPHICAL MODELS joint distribution written product potential functions maximal cliques graph quantity called partition normalization constant ensures distribution correctly considering potential functions satisfy ensure assumed comprises discrete framework equally applicable continuous combination summation replaced appropriate combination summation Note restrict choice potential functions specific probabilistic interpretation marginal conditional contrast directed graphs factor represents conditional distribution corresponding conditioned state special instance undirected graph constructed starting directed potential functions shall consequence generality potential functions product general correctly introduce explicit normalization factor Recall directed joint distribution automatically normalized consequence normalization conditional distributions presence normalization constant major limitations undirected model discrete nodes having evaluation normalization term involves summing KM states worst exponential size partition function needed parameter learning function parameters govern potential functions evaluation local conditional partition function needed conditional ratio partition function cancels numerator denominator evaluating evaluating local marginal probabilities work unnormalized joint distribution normalize marginals explicitly Provided marginals involves small number evaluation normalization coefficient discussed notion conditional independence based simple graph separation proposed factorization joint distribution intended correspond conditional independence formal connection conditional independence factorization undirected need restrict attention potential functions strictly positive zero negative Markov Random Fields choice make precise relationship factorization conditional return concept graphical model corresponding Figure Consider set possible distributions defined fixed set variables corresponding nodes particular undirected define UI set distributions consistent set conditional independence statements read graph graph define UF set distributions expressed factorization form respect maximal cliques Hammersley-Clifford theorem states sets UI UF restricted potential functions strictly positive convenient express exp called energy exponential representation called Boltzmann joint distribution defined product total energy obtained adding energies maximal contrast factors joint distribution directed potentials undirected graph specific probabilistic gives greater flexibility choosing potential normalization does raise question motivate choice potential function particular viewing potential function expressing configurations local variables preferred Global configurations relatively high probability good balance satisfying influences clique turn specific example illustrate use undirected Image de-noising illustrate application undirected graphs example noise removal binary image Geman simple typical sophisticated Let observed noisy image described array binary pixel values yi index runs shall suppose image obtained taking unknown noise-free described binary pixel values xi randomly flipping sign pixels small example binary noise corrupted image obtained flipping sign pixels probability shown Figure noisy goal recover original noise-free noise level know strong correlation xi know neighbouring pixels xi xj image strongly prior knowledge captured Markov GRAPHICAL MODELS Figure Illustration image de-noising Markov random row shows original binary image left corrupted image randomly changing pixels row shows restored images obtained iterated conditional models left graph-cut algorithm ICM produces image pixels agree original corresponding number graph-cut random field model undirected graph shown Figure graph types contains cliques form associated energy function expresses correlation choose simple energy function cliques form positive desired effect giving lower energy encouraging higher xi yi sign higher energy opposite remaining cliques comprise pairs variables indices neighbouring want energy lower pixels sign opposite choose energy positive potential function nonnegative function maximal multiply nonnegative functions subsets Markov Random Fields Figure undirected graphical model representing Markov random field image xi binary variable denoting state pixel unknown noise-free yi denotes corresponding value pixel observed noisy xi yi equivalently add corresponding allows add extra term hxi pixel noise-free term effect biasing model pixel values particular sign preference complete energy function model takes form xi xixj xiyi defines joint distribution fix elements observed values pixels noisy implicitly defines conditional distribution noisefree example Ising widely studied statistical purposes image wish image having high probability maximum shall use simple iterative technique called iterated conditional ICM simply application coordinate-wise gradient idea initialize variables simply setting xi yi node xj time evaluate total energy possible states xj xj keeping node variables set xj whichever state lower leave probability xj increase variable simple local computation performedExercise repeat update suitable stopping criterion nodes updated systematic instance repeatedly raster scanning choosing nodes sequence updates site visited changes variables definition algorithm GRAPHICAL MODELS Figure Example directed equivalent undirected xN converged local maximum need correspond global purposes simple fixed parameters Note leaving simply means prior probabilities states xi Starting observed noisy image initial run ICM leading de-noised image shown lower left panel Figure Note set effectively removes links neighbouring global probable solution xi yi corresponding observed noisy Later shall discuss effective algorithm finding high probability solutions called max-product typically leads better guaranteed global maximum posterior certain classes including exist efficient algorithms based graph cuts guaranteed global maximum et Boykov et Kolmogorov lower right panel Figure shows result applying graph-cut algorithm de-noising Relation directed graphs introduced graphical frameworks representing probability corresponding directed undirected instructive discuss relation Consider problem taking model specified directed graph trying convert undirected cases simple example Figure joint distribution directed graph product conditionals form let convert undirected graph shown Figure undirected maximal cliques simply pairs neighbouring wish write joint distribution form xN Markov Random Fields Figure Example simple directed graph corresponding moral graph easily identifying xN absorbed marginal node potential Note partition function Let consider generalize convert distribution specified factorization directed graph specified factorization undirected achieved clique potentials undirected graph conditional distributions directed order ensure set variables appears conditional distributions member clique undirected nodes directed graph having just achieved simply replacing directed link undirected nodes directed graph having nodes paths encountered discussion conditional Consider simple directed graph nodes shown Figure joint distribution directed graph takes form factor involves variables belong single clique conditional distribution absorbed clique ensure add extra links pairs parents node process known resulting undirected dropping called moral important observe moral graph example fully connected exhibits conditional independence contrast original directed general convert directed graph undirected add additional undirected links pairs parents node graph GRAPHICAL MODELS drop arrows original links moral initialize clique potentials moral graph conditional distribution factor original directed graph multiply clique exist maximal clique contains variables factor result moralization Note cases partition function process converting directed graph undirected graph plays important role exact inference techniques junction tree Converting undirected directed representation common general presents problems normalization saw going directed undirected representation discard conditional independence properties trivially convert distribution directed graph undirected graph simply fully connected undirected discard conditional independence properties process moralization adds fewest extra links retains maximum number independence seen procedure determining conditional independence properties different directed undirected turns types graph express different conditional independence worth exploring issue return view specific graph set possibleSection distributions variables reduced subset respects conditional independencies implied graph said map distribution conditional independence statement satisfied distribution reflected completely disconnected graph trivial map consider specific distribution ask graphs appropriate conditional independence conditional independence statement implied graph satisfied specific graph said map Clearly fully connected graph trivial map case conditional independence property distribution reflected vice graph said perfect map Figure Venn diagram illustrating set distributions set set distributions represented perfect map directed set represented perfect map undirected UD Inference Graphical Models Figure directed graph conditional independence properties expressed undirected graph perfect map map Consider set distributions distribution exists directed graph perfect set distinct set distributions distribution exists undirected graph perfect addition distributions directed undirected graphs offer perfect illustrated Venn diagram Figure Figure shows example directed graph perfect map distribution satisfying conditional independence properties corresponding undirected graph variables perfect consider undirected graph variables shown Figure graph exhibits properties directed graph variables implies set conditional independence graphical framework extended consistent way graphs include directed undirected called chain graphs contain directed undirected graphs considered far special graphs represent broader class distributions directed undirected remain distributions chain graph provide perfect Chain graphs discussed Figure undirected graph conditional independence properties expressed terms directed graph Inference Graphical Models turn problem inference graphical nodes graph clamped observed wish compute posterior distributions subsets shall exploit graphical structure efficient algorithms GRAPHICAL MODELS Figure graphical representation text make structure algorithms shall algorithms expressed terms propagation local messages shall focus primarily techniques exact Chapter shall consider number approximate inference start let consider graphical interpretation Suppose decompose joint distribution variables product factors form represented directed graph shown Figure suppose observe value indicated shaded node Figure view marginal distribution prior latent variable goal infer corresponding posterior distribution sum product rules probability evaluate used theorem calculate joint distribution expressed terms graphical joint distribution represented graph shown Figure direction arrow simplest example inference problem graphical Inference chain consider complex problem involving chain nodes form shown Figure example lay foundation discussion exact inference general graphs later shall consider undirected graph Figure seen directed chain transformed equivalent undirected directed graph does nodes does require addition extra directed undirected versions graph express exactly set conditional independence Inference Graphical Models joint distribution graph takes form xN shall consider specific case nodes represent discrete variables having case potential function comprises joint distribution Let consider inference problem finding marginal distribution specific node xn way Note observed required marginal obtained summing joint distribution variables xN naive evaluate joint distribution perform summations joint distribution represented set possible value variables KN values evaluation storage joint marginalization obtain involve storage computation scale exponentially length obtain efficient algorithm exploiting conditional independence properties graphical substitute factorized expression joint distribution rearrange order summations multiplications allow required marginal evaluated Consider instance summation xN potential xN depends xN perform summation xN xN function use perform summation involve new function potential place summation involves potential performed separately function summation effectively removes variable viewed removal node group potentials summations express GRAPHICAL MODELS desired marginal form xN xN reader encouraged study re-ordering carefully underlying idea forms basis later discussion general sum-product key concept exploiting multiplication distributive ab ac left-hand involves arithmetic operations righthand reduces Let work computational cost evaluating required marginal re-ordered perform summations states involves function summation involves function table sum table value resulting vector numbers multiplied matrix numbers summations multiplications total cost evaluating marginal linear length contrast exponential cost naive able exploit conditional independence properties simple graph order obtain efficient graph fully conditional independence forced work directly joint powerful interpretation calculation terms passing local messages expression marginal decomposes product factors times normalization constant shall interpret message passed forwards chain node node viewed message passed backwards Inference Graphical Models Figure marginal distribution node xn chain obtained multiplying messages messages evaluated recursively passing messages ends chain node xn xN chain node xn node Note messages comprises set choice product messages interpreted point-wise multiplication elements messages set message evaluated recursively evaluate apply repeatedly reach desired Note carefully structure message passing outgoing message obtained multiplying incoming message local potential involving node variable outgoing variable summing node message evaluated recursively starting node xN recursive message passing illustrated Figure normalization constant easily evaluated summing right-hand states operation requires Graphs form shown Figure called Markov corresponding message passing equations represent example ChapmanKolmogorov equations Markov processes GRAPHICAL MODELS suppose wish evaluate marginals node Simply applying procedure separately node computational cost approach wasteful need propagate message node xN node evaluate need propagate messages node xN node involve duplicated computation messages identical Suppose instead launch message starting node xN propagate corresponding messages way node suppose similarly launch message starting node propagate corresponding messages way forward node xN Provided store intermediate messages node evaluate marginal simply applying computational cost twice finding marginal single times Observe message passed direction link Note normalization constant need evaluated convenient nodes graph corresponding variables simply clamped observed values note effect clamping variable xn observed value expressed multiplying joint distribution copies additional function takes value xn value function absorbed potentials contain Summations xn contain term xn suppose wish calculate joint distribution neighbouring nodes similar evaluation marginal single variables summed moments thought required joint distribution writtenExercise form obtain joint distributions sets variables potentials directly completed message passing required obtain useful result practice wish use parametric forms clique equivalently conditional distributions started directed order learn parameters potentials situations variables employ EM turns local joint distributions conditioned observed precisely needed shall consider examples Chapter Trees seen exact inference graph comprising chain nodes performed efficiently time linear number algorithm Inference Graphical Models Figure Examples treestructured showing undirected directed directed interpreted terms messages passed inference performed efficiently local message passing broader class graphs called shall shortly generalize message passing formalism derived chains sum-product provides efficient framework exact inference tree-structured case undirected tree defined graph path pair graphs case directed tree defined single called nodes convert directed tree undirected moralization step add links nodes consequence corresponding moralized graph undirected Examples undirected directed trees shown Figure Note distribution represented directed tree easily converted represented undirected vice nodes directed graph path direction graph called illustrated Figure graph node property having corresponding moralized undirected graph Factor graphs sum-product algorithm derive section applicable undirected directed trees cast particularly simple general form introduce new graphical construction called factor graph Kschischnang et directed undirected graphs allow global function variables expressed product factors subsets Factor graphs make decomposition explicit introducing additional nodes factors addition nodes representing allow explicit details shall Let write joint distribution set variables form product factors xs denotes subset shall denote GRAPHICAL MODELS Figure Example factor corresponds factorization fa fb fc fd individual variables earlier comprise groups variables vectors factor fs function corresponding set variables Directed factorization defined represent special cases factors local conditional undirected special case factors potential functions maximal cliques normalizing coefficient viewed factor defined set factor node usual variable case directed undirected additional nodes small factor joint undirected links connecting factor node variables nodes factor distribution expressed terms factorization expressed factor graph shown Figure Note factors defined set undirected product factors simply lumped clique combined single potential factor keeps factors explicit able convey detailed information underlying fa fb Figure undirected graph single clique potential factor graph factor representing distribution undirected different factor graph representing factors satisfy Inference Graphical Models fc fa fb Figure directed graph factorization factor graph representing distribution directed factor satisfies different factor graph representing distribution factors Factor graphs said bipartite consist distinct kinds links nodes opposite factor graphs drawn rows nodes nodes factor nodes links shown example Figure ways laying graph example factor graph derived directed undirected shall distribution expressed terms undirected readily convert factor create variable nodes corresponding nodes original undirected create additional factor nodes corresponding maximal cliques factors set equal clique Note different factor graphs correspond undirected concepts illustrated Figure convert directed graph factor simply create variable nodes factor graph corresponding nodes directed create factor nodes corresponding conditional finally add appropriate multiple factor graphs correspond directed conversion directed graph factor graph illustrated Figure noted importance tree-structured graphs performing efficient directed undirected tree convert factor result tree factor graph path connecting case directed conversion undirected graph results loops moralization conversion factor graph results illustrated Figure local cycles directed graph links connecting parents node removed conversion factor graph defining appropriate factor shown Figure seen multiple different factor graphs represent directed undirected allows factor graphs specific GRAPHICAL MODELS Figure directed result converting polytree undirected graph showing creation result converting polytree factor retains tree precise form Figure shows example fully connected undirected graph different factor joint distribution general form specific factorization emphasized factorization does correspond conditional independence sum-product algorithm shall make use factor graph framework derive powerful class exact inference algorithms applicable tree-structured shall focus problem evaluating local marginals nodes subsets lead sum-product Later shall modify technique allow probable state giving rise max-sum shall suppose variables model marginalization corresponds performing equally applicable linear-Gaussian models case marginalization involves shall consider example discuss linear dynamical Figure fragment directed graph having local Conversion fragment factor graph having tree Inference Graphical Models fa fcfb Figure fully connected undirected factor graphs corresponds undirected graph algorithm exact inference directed graphs loops known belief propagation Lauritzen equivalent special case sum-product shall consider sum-product algorithm simpler derive shall assume original graph undirected tree directed tree corresponding factor graph tree convert original graph factor graph deal directed undirected models goal exploit structure graph achieve obtain exact inference algorithm finding situations marginals required allow computations shared begin considering problem finding marginal particular variable node shall suppose variables Later shall modify algorithm incorporate evidence corresponding observed marginal obtained summing joint distribution variables denotes set variables variable idea substitute factor graph expression interchange summations products order obtain efficient Consider fragment graph shown Figure tree structure graph allows partition factors joint distribution group associated factor nodes neighbour variable node joint distribution written product form denotes set factor nodes neighbours Xs denotes set variables subtree connected variable node factor node GRAPHICAL MODELS Figure fragment factor graph illustrating evaluation marginal xfs represents product factors group associated factor Substituting interchanging sums obtain Xs introduced set functions defined Xs viewed messages factor nodes fs variable node required marginal product incoming messages arriving node order evaluate turn Figure note factor described factor write xM GM XsM denoted variables associated factor addition xM factorization illustrated Figure Note set variables set variables factor fs denoted notation Substituting obtain xM xM Xxm xM xM Inference Graphical Models Figure Illustration factorization subgraph associated factor node xm xM fs denotes set variable nodes neighbours factor node denotes set node defined following messages variable nodes factor nodes Xsm introduced distinct kinds factor nodes variable nodes denoted variable nodes factor nodes denoted messages passed link function variable associated variable node link connects result says evaluate message sent factor node variable node link connecting product incoming messages links coming factor multiply factor associated marginalize variables associated incoming illustrated Figure important note factor node send message variable node received incoming messages neighbouring variable derive expression evaluating messages variable nodes factor making use Figure term associated node xm product terms associated factor nodes fl linked node xm node product taken neighbours node xm node Note factors represents subtree original graph precisely kind introduced Substituting GRAPHICAL MODELS Figure Illustration evaluation message sent variable node adjacent factor xm fl fL fs obtain Xml used definition messages passed factor nodes variable evaluate message sent variable node adjacent factor node connecting simply product incoming messages Note variable node neighbours performs computation simply passes messages note variable node send message factor node received incoming messages neighbouring factor Recall goal calculate marginal variable node marginal product incoming messages links arriving messages computed recursively terms order start view node root tree begin leaf definition leaf node variable message sends link illustrated Figure leaf node factor message sent form Figure sum-product algorithm begins messages sent leaf depend leaf node variable factor xf Inference Graphical Models illustrated Figure worth pausing summarize particular version sumproduct algorithm obtained far evaluating marginal start viewing variable node root factor graph initiating messages leaves graph message passing steps applied recursively messages propagated root node received messages node send message root received messages root node received messages required marginal evaluated shall illustrate process node receive messages able send use simple inductive argument graph comprising variable root node connected directly factor leaf algorithm trivially involves sending messages form directly leaves imagine building general graph adding nodes suppose particular graph valid node connected single link overall graph remain new node leaf sends message node turn receive messages requires order send message valid completing suppose wish marginals variable node simply running algorithm afresh wasteful required computations obtain efficient procedure multiple message passing algorithms obtain general sum-product algorithm Arbitrarily pick node designate Propagate messages leaves root root node received messages send messages turn received messages neighbours send messages links going away messages passed outwards root way message passed directions link node received message simple inductive argument used verify validity message passing variableExercise node received messages readily calculate marginal distribution variable number messages computed twice number links graph involves twice computation involved finding single run sum-product algorithm separately computation grow quadratically size Note algorithm fact independent node designated GRAPHICAL MODELS Figure sum-product algorithm viewed purely terms messages sent factor nodes factor outgoing message shown blue arrow obtained taking product incoming messages shown green multiplying factor marginalizing variables fs notion node having special status introduced convenient way explain message passing suppose wish marginal distributions associated sets variables belonging similar argument used easy marginal associated factor theExercise product messages arriving factor node local factor node complete analogy marginals variable factors parameterized functions wish learn values parameters EM marginals precisely quantities need calculate shall discuss hidden Markov model Chapter message sent variable node factor simply product incoming messages wish view sum-product algorithm slightly different form eliminating messages variable nodes factor nodes simply considering messages sent factor easily seen considering example Figure neglected issue factor graph derived directed joint distribution correctly marginals obtained sum-product algorithm similarly normalized started undirected general unknown normalization coefficient simple chain example Figure easily handled working unnormalized version joint run sum-product algorithm corresponding unnormalized marginals coefficient easily obtained normalizing computationally efficient normalization single variable entire set variables required normalize helpful consider simple example illustrate operation sum-product Figure shows simple 4-node factor Inference Graphical Models Figure simple factor graph used illustrate sum-product fa fb fc graph unnormalized joint distribution order apply sum-product algorithm let designate node case leaf nodes Starting leaf following sequence messages direction flow messages illustrated Figure message propagation propagate messages root node leaf GRAPHICAL MODELS Figure Flow messages sum-product algorithm applied example graph Figure leaf nodes root node root node leaf message passed direction evaluate simple let verify marginal correct substituting messages assumed variables graph practical subset variables wish calculate posterior distributions conditioned Observed nodes easily handled sum-product algorithm Suppose partition hidden variables observed variables observed value denoted simply multiply joint distribution product corresponds unnormalized version running sum-product efficiently calculate posterior marginals normalization coefficient value efficiently local summations variables collapse single assumed section dealing discrete specific discrete variables graphical framework probabilistic construction sum-product Inference Graphical Models Table Example joint distribution binary variables maximum joint distribution occurs different variable values compared maxima continuous variables summations simply replaced shall example sum-product algorithm applied graph linear-Gaussian variables consider linear dynamical max-sum algorithm sum-product algorithm allows joint distribution expressed factor graph efficiently marginals component common tasks setting variables largest probability value addressed closely related algorithm called viewed application dynamic programming context graphical models et simple approach finding latent variable values having high probability run sum-product algorithm obtain marginals marginal value maximizes set values individually typically wish set values jointly largest words vector xmax maximizes joint xmax arg max corresponding value joint probability max xmax set easily simple Consider joint distribution binary variables Table joint distribution maximized setting corresponding value marginal obtained summing values similarly marginal marginals maximized corresponds value joint difficult construct examples set individually probable values probability zero joint seek efficient algorithm finding value maximizes joint distribution allow obtain value joint distribution address second shall simply write max operator terms components max max xM GRAPHICAL MODELS total number substitute expansion terms product deriving sum-product use distributive law make use analogous law max operator holds case factors graphical allows exchange products Consider simple example chain nodes described evaluation probability maximum written max max xN xN max xN xN calculation exchanging max product operators results efficient easily interpreted terms messages passed node xN backwards chain node readily generalize result arbitrary tree-structured factor graphs substituting expression factor graph expansion exchanging maximizations structure calculation identical sum-product simply translate results present suppose designate particular variable node start set messages propagating inwards leaves tree node sending message root received incoming messages final maximization performed product messages arriving root gives maximum value called max-product algorithm identical sum-product algorithm summations replaced Note messages sent leaves products small probabilities lead numerical underflow convenient work logarithm joint logarithm monotonic max operator logarithm function max max distributive property preserved taking logarithm simply effect replacing products max-product algorithm obtain max-sum Inference Graphical Models results derived earlier sum-product readily write max-sum algorithm terms message passing simply replacing replacing products sums logarithms max xM initial messages sent leaf nodes obtained analogy root node maximum probability analogy pmax max seen maximum joint distribution propagating messages leaves arbitrarily chosen root result irrespective node chosen turn second problem finding configuration variables joint distribution attains maximum sent messages leaves process evaluating value xmax probable value root node defined xmax arg max tempted simply continue message passing algorithm send messages root apply remaining variable maximizing possible multiple configurations rise maximum value strategy fail possible individual variable values obtained maximizing product messages node belong different maximizing giving overall configuration longer corresponds problem resolved adopting different kind message passing root node let return simple chain example variables xN having GRAPHICAL MODELS Figure diagram showing explicitly possible states row variables xn chain illustration arrow shows direction message passing max-product state variable xn column function defines unique state previous indicated black paths lattice correspond configurations global maximum joint probability tracing black lines opposite direction corresponding graph shown Figure Suppose node xN root propagate messages leaf node root node max follow applying particular initial message sent leaf node simply probable value xN xmaxN arg max xN need determine states previous variables correspond maximizing keeping track values variables gave rise maximum state words storing quantities arg max understand better helpful represent chain variables terms lattice trellis diagram shown Figure Note probabilistic graphical model nodes represent individual states variable corresponds column states state unique state previous variable maximizes probability broken systematically corresponding function indicated Inference Graphical Models lines connecting know probable value final node xN simply follow link probable state node initial node corresponds propagating message chain max known Note values maximum value Provided chose values assured globally consistent maximizing Figure indicated shall suppose corresponds global maximum joint probability represent possible values xmaxN starting state tracing black corresponds iterating obtain valid global maximum Note run forward pass max-sum message passing followed backward pass applied node end selecting states path giving overall configuration global necessary instead track maximizing states forward pass functions use back-tracking consistent extension general tree-structured factor graph message sent factor node variable node maximization performed variable nodes xM neighbours factor perform record values variables xM gave rise back-tracking having use stored values assign consistent maximizing states xmax1 xmaxM max-sum gives exact maximizing configuration variables provided factor graph important application technique finding probable sequence hidden states hidden Markov case known Viterbi sum-product inclusion evidence form observed variables observed variables clamped observed maximization performed remaining hidden shown formally including identity functions observed variables factor did sum-product interesting compare max-sum iterated conditional modes algorithm described page step ICM computationally simpler passed node comprise single value consisting new state node conditional distribution max-sum algorithm complex messages functions node variables comprise set values possible state Unlike ICM guaranteed global maximum tree-structured GRAPHICAL MODELS Exact inference general graphs sum-product max-sum algorithms provide efficient exact solutions inference problems tree-structured practical deal graphs having message passing framework generalized arbitrary graph giving exact inference procedure known junction tree algorithm brief outline key steps intended convey detailed understanding flavour various stages starting point directed converted undirected graph starting undirected graph step graph involves finding chord-less cycles containing nodes adding extra links eliminate chord-less graph Figure cycle chord-less link added alternatively Note joint distribution resulting triangulated graph defined product potential considered functions expanded sets triangulated graph used construct new tree-structured undirected graph called join nodes correspond maximal cliques triangulated links connect pairs cliques variables selection pairs cliques connect way important maximal spanning tree defined possible trees link chosen weight tree weight link number nodes shared cliques weight tree sum weights tree clique subset clique absorbed larger gives junction consequence triangulation resulting tree satisfies running intersection means variable contained contained clique path connects ensures inference variables consistent two-stage message passing essentially equivalent sum-product applied junction tree order marginals junction tree algorithm sounds heart simple idea used exploiting factorization properties distribution allow sums products interchanged partial summations avoiding having work directly joint role junction tree provide precise efficient way organize worth emphasizing achieved purely graphical junction tree exact arbitrary graphs efficient sense graph does general exist computationally cheaper algorithm work joint distributions node corresponds clique triangulated computational cost algorithm determined number variables largest Inference Graphical Models clique grow exponentially number case discrete important concept treewidth graph defined terms number variables largest defined size largest ensure tree treewidth general multiple different junction trees constructed starting treewidth defined junction tree largest clique fewest treewidth original graph junction tree algorithm Loopy belief propagation problems practical feasible use exact need exploit effective approximation important class broadly called variational discussed Chapter Complementing deterministic approaches wide range sampling called Monte Carlo based stochastic numerical sampling distributions discussed length Chapter consider simple approach approximate inference graphs builds directly previous discussion exact inference idea simply apply sum-product algorithm guarantee yield good approach known loopy belief propagation possible message passing rules sum-product algorithm purely graph information flow times algorithm order apply need define message passing Let assume message passed time link message sent node replaces previous message sent direction link function recent messages received node previous steps seen message sent link node messages received node loops raises problem initiate message passing resolve suppose initial message unit function passed link node position send possible ways organize message passing flooding schedule simultaneously passes message link directions time schedules pass message time called serial Following Kschischnang et say node message pending link node node received message links time send message node receives message creates pending messages pending messages need transmitted GRAPHICAL MODELS messages simply duplicate previous message graphs tree schedule sends pending messages eventually terminate message passed direction pending product receivedExercise messages variable exact graphs having algorithm terminate pending practice generally converge reasonable time algorithm stopped convergence local marginals computed product recently received incoming messages variable node factor node loopy belief propagation algorithm poor applications proven state-of-the-art algorithms decoding certain kinds error-correcting codes equivalent loopy belief propagation Berrou et McEliece et MacKay Learning graph structure discussion inference graphical assumed structure graph known going inference problem learning graph structure data requires define space possible structures measure used score Bayesian ideally like compute posterior distribution graph structures make predictions averaging respect prior graphs indexed posterior distribution observed data model evidence provides score evaluation evidence involves marginalization latent variables presents challenging computational problem Exploring space structures number different graph structures grows exponentially number necessary resort heuristics good Exercises www marginalizing variables representation joint distribution directed graph correctly provided conditional distributions www property directed cycles directed graph follows statement exists ordered numbering nodes node links going lower-numbered Exercises Table joint distribution binary Consider binary variables having joint distribution Table direct evaluation distribution property marginally independent conditioned Evaluate distributions corresponding joint distribution Table direct evaluation Draw corresponding directed www Draw directed probabilistic graphical model corresponding relevance vector machine described model shown Figure seen number parameters required specify conditional distribution xM xi reduced 2M making use logistic sigmoid representation alternative representation xM parameters represent probabilities additional parameters satisfying conditional distribution known interpreted form logical function function gives xi Discuss interpretation recursion relations mean covariance joint distribution graph shown Figure www implies www d-separation conditional distribution node directed conditioned nodes Markov independent remaining variables GRAPHICAL MODELS Figure Example graphical model used explore conditional independence properties head-to-head path descendant node Consider directed graph shown Figure variables Suppose observe variable general Consider example car fuel shown Figure suppose instead observing state fuel gauge gauge seen driver reports reading report gauge shows shows driver bit expressed following probabilities Suppose driver tells fuel gauge shows words observe Evaluate probability tank evaluate corresponding probability observation battery note second probability Discuss intuition relate result Figure www distinct undirected graphs set distinct random Draw possibilities case Consider use iterated conditional modes minimize energy function Write expression difference values energy associated states particular variable xj variables held depends quantities local xj Consider particular case energy function coefficients probable configuration latent variables xi yi www joint distribution neighbouring nodes graph shown Figure expression form Exercises Consider inference problem evaluating graph shown Figure nodes message passing algorithm discussed Section used solve discuss messages modified Consider graph form shown Figure having nodes Use d-separation message passing algorithm Section applied evaluation result independent value www distribution represented directed tree trivially written equivalent distribution corresponding undirected distribution expressed undirected tree suitable normalization clique written directed Calculate number distinct directed trees constructed undirected Apply sum-product algorithm derived Section chain-ofnodes model discussed Section results recovered special www Consider message passing protocol sum-product algorithm tree-structured factor graph messages propagated leaves arbitrarily chosen root node root node Use proof induction messages passed order node send message received incoming messages necessary construct outgoing www marginal distributions sets variables xs associated factors factor graph running sum-product message passing algorithm evaluating required marginals Consider tree-structured factor subset variable nodes form connected subgraph variable node subset connected variable nodes single factor sum-product algorithm used compute marginal distribution www Section showed marginal distribution variable node xi factor graph product messages arriving node neighbouring factor nodes form marginal written product incoming message links outgoing message marginal distribution variables xs factor tree-structured factor running sum-product message passing written product message arriving factor node times local factor form GRAPHICAL MODELS verified sum-product algorithm run graph Figure node designated root node gives correct marginal correct marginals obtained use result running sum-product algorithm graph gives correct joint distribution Consider tree-structured factor graph discrete suppose wish evaluate joint distribution associated variables xa xb belong common Define procedure sumproduct algorithm evaluate joint distribution variables successively clamped allowed Consider discrete variables having possible example Construct joint distribution variables having property value maximizes marginal value maximizes marginal probability zero joint www concept pending message sum-product algorithm factor graph defined Section graph pending message irrespective long algorithm www sum-product algorithm run factor graph tree structure finite number messages pending 
Mixture Models EM define joint distribution observed latent corresponding distribution observed variables obtained allows relatively complex marginal distributions observed variables expressed terms tractable joint distributions expanded space observed latent introduction latent variables allows complicated distributions formed simpler shall mixture Gaussian mixture discussed Section interpreted terms discrete latent Continuous latent variables form subject Chapter providing framework building complex probability mixture models used cluster begin discussion mixture distributions considering problem finding clusters set data approach nonprobabilistic technique called K-means algorithm introduce latent variableSection MIXTURE MODELS EM view mixture distributions discrete latent variables interpreted defining assignments data points specific components gen-Section eral technique finding maximum likelihood estimators latent variable models expectation-maximization use Gaussian mixture distribution motivate EM algorithm fairly informal careful treatment based latent variable shallSection K-means algorithm corresponds particular nonprobabilistic limit EM applied mixtures discuss EM Gaussian mixture models widely used data pattern machine statistical parameters determined maximum typically EM shall significant limitations maximum likelihood Chapter shall elegant Bayesian treatment framework variational requires little additional computation compared resolves principal difficulties maximum likelihood allowing number components mixture inferred automatically K-means Clustering begin considering problem identifying data points multidimensional Suppose data set consisting observations random D-dimensional Euclidean variable goal partition data set number shall suppose moment value think cluster comprising group data points inter-point distances small compared distances points outside formalize notion introducing set D-dimensional vectors prototype associated kth shall think representing centres goal assignment data points set vectors sum squares distances data point closest vector convenient point define notation assignment data points data point introduce corresponding set binary indicator variables rnk describing clusters data point xn assigned data point xn assigned cluster rnk rnj known 1-of-K coding define objective called distortion represents sum squares distances data point K-means Clustering assigned vector goal values minimize iterative procedure iteration involves successive steps corresponding successive optimizations respect rnk choose initial values phase minimize respect keeping second phase minimize respect keeping rnk two-stage optimization repeated shall stages updating rnk updating correspond respectively steps EM emphasize shall use theSection terms step step context K-means Consider determination linear function optimization performed easily closed form terms involving different independent optimize separately choosing rnk whichever value gives minimum value simply assign nth data point closest cluster expressed rnk arg minj consider optimization rnk held objective function quadratic function minimized setting derivative respect zero giving easily solve rnk denominator expression equal number points assigned cluster result simple set equal mean data points xn assigned cluster procedure known K-means phases re-assigning data points clusters re-computing cluster means repeated turn change assignments maximum number iterations phase reduces value objective function convergence algorithm How-Exercise converge local global minimum convergence properties K-means algorithm studied MacQueen K-means algorithm illustrated Old Faithful data set Fig-Appendix ure purposes linear re-scaling known variables zero mean unit standard chosen MIXTURE MODELS EM Figure Illustration K-means algorithm re-scaled Old Faithful data Green points denote data set two-dimensional Euclidean initial choices centres shown red blue initial data point assigned red cluster blue according cluster centre equivalent classifying points according perpendicular bisector cluster shown magenta lie subsequent cluster centre re-computed mean points assigned corresponding successive steps final convergence K-means Clustering Figure Plot cost function step step Kmeans algorithm example shown Figure algorithm converged final EM cycle produces changes assignments prototype assignment data point nearest cluster centre equivalent classification data points according lie perpendicular bisector cluster plot cost function Old Faithful example shown Figure Note deliberately chosen poor initial values cluster centres algorithm takes steps better initialization procedure choose cluster centres equal random subset data worth noting K-means algorithm used initialize parameters Gaussian mixture model applying EM direct implementation K-means algorithm discussed relatively step necessary compute Euclidean distance prototype vector data Various schemes proposed speeding K-means based precomputing data structure tree nearby points subtree approaches make use triangle inequality avoiding unnecessary distance calculations considered batch version K-means data set used update prototype derive on-line stochastic algorithm applying Robbins-Monro procedureSection problem finding roots regression function derivatives respect leads sequential update eachExercise data point xn update nearest prototype old learning rate typically decrease monotonically data points K-means algorithm based use squared Euclidean distance measure dissimilarity data point prototype does limit type data variables considered inappropriate cases variables represent categorical labels MIXTURE MODELS EM make determination cluster means nonrobust WeSection generalize K-means algorithm introducing general dissimilarity measure vectors minimizing following distortion measure gives K-medoids step cluster prototypes assigning data point cluster dissimilarity corresponding prototype computational cost case standard K-means general choice dissimilarity step potentially complex common restrict cluster prototype equal data vectors assigned allows algorithm implemented choice dissimilarity measure long readily step cluster discrete search Nk points assigned requires evaluations notable feature K-means algorithm data point assigned uniquely data points closer particular centre data points lie roughly midway cluster clear hard assignment nearest cluster shall section adopting probabilistic obtain assignments data points clusters way reflects level uncertainty appropriate probabilistic formulation brings numerous Image segmentation compression illustration application K-means consider related problems image segmentation image goal segmentation partition image regions reasonably homogeneous visual appearance corresponds objects parts objects pixel image point 3-dimensional space comprising intensities green segmentation algorithm simply treats pixel image separate data Note strictly space Euclidean channel intensities bounded interval apply K-means algorithm illustrate result running K-means particular value re-drawing image replacing pixel vector intensity triplet centre pixel Results various values shown Figure value algorithm representing image palette emphasized use K-means particularly sophisticated approach image takes account spatial proximity different image segmentation problem general extremely difficult K-means Clustering Original image Figure examples application K-means clustering algorithm image segmentation showing initial images K-means segmentations obtained various values illustrates use vector quantization data smaller values higher compression expense poorer image remains subject active research introduced simply illustrate behaviour K-means use result clustering algorithm perform data important distinguish lossless data goal able reconstruct original data exactly compressed lossy data accept errors reconstruction return higher levels compression achieved lossless apply K-means algorithm problem lossy data compression data store identity cluster store values cluster centres typically requires significantly provided choose data point approximated nearest centre New data points similarly compressed finding nearest storing label instead original data framework called vector vectors called code-book MIXTURE MODELS EM image segmentation problem discussed provides illustration use clustering data Suppose original image pixels comprising values stored bits transmit image directly cost 24N suppose run K-means image instead transmitting original pixel intensity vectors transmit identity nearest vector requires log2 bits transmit code book vectors requires 24K total number bits required transmit image 24K log2 nearest original image shown Figure pixels requires bits transmit compressed images require bits bits bits represent compression ratios compared original image trade-off degree compression image Note aim example illustrate K-means aiming produce good image fruitful consider small blocks adjacent instance exploit correlations exist natural images nearby Mixtures Gaussians Section motivated Gaussian mixture model simple linear superposition Gaussian aimed providing richer class density models single turn formulation Gaussian mixtures terms discrete latent provide deeper insight important serve motivate expectation-maximization Recall Gaussian mixture distribution written linear superposition Gaussians form Let introduce K-dimensional binary random variable having 1-of-K representation particular element zk equal elements equal values zk satisfy zk zk possible states vector according element shall define joint distribution terms marginal distribution conditional distribution corresponding graphical model Figure marginal distribution specified terms mixing coefficients Mixtures Gaussians Figure Graphical representation mixture joint distribution expressed form parameters satisfy order valid uses 1-of-K write distribution form conditional distribution particular value Gaussian written form joint distribution marginal distribution obtained summing joint distribution possible states giveExercise use marginal distribution Gaussian mixture form observations represented marginal distribution form follows observed data point xn corresponding latent variable equivalent formulation Gaussian mixture involving explicit latent gained doing able work joint distribution MIXTURE MODELS EM instead marginal distribution lead significant notably introduction expectation-maximization quantity play important role conditional probability shall use denote value theorem shall view prior probability zk quantity corresponding posterior probability observed shall viewed responsibility component takes observation use technique ancestral sampling generate random samplesSection distributed according Gaussian mixture generate value denote marginal distribution generate value conditional distribution Techniques sampling standard distributions discussed Chapter depict samples joint distribution plotting points corresponding values colouring according value words according Gaussian component responsible generating shown Figure Similarly samples marginal distribution obtained taking samples joint distribution ignoring values illustrated Figure plotting values coloured use synthetic data set illustrate data posterior probability component mixture distribution data set represent value responsibilities associated data point xn plotting corresponding point proportions green ink shown Figure data point coloured coloured equal proportions blue green ink appear compared Figure data points labelled true identity component Maximum likelihood Suppose data set observations wish model data mixture represent data set Mixtures Gaussians Figure Example points drawn mixture Gaussians shown Figure Samples joint distribution states corresponding components depicted corresponding samples marginal distribution obtained simply ignoring values just plotting data set said samples colours represent value responsibilities associated data point obtained plotting corresponding point proportions green ink respectively matrix nth row xTn corresponding latent variables denoted matrix rows zTn assume data points drawn independently express Gaussian mixture model data set graphical representation shown Figure log likelihood function discussing maximize worth emphasizing significant problem associated maximum likelihood framework applied Gaussian mixture presence consider Gaussian mixture components covariance matrices unit conclusions hold general covariance Suppose components mixture let say jth mean exactly equal data Figure Graphical representation Gaussian mixture model set data points corresponding latent points xn MIXTURE MODELS EM Figure Illustration singularities likelihood function arise mixtures compared case single Gaussian shown Figure singularities points xn value data point contribute term likelihood function form consider limit term goes infinity log likelihood function maximization log likelihood function posed problem singularities present occur Gaussian components specific data Recall problem did arise case single Gaussian understand note single Gaussian collapses data point contribute multiplicative factors likelihood function arising data points factors zero exponentially giving overall likelihood goes zero components components finite variance assign finite probability data points component shrink specific data point contribute increasing additive value log illustrated Figure singularities provide example severe over-fitting occur maximum likelihood shall difficulty does occur adopt Bayesian simply note applying maximum likelihood Gaussian mixture models steps avoid finding pathological solutions instead seek local maxima likelihood function hope avoid singularities suitable instance detecting Gaussian component collapsing resetting mean randomly chosen value resetting covariance large continuing issue finding maximum likelihood solutions arises fact maximum likelihood K-component mixture total equivalent solutions corresponding ways assigning sets parameters point space parameter values additional points rise exactly problem known Mixtures Gaussians identifiability important issue wish interpret parameter values discovered Identifiability arise discuss models having continuous latent variables Chapter purposes finding good density irrelevant equivalent solutions good Maximizing log likelihood function Gaussian mixture model turns complex problem case single difficulty arises presence summation appears inside logarithm logarithm function longer acts directly set derivatives log likelihood longer obtain closed form shall approach apply gradient-based optimization techniques Nocedal Bishop gradient-based techniques play important role discuss mixture density networks Chapter consider alternative approach known EM algorithm broad applicability lay foundations discussion variational inference techniques Chapter EM Gaussian mixtures elegant powerful method finding maximum likelihood solutions models latent variables called expectation-maximization EM algorithm et McLachlan Later shall general treatment shall EM generalized obtain variational inference shall motivate EMSection algorithm giving relatively informal treatment context Gaussian mixture EM broad encountered context variety different models Let begin writing conditions satisfied maximum likelihood Setting derivatives respect means Gaussian components obtain use form Gaussian Note posterior appear naturally right-hand Multiplying assume rearranging obtain Nk defined Nk MIXTURE MODELS EM interpret Nk effective number points assigned cluster Note carefully form mean kth Gaussian component obtained taking weighted mean points data weighting factor data point xn posterior probability component responsible generating set derivative respect follow similar line making use result maximum likelihood solution covariance matrix single obtainSection Nk form corresponding result single Gaussian fitted data data point weighted corresponding posterior probability denominator effective number points associated corresponding maximize respect mixing coefficients account constraint requires mixing coefficients sum achieved Lagrange multiplier andAppendix maximizing following quantity gives appearance multiply sides sum making use constraint eliminate rearranging obtain Nk mixing coefficient kth component average responsibility component takes explaining data worth emphasizing results constitute closed-form solution parameters mixture model responsibilities depend parameters complex way results suggest simple iterative scheme finding solution maximum likelihood shall turns instance EM algorithm particular case Gaussian mixture choose initial values mixing alternate following updates shall step Mixtures Gaussians Figure Illustration EM algorithm Old Faithful set used illustration K-means algorithm Figure text reasons apparent expectation use current values parameters evaluate posterior use probabilities maximization re-estimate mixing coefficients results Note doing evaluate new means use new values covariances keeping corresponding result single Gaussian shall update parameters resulting step followed step guaranteed increase log likelihood algorithm deemed converged changeSection log likelihood alternatively falls illustrate EM algorithm mixture Gaussians applied rescaled Old Faithful data set Figure mixture Gaussians centres initialized values K-means algorithm Figure precision matrices initialized proportional unit Plot shows data points initial configuration mixture model standard-deviation contours MIXTURE MODELS EM Gaussian components shown blue red Plot shows result initial data point depicted proportion blue ink equal posterior probability having generated blue corresponding proportion red ink posterior probability having generated red points significant probability belonging cluster appear situation step shown plot mean blue Gaussian moved mean data weighted probabilities data point belonging blue words moved centre mass blue covariance blue Gaussian set equal covariance blue Analogous results hold red Plots results complete cycles plot algorithm close Note EM algorithm takes iterations reach convergence compared K-means cycle requires significantly common run K-means algorithm order suitable initialization Gaussian mixture model subsequently adapted covariance matrices conveniently initialized sample covariances clusters K-means mixing coefficients set fractions data points assigned respective gradient-based approaches maximizing log techniques employed avoid singularities likelihood function Gaussian component collapses particular data emphasized generally multiple local maxima log likelihood EM guaranteed largest EM algorithm Gaussian mixtures plays important summarize EM Gaussian Mixtures Gaussian mixture goal maximize likelihood function respect parameters means covariances components mixing Initialize means covariances mixing coefficients evaluate initial value log Evaluate responsibilities current parameter values Alternative View EM Re-estimate parameters current responsibilities Nk Nk Nk Nk Evaluate log likelihood check convergence parameters log convergence criterion satisfied return step Alternative View EM present complementary view EM algorithm recognizes key role played latent discuss approach abstract illustration consider case Gaussian goal EM algorithm maximum likelihood solutions models having latent denote set observed data nth row represents xTn similarly denote set latent variables corresponding row zTn set model parameters denoted log likelihood function Note discussion apply equally continuous latent variables simply replacing sum key observation summation latent variables appears inside joint distribution belongs exponential MIXTURE MODELS EM marginal distribution typically does result presence sum prevents logarithm acting directly joint resulting complicated expressions maximum likelihood suppose observation told corresponding value latent variable shall complete data shall refer actual observed data illustrated Figure likelihood function complete data set simply takes form shall suppose maximization complete-data log likelihood function complete data set incomplete data state knowledge values latent variables posterior distribution use complete-data log consider instead expected value posterior distribution latent corresponds shall step EM subsequent maximize current estimate parameters denoted pair successive steps gives rise revised estimate algorithm initialized choosing starting value parameters use expectation somewhat shall motivation choice deeper treatment EM Section use current parameter values posterior distribution latent variables use posterior distribution expectation complete-data log likelihood evaluated general parameter value denoted determine revised parameter estimate maximizing function arg max Note definition logarithm acts directly joint distribution corresponding M-step maximization general EM algorithm summarized shall cycle EM increase incomplete-data log likelihood local General EM Algorithm joint distribution observed variables latent variables governed parameters goal maximize likelihood function respect Choose initial setting parameters Alternative View EM step Evaluate step Evaluate arg max Check convergence log likelihood parameter convergence criterion let return step EM algorithm used MAP solutions models prior defined case EExercise step remains maximum likelihood step quantity maximized Suitable choices prior remove singularities kind illustrated Figure considered use EM algorithm maximize likelihood function discrete latent applied unobserved variables correspond missing values data distribution observed values obtained taking joint distribution variables marginalizing missing EM used maximize corresponding likelihood shall example application technique context principal component analysis Figure valid procedure data values missing meaning mechanism causing values missing does depend unobserved situations instance sensor fails return value quantity measuring exceeds Gaussian mixtures revisited consider application latent variable view EM specific case Gaussian mixture Recall goal maximize log likelihood function computed observed data set saw difficult case single Gaussian distribution presence summation occurs inside Suppose addition observed data set values corresponding discrete variables Recall Figure shows data set includes labels showing component generated data Figure shows corresponding data graphical model complete data shown Figure MIXTURE MODELS EM Figure shows graph Figure suppose discrete variables data variables xn consider problem maximizing likelihood complete data set likelihood function takes form znk denotes kth component Taking obtain znk lnN Comparison log likelihood function incomplete data shows summation logarithm logarithm acts directly Gaussian member exponential leads simpler solution maximum likelihood Consider maximization respect means K-dimensional vector elements equal single element having value complete-data log likelihood function simply sum independent mixture maximization respect mean covariance exactly single involves subset data points maximization respect mixing note coupled different values virtue summation constraint enforced Lagrange multiplier leads result znk mixing coefficients equal fractions data points assigned corresponding complete-data log likelihood function maximized trivially closed values latent variables discussed consider respect posterior distribution latent complete-data log Alternative View EM posterior distribution takes form factorizes posterior distribution easily verified inspection directed graph Figure making use d-separation expected value indicatorSection variable znk posterior distribution znk znk znj just responsibility component data point expected value complete-data log likelihood function lnN proceed choose initial values parameters use evaluate responsibilities responsibilities fixed maximize respect leads closed form solutions precisely EM algorithm forExercise Gaussian mixtures derived shall gain insight role expected complete-data log likelihood function proof convergence EM algorithm Section Relation K-means Comparison K-means algorithm EM algorithm Gaussian mixtures shows close K-means algorithm performs hard assignment data points data point associated uniquely EM algorithm makes soft assignment based posterior derive K-means algorithm particular limit EM Gaussian mixtures Consider Gaussian mixture model covariance matrices mixture components variance parameter shared MIXTURE MODELS EM identity exp consider EM algorithm mixture Gaussians form treat fixed instead parameter posterior particular data point exp exp consider limit denominator term smallest zero responsibilities data point xn zero term responsibility Note holds independently values long obtain hard assignment data points just K-means rnk rnk defined data point assigned cluster having closest EM re-estimation equation reduces K-means result Note re-estimation formula mixing coefficients simply re-sets value equal fraction data points assigned cluster parameters longer play active role limit expected complete-data log becomesExercise maximizing expected complete-data log likelihood equivalent minimizing distortion measure K-means algorithm Note K-means algorithm does estimate covariances clusters cluster hard-assignment version Gaussian mixture model general covariance known elliptical K-means considered Sung Poggio Mixtures Bernoulli distributions far focussed distributions continuous variables described mixtures example mixture illustrate EM algorithm different discuss mixtures discrete binary variables described Bernoulli model known latent class analysis McLachlan practical importance discussion Bernoulli mixtures lay foundation consideration hidden Markov models discrete Alternative View EM Consider set binary variables governed Bernoulli distribution parameter individual variables xi mean covariance distribution easily seen let consider finite mixture distributions mean covariance mixture distribution byExercise diag covariance matrix longer mixture distribution capture correlations unlike single Bernoulli data set log likelihood function model appearance summation inside maximum likelihood solution longer closed derive EM algorithm maximizing likelihood function mixture Bernoulli introduce explicit latent MIXTURE MODELS EM variable associated instance case Gaussian binary K-dimensional variable having single component equal components equal write conditional distribution latent prior distribution latent variables mixture Gaussians form product marginalize recover order derive EM write complete-data log likelihood znk expectation complete-data log likelihood respect posterior distribution latent variables posterior component data point responsibilities evaluated takes form znk znk znj Alternative View EM consider sum responsibilities enter written Nk xk Nk Nk effective number data points associated component maximize expected complete-data log likelihood respect parameters set derivative respect equal zero rearrange obtainExercise sets mean component equal weighted mean weighting coefficients responsibilities component takes data maximization respect need introduce Lagrange multiplier enforce constraint Following analogous steps used mixture obtainExercise Nk represents intuitively reasonable result mixing coefficient component effective fraction points data set explained Note contrast mixture singularities likelihood function goes seen noting likelihood function bounded existExercise singularities likelihood function goes EM provided initialized pathological starting EM algorithm increases value likelihood local maximum illustrate Bernoulli mixture model Figure bySection model handwritten digit images turned binary vectors setting elements values exceed setting remaining elements fit data set comprising digits mixture Bernoulli distributions running iterations EM mixing coefficients initialized parameters set random values chosen uniformly range normalized satisfy constraint mixture Bernoulli distributions able clusters data set corresponding different conjugate prior parameters Bernoulli distribution beta seen beta prior equivalent introducing MIXTURE MODELS EM Figure Illustration Bernoulli mixture model row shows examples digits data set converting pixel values grey scale binary threshold row images parameters components mixture fit data set single multivariate Bernoulli maximum amounts simply averaging counts pixel shown right-most image additional effective observations similarly introduce priors theSection Bernoulli mixture use EM maximize posterior probability straightforward extend analysis Bernoulli mixtures case multinomial binary variables having states making use discrete dis-Exercise tribution introduce Dirichlet priors model parameters EM Bayesian linear regression example application return evidence approximation Bayesian linear Section obtained reestimation equations hyperparameters evaluation evidence setting derivatives resulting expression turn alternative approach finding based EM Recall goal maximize evidence function respect parameter vector marginalized regard latent optimize marginal likelihood function compute posterior distribution current setting parameters use expected complete-data log maximize quantity respect derived posterior distribution complete-data log likelihood function Alternative View EM likelihood prior Taking expectation respect posterior distribution gives wTw Setting derivatives respect obtain step re-estimation equationExercise mTNmN analogous result holds Note re-estimation equation takes slightly different form corresponding result derived direct evaluation evidence involve computation inversion eigen matrix comparable computational cost approaches determining course converge result local maximum evidence verified noting quantity defined stationary point evidence re-estimation equation self-consistently substitute solving obtain precisely EM re-estimation final consider closely related relevance vector machine regression discussed Section used direct maximization marginal likelihood derive re-estimation equations hyperparameters consider alternative approach view weight vector latent variable apply EM step involves finding posterior distribution step maximize expected complete-data log defined Ew expectation taken respect posterior distribution computed parameter compute new parameter values maximize respect giveExercise MIXTURE MODELS EM m2i re-estimation equations formally equivalent obtained direct EM Algorithm General expectation maximization EM general technique finding maximum likelihood solutions probabilistic models having latent variables et McLachlan general treatment EM algorithm process provide proof EM algorithm derived heuristically Sections Gaussian mixtures does maximize likelihood function Neal discussion form basis derivation variational inference Consider probabilistic model collectively denote observed variables hidden variables joint distribution governed set parameters denoted goal maximize likelihood function assuming discussion identical comprises continuous variables combination discrete continuous summation replaced integration shall suppose direct optimization optimization complete-data likelihood function significantly introduce distribution defined latent observe choice following decomposition holds defined Note functional Appendix discussion distribution function parameters worth studying EM Algorithm General Figure Illustration decomposition holds choice distribution Kullback-Leibler divergence satisfies quantity lower bound log likelihood function carefully forms expressions particular noting differ sign contains joint distribution contains conditional distribution verify decomposition make use product rule probability giveExercise substitute expression gives rise cancels gives required log likelihood noting normalized distribution sums Kullback-Leibler divergence posterior distribution Recall Kullback-Leibler divergence satisfies equality ItSection follows words lower bound decomposition illustrated Figure EM algorithm two-stage iterative optimization technique finding maximum likelihood use decomposition define EM algorithm demonstrate does maximize log Suppose current value parameter vector lower bound maximized respect holding solution maximization problem easily seen noting value does depend largest value occur Kullback-Leibler divergence words equal posterior distribution lower bound equal log illustrated Figure subsequent distribution held fixed lower bound maximized respect new value cause lower bound increase necessarily cause corresponding log likelihood function distribution determined old parameter values new values held fixed equal new posterior distribution nonzero KL increase log likelihood function greater increase lower MIXTURE MODELS EM Figure Illustration step EM distribution set equal posterior distribution current parameter values causing lower bound value log likelihood KL divergence shown Figure substitute lower bound takes form const constant simply negative entropy distribution independent quantity maximized expectation complete-data log saw earlier case mixtures Note variable optimizing appears inside joint distribution comprises member exponential product logarithm cancel exponential lead step typically simpler maximization corresponding incomplete-data log likelihood function operation EM algorithm viewed space illustrated schematically Figure red curve depicts Illustration step EM distribution held fixed lower bound maximized respect parameter vector revised value KL divergence causes log likelihood increase lower bound EM Algorithm General Figure EM algorithm involves alternately computing lower bound log likelihood current parameter values maximizing bound obtain new parameter text new complete log likelihood function value wish start initial parameter value step evaluate posterior distribution latent gives rise lower bound value equals log likelihood shown blue Note bound makes tangential contact log likelihood curves bound convex function having uniqueExercise maximum mixture components exponential bound maximized giving value gives larger value log likelihood subsequent step constructs bound tangential shown green particular case identically distributed data comprise data points comprise corresponding latent variables independence marginalizing sum product posterior probability evaluated step takes form posterior distribution factorizes respect case Gaussian mixture model simply says responsibility mixture components takes particular data point xn depends value xn parameters mixture values data seen steps EM algorithm increasing value well-defined bound log likelihood function MIXTURE MODELS EM complete EM cycle change model parameters way cause log likelihood increase case parameters remain use EM algorithm maximize posterior distribution models introduced prior note function Making use decomposition optimize right-hand alternately respect optimization respect gives rise Estep equations standard EM appears M-step equations modified introduction prior term typically requires small modification standard maximum likelihood M-step EM algorithm breaks potentially difficult problem maximizing likelihood function step prove simpler complex models case step remain leads possible extensions EM generalized algorithm addresses problem intractable Instead aiming maximize respect seeks instead change parameters way increase lower bound log likelihood complete EM cycle GEM algorithm guaranteed increase value log likelihood parameters correspond local way exploit GEM approach use nonlinear optimization conjugate gradients form GEM known expectation conditional involves making constrained optimizations step parameters partitioned step broken multiple steps involves optimizing subset remainder held similarly generalize step EM algorithm performing optimization respect value unique maximum respect corresponds posterior distribution choice bound equal log likelihood function follows algorithm converges global maximum value global maximum log likelihood Provided continuous function Exercises local maximum local maximum Consider case independent data points corresponding latent variables joint distribution factorizes data structure exploited incremental form EM EM cycle data point processed instead recomputing responsibilities data just re-evaluate responsibilities data appear subsequent step require computation involving responsibilities data mixture components members exponential responsibilities enter simple sufficient updated case Gaussian suppose perform update data point corresponding old new values responsibilities denoted required sufficient statistics updated means sufficient statistics defined obtainExercise old Nnewk xm Nnewk old corresponding results covariances mixing coefficients step step fixed time independent total number data parameters revised data waiting data set incremental version converge faster batch step incremental algorithm increasing value shown algorithm converges local maximum correspond local maximum log likelihood function Exercises www Consider K-means algorithm discussed Section consequence finite number possible assignments set discrete indicator variables assignment unique optimum K-means algorithm converge finite number Apply Robbins-Monro sequential estimation procedure described Section problem finding roots regression function derivatives respect leads stochastic K-means algorithm data point nearest prototype updated MIXTURE MODELS EM www Consider Gaussian mixture model marginal distribution latent variable conditional distribution observed variable marginal distribution obtained summing possible values Gaussian mixture form Suppose wish use EM algorithm maximize posterior distribution parameters model containing latent observed data step remains maximum likelihood step quantity maximized defined Consider directed graph Gaussian mixture model shown Figure making use d-separation criterion discussed Section posterior distribution latent variables factorizes respect different data points Consider special case Gaussian mixture model covariance matrices components constrained common value Derive EM equations maximizing likelihood function www Verify maximization complete-data log likelihood Gaussian mixture model leads result means covariances component fitted independently corresponding group data mixing coefficients fractions points www maximize respect keeping responsibilities obtain closed form solution maximize respect keeping responsibilities obtain closed form solutions Consider density model mixture distribution suppose partition vector parts conditional density mixture distribution expressions mixing coefficients component Exercises Section obtained relationship means EM Gaussian mixtures considering mixture model components covariance limit maximizing expected completedata log likelihood equivalent minimizing distortion measure K-means algorithm www Consider mixture distribution form elements discrete continuous combination Denote mean covariance mean covariance mixture distribution re-estimation equations EM mixture Bernoulli parameters set values corresponding maximum likelihood property xn parameters model initialized components mean EM algorithm converge choice initial mixing solution property Note represents degenerate case mixture model components practice try avoid solutions appropriate Consider joint distribution latent observed variables Bernoulli distribution obtained forming product marginalize joint distribution respect obtain www maximize expected complete-data log likelihood function mixture Bernoulli distributions respect obtain step equation maximize expected complete-data log likelihood function mixture Bernoulli distributions respect mixing coefficients Lagrange multiplier enforce summation obtain step equation www consequence constraint discrete variable incomplete-data log likelihood function mixture Bernoulli distributions bounded singularities likelihood goes MIXTURE MODELS EM Consider Bernoulli mixture model discussed Section prior distribution parameter vectors beta distribution Dirichlet prior Derive EM algorithm maximizing posterior probability Consider D-dimensional variable components multinomial variable degree binary vector components xij subject constraint xij Suppose distribution variables described mixture discrete multinomial distributions considered Section xij kij parameters represent probabilities satisfy constraint values observed data set derive step equations EM algorithm optimizing mixing coefficients component parameters distribution maximum www maximization expected complete-data log likelihood function Bayesian linear regression model leads step reestimation result evidence framework Section derive M-step re-estimation equations parameter Bayesian linear regression analogous result maximization expected complete-data log likelihood defined derive step equations re-estimating hyperparameters relevance vector machine www Section used direct maximization marginal likelihood derive re-estimation equations finding values hyperparameters regression Section used EM algorithm maximize marginal giving re-estimation equations sets re-estimation equations formally Verify relation defined Exercises www lower bound gradient respect log likelihood function point www Consider incremental form EM algorithm mixture responsibilities recomputed specific data point Starting M-step formulae derive results updating component Derive M-step formulae updating covariance matrices mixing coefficients Gaussian mixture model responsibilities updated analogous result updating 
Approximate Inference central task application probabilistic models evaluation posterior distribution latent variables observed data variables evaluation expectations computed respect model contain deterministic leave implicit fully Bayesian model unknown parameters prior distributions absorbed set latent variables denoted vector EM algorithm need evaluate expectation complete-data log likelihood respect posterior distribution latent models practical infeasible evaluate posterior distribution compute expectations respect dimensionality latent space high work directly posterior distribution highly complex form expectations analytically case continuous required integrations closed-form APPROXIMATE INFERENCE analytical dimensionality space complexity integrand prohibit numerical discrete marginalizations involve summing possible configurations hidden possible practice exponentially hidden states exact calculation prohibitively need resort approximation fall broadly according rely stochastic deterministic Stochastic techniques Markov chain Monte described Chapter enabled widespread use Bayesian methods generally property infinite computational generate exact approximation arises use finite processor sampling methods computationally limiting use small-scale difficult know sampling scheme generating independent samples required introduce range deterministic approximation scale large based analytical approximations posterior example assuming factorizes particular way specific parametric form generate exact strengths weaknesses complementary sampling Section discussed Laplace based local Gaussian approximation mode turn family approximation techniques called variational inference variational use global criteria widely conclude brief introduction alternative variational framework known expectation Variational Inference Variational methods origins 18th century work calculus Standard calculus concerned finding derivatives think function mapping takes value variable input returns value function derivative function describes output value varies make infinitesimal changes input define functional mapping takes function input returns value functional example entropy takes probability distribution input returns quantity dx Variational Inference introduce concept functional expresses value functional changes response infinitesimal changes input function et rules calculus variations mirror standard calculus discussed Appendix problems expressed terms optimization problem quantity optimized solution obtained exploring possible input functions Variational methods broad applicability include areas finite element methods maximum entropy intrinsically approximate variational naturally lend finding approximate restricting range functions optimization instance considering quadratic functions considering functions composed linear combination fixed basis functions coefficients linear combination case applications probabilistic restriction example form factorization assumptions et let consider concept variational optimization applied inference Suppose fully Bayesian model parameters prior model latent variables shall denote set latent variables parameters denote set observed variables set identically distributed probabilistic model specifies joint distribution goal approximation posterior distribution model evidence discussion decompose log marginal probability defined dZ differs discussion EM parameter vector longer parameters stochastic variables absorbed chapter mainly interested continuous variables used integrations summations formulating analysis goes unchanged variables discrete simply replacing integrations summations maximize lower bound optimization respect distribution equivalent minimizing KL allow possible choice maximum lower bound occurs KL divergence occurs equals posterior distribution APPROXIMATE INFERENCE Figure Illustration variational approximation example considered earlier Figure left-hand plot shows original distribution Laplace variational right-hand plot shows negative logarithms corresponding shall suppose model working true posterior distribution consider instead restricted family distributions seek member family KL divergence goal restrict family sufficiently comprise tractable time allowing family sufficiently rich flexible provide good approximation true posterior important emphasize restriction imposed purely achieve subject requirement use rich family approximating distributions associated highly flexible flexible approximations simply allows approach true posterior distribution way restrict family approximating distributions use parametric distribution governed set parameters lower bound function exploit standard nonlinear optimization techniques determine optimal values example variational distribution Gaussian optimized respect mean shown Figure Factorized distributions consider alternative way restrict family distributions Suppose partition elements disjoint groups denote Zi assume distribution factorizes respect Variational Inference emphasized making assumptions place restriction functional forms individual factors factorized form variational inference corresponds approximation framework developed physics called mean field theory distributions having form seek distribution lower bound wish make free form optimization respect distributions optimizing respect factors achieve substitute dissect dependence factors Denoting simply qj notation obtain qi qi dZ qj qi dZi dZj qj qj dZj const qj dZj qj qj dZj const defined new distribution relation Ei notation Ei denotes expectation respect distributions variables zi Ei qi suppose fixed maximize respect possible forms distribution easily recognizing negative Kullback-Leibler divergence maximizing equivalent minimizing Kullback-Leibler Leonhard Euler Euler Swiss mathematician physicist worked Petersburg Berlin widely considered greatest mathematicians certainly collected works formulated modern theory developed calculus discovered formula relates important numbers years totally produced nearly half results APPROXIMATE INFERENCE minimum occurs obtain general expression optimal solution Ei worth taking moments study form solution provides basis applications variational says log optimal solution factor qj obtained simply considering log joint distribution hidden visible variables taking expectation respect factors additive constant set normalizing distribution exponential sides exp exp dZj shall convenient work form reinstate normalization constant clear subsequent set equations represent set consistency conditions maximum lower bound subject factorization represent explicit solution expression right-hand optimum depends expectations computed respect factors seek consistent solution initializing factors appropriately cycling factors replacing turn revised estimate right-hand evaluated current estimates Convergence guaranteed bound convex respect factors Properties factorized approximations approach variational inference based factorized approximation true posterior Let consider moment problem approximating general distribution factorized begin discuss problem approximating Gaussian distribution factorized provide useful insight types inaccuracy introduced factorized Consider Gaussian distribution correlated variables mean precision elements symmetry precision suppose wish approximate distribution factorized Gaussian form apply general result expression Variational Inference optimal factor doing useful note right-hand need retain terms functional dependence terms absorbed normalization Ez2 const Ez2 const observe right-hand expression quadratic function identify Gaussian worth emphasizing did assume derived result variational optimization KL divergence possible distributions Note need consider additive constant explicitly represents normalization constant end inspection technique completing identifySection mean precision giving Gaussian written Note solutions depends expectations computed respect vice address treating variational solutions re-estimation equations cycling variables turn updating convergence criterion shall example note problem sufficiently simple closed form solution equations satisfied easily shown solution provided distribution result illustrated Figure theExercise mean correctly captured variance controlled direction smallest variance variance orthogonal direction significantly general result factorized variational approximation tends approximations posterior distribution way suppose instead minimizing reverse Kullback-Leibler divergence shall form KL divergence APPROXIMATE INFERENCE Figure Comparison alternative forms Kullback-Leibler green contours corresponding standard deviations correlated Gaussian distribution variables red contours represent corresponding levels approximating distribution variables product independent univariate Gaussian distributions parameters obtained minimization KullbackLeibler divergence reverse Kullback-Leibler divergence used alternative approximate inference framework called expectation consider general problem minimizing whenSection factorized approximation form KL divergence written form dZ const constant term simply entropy does depend optimize respect factors easily Lagrange multiplier giveExercise dZi optimal solution just corresponding marginal distribution Note closed-form solution does require apply result illustrative example Gaussian distribution vector use gives result shown Figure mean approximation places significant probability mass regions variable space low difference results understood noting large positive contribution Kullback-Leibler divergence dZ Variational Inference Figure comparison alternative forms Kullback-Leibler blue contours bimodal distribution mixture red contours correspond single Gaussian distribution best approximates sense minimizing KullbackLeibler divergence red contours correspond Gaussian distribution numerical minimization Kullback-Leibler divergence showing different local minimum Kullback-Leibler regions space near zero unless close minimizing form KL divergence leads distributions avoid regions Kullback-Leibler divergence minimized distributions nonzero regions gain insight different behaviour KL divergences consider approximating multimodal distribution unimodal illustrated Figure practical true posterior distribution posterior mass concentrated number relatively small regions parameter multiple modes arise nonidentifiability latent space complex nonlinear dependence types multimodality encountered Chapter context Gaussian manifested multiple maxima likelihood variational treatment based minimization tend minimize resulting approximations average modes context mixture lead poor predictive distributions average good parameter values typically good parameter possible make use define useful inference requires different approach discussed considered discuss expectation forms Kullback-Leibler divergence members alpha family APPROXIMATE INFERENCE divergences defined dx continuous Kullback-Leibler divergence corresponds limit corresponds limit values equality Suppose fixed minimize respect set distributions divergence zero values typically under-estimate support tend seek mode largest Conversely divergence values typically stretch cover over-estimate support obtain symmetric divergence linearly related Hellinger distance square root Hellinger distance valid distance univariate Gaussian illustrate factorized variational approximation Gaussian distribution single variable goal infer posterior distribution mean precision data set observed values assumed drawn independently likelihood function exp introduce conjugate prior distributions gamma distribution defined distributions constitute Gaussian-Gamma conjugate prior simple problem posterior distribution takes form Gaussian-gamma tutorial purposesExercise consider factorized variational approximation posterior distribution Variational Inference Note true posterior distribution does factorize optimum factors obtained general result const Completing square Gaussian mean precision byExercise Nx Note gives maximum likelihood result precision optimal solution factor const const gamma distribution bN parameters bN exhibits expected behaviour emphasized did assume specific functional forms optimal distributions arose naturally structure likelihood function corresponding conjugate expressions optimal distributions depends moments evaluated respect approach finding solution make initial guess moment use re-compute distribution revised distribution extract required moments use recompute distribution space hidden variables example illustrate variational approximation posterior distribution plotting contours true posterior factorized illustrated Figure APPROXIMATE INFERENCE Figure Illustration variational inference mean precision univariate Gaussian Contours true posterior distribution shown Contours initial factorized approximation shown re-estimating factor re-estimating factor Contours optimal factorized iterative scheme shown need use iterative approach order solve optimal factorized posterior simple example considering explicit solution solving simultaneous equations optimal factors doing simplify expressions considering noninformative priors parameter settings correspond improper posterior distribution standard result mean gamma withAppendix obtain second order moments Variational Inference form substitute moments solve giveExercise recognize right-hand familiar unbiased estimator variance univariate Gaussian use Bayesian approach avoided bias maximum likelihood Model comparison performing inference hidden variables wish compare set candidate labelled index having prior probabilities goal approximate posterior probabilities observed slightly complex situation considered far different models different structure different dimensionality hidden variables simply consider factorized approximation instead recognize posterior conditioned consider readily verify following decomposition based variational distributionExercise Lm Lm lower bound Lm assuming discrete analysis applies continuous latent variables provided summations replaced maximize Lm respect distribution Lagrange resultExercise maximize Lm respect solutions different expect conditioned proceed instead optimizing individually optimization APPROXIMATE INFERENCE subsequently determining normalization resulting values used model selection model averaging usual Variational Mixture Gaussians return discussion Gaussian mixture model apply variational inference machinery developed previous provide good illustration application variational methods demonstrate Bayesian treatment elegantly resolves difficulties associated maximum likelihood approach reader encouraged work example provides insights practical application variational Bayesian corresponding sophisticated solved straightforward extensions generalizations starting point likelihood function Gaussian mixture illustrated graphical model Figure observation xn corresponding latent variable comprising 1-of-K binary vector elements znk denote observed data set similarly denote latent variables write conditional distribution mixing coefficients form write conditional distribution observed data latent variables component parameters Note working terms precision matrices covariance matrices somewhat simplifies introduce priors parameters analysis considerably simplified use conjugate prior choose aSection Dirichlet distribution mixing coefficients symmetry chosen parameter normalization constant Dirichlet distribution defined Variational Mixture Gaussians Figure Directed acyclic graph representing Bayesian mixture Gaussians box denotes set denotes denotes xn parameter interpreted effectiveSection prior number observations associated component value posterior distribution influenced primarily data introduce independent Gaussian-Wishart prior governing mean precision Gaussian represents conjugate prior distribution mean precision Typically choose resulting model represented directed graph shown Figure Note link variance distribution function example provides nice illustration distinction latent variables Variables appear inside plate regarded latent variables number variables grows size data variables outside plate fixed number independently size data regarded perspective graphical really fundamental difference Variational distribution order formulate variational treatment write joint distribution random various factors defined reader moment verify decomposition does correspond probabilistic graphical model shown Figure Note variables APPROXIMATE INFERENCE consider variational distribution factorizes latent variables parameters remarkable assumption need make order obtain tractable practical solution Bayesian mixture functional form factors determined automatically optimization variational Note omitting subscripts distributions relying arguments distinguish different corresponding sequential update equations factors easily derived making use general result Let consider derivation update equation factor log optimized factor make use decomposition Note interested functional dependence right-hand variable terms depend absorbed additive normalization giving Substituting conditional distributions right-hand absorbing terms independent additive znk const defined dimensionality data variable Taking exponential sides obtain Requiring distribution noting value quantities znk binary sum values obtainExercise rznknk Variational Mixture Gaussians rnk optimal solution factor takes functional form prior Note exponential real quantities rnk nonnegative sum discrete distribution standard result rnk quantities rnk playing role Note optimal solution depends moments evaluated respect distributions variational update equations coupled solved shall convenient define statistics observed data set evaluated respect Nk rnk xk Nk rnkxn Sk Nk Note analogous quantities evaluated maximum likelihood EM algorithm Gaussian mixture let consider factor variational posterior general result EZ lnN observe right-hand expression decomposes sum terms involving terms involving implies variational posterior factorizes terms involving comprise sum terms involving leading factorization APPROXIMATE INFERENCE Identifying terms right-hand depend rnk const used Taking exponential recognize Dirichlet distribution components variational posterior distribution does factorize product use product rule write form factors inspecting reading terms involve Gaussian-Wishart distribution byExercise defined Nk mk NkSk Nk update equations analogous M-step equations EM algorithm maximum likelihood solution mixture computations performed order update variational posterior distribution model parameters involve evaluation sums data arose maximum likelihood order perform variational need expectations rnk representing obtained normalizing expression involves expectations respect variational distributions easily evaluated giveExercise Variational Mixture Gaussians introduced definitions digamma function defined results follow standard properties Wishart Dirichlet substitute make use obtain following result responsibilities rnk exp Notice similarity corresponding result responsibilities maximum likelihood written form rnk exp used precision place covariance highlight similarity optimization variational posterior distribution involves cycling stages analogous steps maximum likelihood EM variational equivalent use current distributions model parameters evaluate moments evaluate subsequent variational equivalent responsibilities fixed use re-compute variational distribution parameters variational posterior distribution functional form corresponding factor joint distribution general result consequence choice conjugate Figure shows results applying approach rescaled Old Faithful data set Gaussian mixture model having components expected values mixing coefficients numerically distinguishable prior effect understood qualitatively terms automatic trade-off Bayesian model fitting data complexity whichSection complexity penalty arises components parameters pushed away prior Components essentially responsibility explaining data points rnk Nk parameters revert prior principle components fitted slightly data broad priors effect small seen variational Gaussian mixture model expected values mixing coefficients posterior distribution byExercise Nk Consider component Nk prior broad component plays role APPROXIMATE INFERENCE Figure Variational Bayesian mixture Gaussians applied Old Faithful data ellipses denote standard-deviation density contours density red ink inside ellipse corresponds mean value mixing coefficient number left diagram shows number iterations variational Components expected mixing coefficient numerically indistinguishable zero prior tightly constrains mixing coefficients Figure prior mixing coefficients Dirichlet form Recall Figure prior favours solutions mixing coefficients Figure obtained resulted components having nonzero mixing instead choose obtain components nonzero mixing components nonzero mixing seen close similarity variational solution Bayesian mixture Gaussians EM algorithm maximum fact consider limit Bayesian treatment converges maximum likelihood EM small data dominant computational cost variational algorithm Gaussian mixtures arises evaluation evaluation inversion weighted data covariance computations mirror precisely arise maximum likelihood EM little computational overhead Bayesian approach compared traditional maximum likelihood substantial singularities arise maximum likelihood Gaussian component specific data point absent Bayesian Variational Mixture Gaussians singularities removed simply introduce prior use MAP estimate instead maximum over-fitting choose large number components saw Figure variational treatment opens possibility determining optimal number components mixture resorting techniques cross Variational lower bound straightforwardly evaluate lower bound useful able monitor bound re-estimation order test provide valuable check mathematical expressions solutions software step iterative re-estimation procedure value bound stage provide deeper test correctness mathematical derivation update equations software implementation finite differences check update does maximum bound variational mixture lower bound notation omitted superscript subscripts expectation operators expectation taken respect random variables various terms bound easily evaluated following resultsExercise Nk rnk APPROXIMATE INFERENCE rnk rnk dimensionality entropy Wishart distribution coefficients defined Note terms involving expectations logs distributions simply represent negative entropies simplifications combination terms performed expressions summed lower kept expressions separate ease worth noting lower bound provides alternative approach deriving variational re-estimation equations obtained Section use fact model conjugate functional form factors variational posterior distribution discrete Dirichlet Gaussian-Wishart taking general parametric forms distributions derive form lower bound function parameters Maximizing bound respect parameters gives required re-estimation Predictive density applications Bayesian mixture Gaussians model interested predictive density new value observed Associated observation corresponding latent variable predictive density bz Variational Mixture Gaussians true posterior distribution perform summation remaining integrations approximate predictive density replacing true posterior distribution variational approximation use factorization term implicitly integrated variables remaining integrations evaluated analytically giving mixture t-distributionsExercise kth component mean precision Lk Wk size data set large predictive distribution reduces mixture Determining number components seen variational lower bound used determine posterior distribution number components mixture ThereSection subtlety needs setting parameters Gaussian mixture model specific degenerate exist parameter settings density observed variables parameter values differ re-labelling consider mixture Gaussians single observed variable parameters values parameter values components symmetry rise value mixture model comprising parameter setting member family equivalent context maximum redundancy irrelevant parameter optimization algorithm example depending initialization specific equivalent solutions play Bayesian marginalize possible APPROXIMATE INFERENCE Figure Plot variational lower bound versus number components Gaussian mixture Old Faithful showing distinct peak value model trained different random results shown symbols plotted small random horizontal perturbations Note solutions suboptimal local happens parameter seen Figure true posterior distribution variational inference based minimization tend approximate distribution neighbourhood modes ignore equivalent modes equivalent predictive concern provided considering model having specific number wish compare different values need account simple approximate solution add term lower bound used model comparison Figure shows plot lower including multimodality versus number components Old Faithful data worth emphasizing maximum likelihood lead values likelihood function increase monotonically singular solutions discounting effects local used determine appropriate model Bayesian inference automatically makes trade-off model complexity fitting approach determination requires range models having different values trained alternative approach determining suitable value treat mixing coefficients parameters make point estimates values maximizing lower bound respect instead maintaining probability distribution fully Bayesian leads re-estimation equationExercise rnk maximization interleaved variational updates distribution remaining Components provide insufficient contribution Variational Mixture Gaussians explaining data mixing coefficients driven zero effectively removed model automatic relevance allows make single training run start relatively large initial value allow surplus components pruned origins sparsity optimizing respect hyperparameters discussed context relevance vector Induced factorizations deriving variational update equations Gaussian mixture assumed particular factorization variational posterior distribution optimal solutions various factors exhibit additional solution product independent distribution components variational posterior distribution latent factorizes independent distribution observation does factorize respect value znk constrained sum additional factorizations consequence interaction assumed factorization conditional independence properties true characterized directed graph Figure shall refer additional factorizations induced factorizations arise interaction factorization assumed variational posterior distribution conditional independence properties true joint numerical implementation variational approach important account additional inefficient maintain precision matrix Gaussian distribution set variables optimal form distribution diagonal precision matrix factorization respect individual variables described induced factorizations easily detected simple graphical test based d-separation partition latent variables disjoint groups let suppose assuming factorization remaining latent general result product rule optimal solution const ask resulting solution factorize words happen conditional independence relation APPROXIMATE INFERENCE test relation does choice making use d-separation illustrate consider Bayesian mixture Gaussians represented directed graph Figure assuming variational factorization immediately variational posterior distribution parameters factorize remaining parameters paths connecting pass nodes conditioning set conditional independence test head-to-tail respect Variational Linear Regression second illustration variational return Bayesian linear regression model Section evidence approximated integration making point estimates obtained maximizing log marginal fully Bayesian approach integrate hyperparameters exact integration use variational methods tractable order simplify shall suppose noise precision parameter fixed true framework easily extended include distribution linear regression variational treatmentExercise turn equivalent evidence provides good exercise use variational methods lay foundation variational treatment Bayesian logistic regression Section Recall likelihood function prior introduce prior distribution discussion Section know conjugate prior precision Gaussian gamma choose defined joint distribution variables represented directed graphical model shown Figure Variational distribution goal approximation posterior distribution employ variational framework Section variational Variational Linear Regression Figure Probabilistic graphical model representing joint distribution Bayesian linear regression posterior distribution factorized expression re-estimation equations factors distribution making use general result Recall log joint distribution variables average respect variables Consider distribution Keeping terms functional dependence Ew const recognize log gamma identifying coefficients obtain bN bN variational re-estimation equation posterior distribution general result keeping terms functional dependence const const wT quadratic distribution complete square usual way identify mean giving APPROXIMATE INFERENCE mN SN Note close similarity posterior distribution obtained treated fixed difference replaced expectation variational chosen use notation covariance matrix SN standard results obtain required moments follows mNmTN SN evaluation variational posterior distribution begins initializing parameters distributions alternately re-estimates factors turn suitable convergence criterion satisfied specified terms lower bound discussed instructive relate variational solution evidence framework Section consider case corresponding limit infinitely broad prior mean variational posterior distribution bN mTNmN Comparison shows case particularly simple variational approach gives precisely expression obtained maximizing evidence function EM point estimate replaced expected distribution depends expectation approaches identical results case infinitely broad Predictive distribution predictive distribution new input easily evaluated model Gaussian variational posterior parameters dw dw dw Variational Linear Regression evaluated integral making use result linear-Gaussian input-dependent variance Note takes form result obtained fixed expected value appears definition SN Lower bound quantity importance lower bound defined Evaluation various terms making use results obtained inExercise previous gives tTt Tt Tr SN bN 2bN mTNmN bN bN bN Figure shows plot lower bound versus degree polynomial model synthetic data set generated degree prior parameters set corresponding noninformative prior uniform discussed Section saw Section quantity represents lower bound log marginal likelihood assign equal prior probabilities different values interpret approximation posterior model probability variational framework assigns highest probability model contrasted maximum likelihood assigns smaller residual error models increasing complexity residual error driven causing maximum likelihood favour severely over-fitted APPROXIMATE INFERENCE Figure Plot lower bound versus order polynomial set data points generated polynomial sampled interval additive Gaussian noise variance value bound gives log probability value bound peaks corresponding true model data set Exponential Family Distributions Chapter discussed important role played exponential family distributions conjugate models discussed complete-data likelihood drawn exponential general case marginal likelihood function observed mixture joint distribution observations xn corresponding hidden variables member exponential marginal distribution xn mixture Gaussians grouped variables model observed variables hidden make distinction latent denoted denoted parameters intensive number independent size data latent variables extensive number size data Gaussian mixture indicator variables zkn specify component responsible generating data point represent latent means precisions mixing proportions represent Consider case independent identically distributed denote data values corresponding latent variables suppose joint distribution observed latent variables member exponential parameterized natural parameters exp shall use conjugate prior written exp Recall conjugate prior distribution interpreted prior number observations having value consider variational Exponential Family Distributions distribution factorizes latent variables general result solve factors follows const decomposes sum independent value solution factorize example induced Taking exponentialSection exp normalization coefficient re-instated comparison standard form exponential variational distribution const taking exponential re-instating normalization coefficient exp defined Ezn Note solutions solve iteratively two-stage variational evaluate expected sufficient statistics current posterior distribution latent variables use compute revised posterior distribution subsequent variational use revised parameter posterior distribution expected natural parameters gives rise revised variational distribution latent Variational message passing illustrated application variational methods considering specific Bayesian mixture model APPROXIMATE INFERENCE described directed graph shown Figure consider generally use variational methods models described directed graphs derive number widely applicable joint distribution corresponding directed graph written decomposition xi denotes associated node pai denotes parent set corresponding node Note xi latent variable belong set observed consider variational approximation distribution assumed factorize respect xi Note observed factor variational substitute general result Ei terms right-hand depend xj absorbed additive terms depend xj conditional distribution xj conditional distributions xj conditioning conditional distributions correspond children node depend co-parents child parents child nodes node xj set nodes depends corresponds Markov blanket node xj illustrated Figure update factors variational posterior distribution represents local calculation makes possible construction general purpose software variational inference form model does need specified advance et specialize case model conditional distributions conjugate-exponential variational update procedure cast terms local message passing algorithm distribution associated particular node updated node received messages parents turn requires children received messages evaluation lower bound simplified required quantities evaluated message passing distributed message passing formulation good scaling properties suited large Local Variational Methods Local Variational Methods variational framework discussed Sections considered method sense directly seeks approximation posterior distribution random alternative approach involves finding bounds functions individual variables groups variables seek bound conditional distribution just factor larger probabilistic model specified directed purpose introducing bound course simplify resulting local approximation applied multiple variables turn tractable approximation Section shall practical example approach context logistic focus developing bounds seen discussion Kullback-Leibler divergence convexity logarithm function played key role developing lower bound global variational defined convex function chord lies Convexity plays aSection central role local variational Note discussion apply equally concave functions interchanged lower bounds replaced upper Let begin considering simple function convex function shown left-hand plot Figure goal approximate simpler particular linear function Figure linear function lower bound corresponds obtain tangent line specific value say making order Taylor expansion equality example function Figure left-hand figure red curve shows function blue line shows tangent defined line slope Note tangent example ones shown smaller value right-hand figure shows corresponding plot function versus maximum corresponds APPROXIMATE INFERENCE Figure left-hand plot red curve shows convex function blue line represents linear function lower bound value slope contact point tangent line having slope minimizing respect discrepancy green dashed defines dual function corresponds intercept tangent line having slope obtain tangent line form linear function parameterized consistency subsequent let define Different values correspond different tangent lines lower bounds write function form max succeeded approximating convex function linear function price paid introduced variational parameter obtain tightest bound optimize respect formulate approach generally framework convex duality Jordan et Consider illustration convex function shown left-hand plot Figure function lower bound best lower bound achieved linear function having slope tightest bound tangent Let write equation tangent having slope intercept clearly depends slope determine note line moved vertically equal smallest vertical distance line shown Figure max Local Variational Methods instead fixing varying consider particular adjust tangent plane tangent particular value tangent line particular maximized value coincides contact max functions play dual related Let apply duality relations simple example maximizing value back-substituting obtain conjugate function form obtained function right-hand plot Figure substitute gives maximizing value back-substituting recovers original function concave follow similar argument obtain upper replaced min min function convex directly apply method obtain seek invertible transformations function argument change convex calculate conjugate function transform original important arises frequently pattern logistic sigmoid function defined stands function convex logarithm obtain function easily verified finding second corresponding conjugate function takesExercise form min recognize binary entropy function variable probability having value obtain upper bound logAppendix sigmoid APPROXIMATE INFERENCE Figure left-hand plot shows logistic sigmoid function defined examples exponential upper bound shown right-hand plot shows logistic sigmoid red Gaussian lower bound shown parameter bound exact denoted dashed green taking obtain upper bound logistic sigmoid form plotted values left-hand plot Figure obtain lower bound sigmoid having functional form follow Jaakkola Jordan make transformations input variable function log logistic function decompose note function convex function variable verified finding second leadsExercise lower bound linear function conjugate function max stationarity condition leads dx dx2 dx 4x tanh denote value corresponding contact point tangent line particular value tanh Local Variational Methods Instead thinking variational let play role leads simpler expressions conjugate bound written bound sigmoid exp defined bound illustrated right-hand plot Figure bound form exponential quadratic function prove useful seek Gaussian representations posterior distributions defined logistic sigmoid logistic sigmoid arises frequently probabilistic models binary variables function transforms log odds ratio posterior corresponding transformation multiclass distribution softmax lower bound derived logisticSection sigmoid does directly extend Gibbs proposes method constructing Gaussian distribution conjectured bound rigorous proof used apply local variational methods multiclass shall example use local variational bounds Sections instructive consider general terms bounds Suppose wish evaluate integral form da logistic Gaussian probability integrals arise Bayesian models wish evaluate predictive case represents posterior parameter integral employ variational bound write form variational integral product exponential-quadratic functions integrated analytically bound da freedom choose variational parameter finding value maximizes function resulting value represents tightest bound family bounds used approximation optimized general APPROXIMATE INFERENCE bound logistic sigmoid optimized required choice depends value bound exact value quantity obtained integrating values value represents weighted distribution Variational Logistic Regression illustrate use local variational methods returning Bayesian logistic regression model studied Section focussed use Laplace consider variational treatment based approach Jaakkola Jordan Like Laplace leads Gaussian approximation posterior greater flexibility variational approximation leads improved accuracy compared Laplace Furthermore Laplace variational approach optimizing defined objective function rigourous bound model Logistic regression treated Dybowski Roberts Bayesian perspective Monte Carlo sampling Variational posterior distribution shall make use variational approximation based local bounds introduced Section allows likelihood function logistic governed logistic approximated exponential quadratic convenient choose conjugate Gaussian prior form shall treat hyperparameters fixed Section shall demonstrate variational formalism extended case unknown hyperparameters values inferred variational seek maximize lower bound marginal Bayesian logistic regression marginal likelihood takes form dw note conditional distribution written eat order obtain lower bound make use variational lower bound logistic sigmoid function Variational Logistic Regression reproduce convenience exp write Note bound applied terms likelihood function variational parameter corresponding training set observation multiplying prior obtain following bound joint distribution denotes set variational exp Evaluation exact posterior distribution require normalization lefthand work instead right-hand Note function right-hand interpreted probability density normalized variational posterior distribution longer represents logarithm function monotonically inequality implies lnA gives lower bound log joint distribution form Substituting prior right-hand inequality function APPROXIMATE INFERENCE quadratic function obtain corresponding variational approximation posterior distribution identifying linear quadratic terms giving Gaussian variational posterior form mN SN Laplace obtained Gaussian approximation posterior additional flexibility provided variational parameters leads improved accuracy approximation considered batch learning context training data available Bayesian methods intrinsically suited sequential learning data points processed time formulation variational approach sequential case Note bound applies two-class problem approach does directly generalize classification problems alternative bound multiclass case explored Gibbs Optimizing variational parameters normalized Gaussian approximation posterior shall use shortly evaluate predictive distribution new data need determine variational parameters maximizing lower bound marginal substitute inequality marginal likelihood dw dw optimization hyperparameter linear regression model Section approaches determining recognize function defined integration view latent variable invoke EM second integrate analytically perform direct maximization Let begin considering EM EM algorithm starts choosing initial values parameters denote collectively step EM Variational Logistic Regression use parameter values posterior distribution maximize expected complete-data log likelihood expectation taken respect posterior distribution evaluated Noting does depend substituting obtain const denotes terms independent set derivative respect equal lines making use definitions gives note monotonic function restrict attention nonnegative values loss generality symmetry bound obtain following re-estimation equationsExercise SN mNmTN used Let summarize EM algorithm finding variational posterior initialize variational parameters evaluate posterior distribution mean covariance defined use variational posterior compute new value steps repeated suitable convergence criterion practice typically requires alternative approach obtaining re-estimation equations note integral definition lower bound integrand Gaussian-like form integral evaluated Having evaluated differentiate respect turns gives rise exactly re-estimation equations does EM approach emphasized application variational methods useful able evaluate lower integration performed analytically noting Gaussian exponential quadratic function completing square making use standard result normalization coefficient Gaussian obtain closed form solution takes formExercise APPROXIMATE INFERENCE Figure Illustration Bayesian approach logistic regression simple linearly separable data plot left shows predictive distribution obtained variational decision boundary lies roughly mid way clusters data contours predictive distribution splay away data reflecting greater uncertainty classification plot right shows decision boundaries corresponding samples parameter vector drawn posterior distribution mTNS mN mT0 variational framework applied situations data arriving sequentially case maintain Gaussian posterior distribution initialized prior data point posterior updated making use bound normalized updated posterior predictive distribution obtained marginalizing posterior takes form Laplace approximation discussed Section Figure shows variational predictive distributions synthetic data example provides interesting insights concept discussed Section qualitatively similar behaviour Bayesian Inference hyperparameters treated hyperparameter prior distribution known extend Bayesian logistic regression model allow value parameter inferred data achieved combining global local variational approximations single maintain lower bound marginal likelihood combined approach adopted Bishop context Bayesian treatment hierarchical mixture experts Variational Logistic Regression consider simple isotropic Gaussian prior distribution form analysis readily extended general Gaussian instance wish associate different hyperparameter different subsets parameters wj consider conjugate hyperprior gamma distribution governed constants marginal likelihood model takes form dw joint distribution faced analytically intractable integration shall tackle local global variational approaches model begin introduce variational distribution apply decomposition instance takes form lower bound Kullback-Leibler divergence defined dw dw lower bound intractable form likelihood factor apply local variational bound logistic sigmoid factors allows use inequality place lower bound lower bound log marginal likelihood dw assume variational distribution factorizes parameters hyperparameters APPROXIMATE INFERENCE factorization appeal general result expressions optimal Consider distribution Discarding terms independent const substitute giving wTw quadratic function solution Completing square usual obtain defined optimal solution factor obtained Ew Substituting obtain wTw recognize log gamma obtain bN bN Ew wTw Expectation Propagation need optimize variational parameters maximizing lower bound Omitting terms independent integrating dw Note precisely form appeal earlier result obtained direct optimization marginal likelihood leading re-estimation equations form obtained re-estimation equations quantities making suitable cycle updating required moments byAppendix bN wTw Expectation Propagation conclude chapter discussing alternative form deterministic approximate known expectation propagation EP variational Bayes methods discussed based minimization Kullback-Leibler divergence reverse gives approximation different Consider moment problem minimizing respect fixed distribution member exponential family written form exp function Kullback-Leibler divergence const constant terms independent natural parameters minimize family distributions setting gradient respect giving seen negative gradient expectation distribution Equating obtain APPROXIMATE INFERENCE optimum solution simply corresponds matching expected sufficient Gaussian minimize Kullback-Leibler divergence setting mean equal mean distribution covariance equal covariance called moment example seen Figure let exploit result obtain practical algorithm approximate probabilistic joint distribution data hidden variables comprises product factors form model identically distributed data factor data point factor corresponding apply model defined directed probabilistic graph factor conditional distribution corresponding undirected graph factor clique interested evaluating posterior distribution purpose making model evidence purpose model posterior model evidence considering continuous following discussion applies equally discrete variables integrals replaced shall suppose marginalization marginalizations respect posterior distribution required make intractable form approximation Expectation propagation based approximation posterior distribution product factors factor approximation corresponds factors true posterior factor normalizing constant needed ensure left-hand integrates order obtain practical need constrain factors particular shall assume come exponential product factors exponential family Expectation Propagation described finite set sufficient overall approximation Ideally like determine minimizing Kullback-Leibler divergence true posterior approximation KL KL 1Z Note reverse form KL divergence compared used variational minimization intractable KL divergence involves averaging respect true rough instead minimize KL divergences corresponding pairs represents simpler problem advantage algorithm factor individually product factors poor Expectation propagation makes better approximation optimizing factor turn context remaining starts initializing factors cycles factors refining similar spirit update factors variational Bayes framework considered Suppose wish refine factor remove factor product determine revised form factor ensuring product close possible fixed factors ensures approximation accurate regions high posterior probability defined remaining shall example effect apply EP achieve remove factor theSection current approximation posterior defining unnormalized distribution Note instead product factors practice division usually combined factor distribution Zj APPROXIMATE INFERENCE Figure Illustration expectation propagation approximation Gaussian distribution example considered earlier Figures left-hand plot shows original distribution Laplace global variational EP right-hand plot shows corresponding negative logarithms Note EP distribution broader variational consequence different form KL Zj normalization constant Zj determine revised factor minimizing Kullback-Leibler divergence KL Zj easily solved approximating distribution exponential appeal result tells parameters obtained matching expected sufficient statistics corresponding moments shall assume tractable choose Gaussian distribution set equal mean distribution set straightforward obtain required expectations member exponential provided expected statistics related derivatives normalization EP approximation illustrated Figure revised factor taking dividing remaining factors used coefficient determined multiplying Expectation Propagation sides integrating used fact value matching zeroth-order Combining Zj evaluating integral passes set revising factor posterior distribution approximated model evidence approximated factors replaced approximations Expectation Propagation joint distribution observed data stochastic variables form product factors wish approximate posterior distribution distribution form wish approximate model evidence Initialize approximating factors Initialize posterior approximation setting Choose factor Remove posterior division APPROXIMATE INFERENCE Evaluate new posterior setting sufficient statistics equal including evaluation normalization constant Zj Evaluate store new factor Zj Evaluate approximation model evidence special case known assumed density filtering moment matching Boyen Opper obtained initializing approximating factors unity making pass factors updating Assumed density filtering appropriate on-line learning data points arriving sequence need learn data point discard considering batch setting opportunity re-use data points times order achieve improved idea exploited expectation apply ADF batch results undesirable dependence order data points EP disadvantage expectation propagation guarantee iterations approximations exponential iterations resulting solution stationary point particular energy function iteration EP does necessarily decrease value energy contrast variational iteratively maximizes lower bound log marginal iteration guaranteed decrease possible optimize EP cost function case guaranteed resulting algorithms slower complex difference variational Bayes EP arises form KL divergence minimized minimizes minimizes saw Figure distributions minimizing lead poor EP applied mixtures results sensible approximation tries capture modes posterior logistic-type EP out-performs local variational methods Laplace approximation Expectation Propagation Figure Illustration clutter problem data space dimensionality Training data denoted drawn mixture Gaussians components shown red goal infer mean green Gaussian observed clutter problem Following Minka illustrate EP algorithm simple example goal infer mean multivariate Gaussian distribution variable set observations drawn make problem observations embedded background Gaussian illustrated Figure distribution observed values mixture form wN proportion background clutter assumed prior taken Gaussian Minka chooses parameter values joint distribution observations posterior distribution comprises mixture 2N computational cost solving problem exactly grow exponentially size data exact solution intractable moderately large apply EP clutter identify factors select approximating distribution exponential example convenient choose spherical Gaussian APPROXIMATE INFERENCE factor approximations form exponential-quadratic functions form snN set equal prior Note use does imply right-hand well-defined Gaussian density shall variance parameter vn simply convenient shorthand approximations initialized corresponding sn vn mn dimensionality initial defined equal iteratively refine factors taking factor time applying Note need revise term EP update leave term state theExercise results leave reader remove current estimate division mean inverse variance byExercise evaluate normalization constant wN compute mean variance finding mean variance giveExercise quantity simple interpretation probability point xn use compute refined factor parameters mn sn refinement process repeated suitable termination criterion instance maximum change parameter values resulting complete Expectation Propagation Figure Examples approximation specific factors one-dimensional version clutter showing Notice current form controls range good approximation pass factors use evaluate approximation model mTnmn vn Examples factor approximations clutter problem one-dimensional parameter space shown Figure Note factor approximations infinite negative values parameter simply corresponds approximations curve upwards instead downwards necessarily problematic provided overall approximate posterior positive Figure compares performance EP variational Bayes field Laplace approximation clutter Expectation propagation graphs far general discussion allowed factors distribution functions components similarly approximating factors approximating distribution consider situations factors depend subsets restrictions conveniently expressed framework probabilistic graphical discussed Chapter use factor graph representation encompasses directed undirected APPROXIMATE INFERENCE ep vblaplace Posterior mean FLOPS Er ro ep vb laplace Evidence FLOPS Er ro Figure Comparison expectation variational Laplace approximation clutter left-hand plot shows error predicted posterior mean versus number floating point right-hand plot shows corresponding results model shall focus case approximating distribution fully shall case expectation propagation reduces loopy belief propagation start context simple shall explore general recall minimize Kullback-Leibler divergence respect factorized distribution optimal solution factor simply corresponding marginal consider factor graph shown left Figure introduced earlier context sum-product joint distributionSection seek approximation Note normalization constants re-instated end local generally belief suppose restrict attention approximations factors factorize respect individual variables corresponds factor graph shown right Figure individual factors overall distribution fully apply EP algorithm fully factorized Suppose initialized factors choose refine factor Expectation Propagation fa fb fc Figure left simple factor graph Figure reproduced right corresponding factorized remove factor approximating distribution multiply exact factor minimizing Kullback-Leibler divergence noted comprises product variable factor corresponding marginal marginals obtained multiplying marginals factors change update involve variables fb obtain refined factor simply divide gives APPROXIMATE INFERENCE precisely messages obtained belief propagation mes-Section sages variable nodes factor nodes folded messages factor nodes variable corresponds message sent factor node fb variable node substitute obtain corresponds corresponds giving message corresponds result differs slightly standard belief propagation messages passed directions easily modify EP procedure standard form sum-product algorithm updating just factors instance refine unchanged refined version refining term choose order refinements tree-structured graph follow two-pass update corresponding standard belief propagation result exact inference variable factor initialization approximation factors case let consider general factor graph corresponding distribution represents subset variables associated factor approximate fully factorized distribution form corresponds individual variable Suppose wish refine particular term keeping terms remove term multiply exact factor determine refined term need consider functional dependence simply corresponding marginal multiplicative involves taking marginal multiplied terms functions variables Terms correspond factors cancel numerator denominator subsequently divide obtain Exercises recognize sum-product rule form messages variable nodes factor nodes illustrated example shown Figure quantity corresponds message factor node sends variable node product factors depend variables variables variable common factor compute outgoing message factor product incoming messages factor multiply local sum-product algorithm arises special case expectation propagation use approximating distribution fully suggests flexible approximating corresponding partially disconnected used achieve higher generalization group factors sets refine factors set approaches lead improvements accuracy problem choosing best combination grouping disconnection open research seen variational message passing expectation propagation optimize different forms Kullback-Leibler Minka shown broad range message passing algorithms derived common framework involving minimization members alpha family include variational message loopy belief expectation range space discuss tree-reweighted message passing et fractional belief propagation power EP Exercises www Verify log marginal distribution observed data decomposed terms form Use properties solve simultaneous equations provided original distribution unique solution means factors approximation distribution www Consider factorized variational distribution form technique Lagrange verify minimization Kullback-Leibler divergence respect factors keeping factors leads solution Suppose fixed distribution wish approximate Gaussian distribution writing form KL divergence Gaussian APPROXIMATE INFERENCE minimization respect leads result expectation www Consider model set hidden stochastic denoted collectively comprises latent variables model parameters Suppose use variational distribution factorizes latent variables parameters distribution approximated point estimate form vector free variational optimization factorized distribution equivalent EM step optimizes step maximizes expected complete-data log posterior distribution respect alpha family divergences defined KullbackLeibler divergence corresponds writing taking Similarly corresponds Consider problem inferring mean precision univariate Gaussian factorized variational considered Section factor Gaussian form mean precision Similarly factor gamma distribution form bN parameters Consider variational posterior distribution precision univariate Gaussian parameters standard results mean variance gamma distribution let variational posterior distribution mean inverse maximum likelihood estimator variance variance goes making use standard result mean gamma derive result reciprocal expected precision factorized variational treatment univariate www Derive decomposition used approximate posterior distributions models variational www Lagrange multiplier enforce normalization constraint distribution maximum lower bound Starting joint distribution applying general result optimal variational distribution latent variables Bayesian mixture Gaussians verifying steps Exercises www Starting derive result optimum variational posterior distribution Bayesian mixture verify expressions parameters distribution distribution verify result result expected value mixing coefficients variational mixture Gaussians www Verify results terms lower bound variational Gaussian mixture model Verify results remaining terms lower bound variational Gaussian mixture model shall derive variational re-estimation equations Gaussian mixture model direct differentiation lower assume variational distribution factorization defined factors Substitute obtain lower bound function parameters variational maximizing bound respect derive re-estimation equations factors variational obtained Section Derive result predictive distribution variational treatment Bayesian mixture Gaussians www exercise explores variational Bayes solution mixture Gaussians model size data set large shows reduces maximum likelihood solution based EM derived Chapter Note results Appendix used help answer posterior distribution precisions sharply peaked maximum likelihood posterior distribution means consider posterior distribution mixing coefficients sharply peaked maximum likelihood responsibilities equal corresponding maximum likelihood values large making use following asymptotic result digamma function large lnx making use large predictive distribution mixture number equivalent parameter settings interchange symmetries mixture model components APPROXIMATE INFERENCE seen mode posterior distribution Gaussian mixture model member family equivalent Suppose result running variational inference algorithm approximate posterior distribution localized neighbourhood approximate posterior distribution mixture centred mode having equal mixing assume negligible overlap components resulting lower bound differs single component distribution addition extra term www Consider variational Gaussian mixture model prior distribution mixing coefficients mixing coefficients treated values maximizing variational lower bound log marginal maximizing lower bound respect mixing Lagrange multiplier enforce constraint mixing coefficients sum leads re-estimation result Note need consider terms lower bound dependence bound www seen Section singularities arising maximum likelihood treatment Gaussian mixture models arise Bayesian Discuss singularities arise Bayesian model solved maximum posterior variational treatment Bayesian mixture discussed Section use factorized approximation posterior saw Figure factorized assumption causes variance posterior distribution under-estimated certain directions parameter Discuss qualitatively effect variational approximation model effect vary number components explain variational Gaussian mixture tend under-estimate over-estimate optimal number Extend variational treatment Bayesian linear regression include gamma hyperprior solve assuming factorized variational distribution form Derive variational update equations factors variational distribution obtain expression lower bound predictive making use formulae Appendix variational lower bound linear basis function regression defined written form various terms defined Rewrite model Bayesian mixture introduced Section conjugate model exponential discussed Section use general results derive specific results Exercises www function concave computing second Determine form dual function defined verify minimization respect according recovers function evaluating second log logistic function Derive variational upper bound directly making second order Taylor expansion log logistic function point finding second derivative respect function concave function consider second derivatives respect variable convex function Plot graphs Derive lower bound logistic sigmoid function directly making order Taylor series expansion function variable centred value www Consider variational treatment logistic regression sequential learning data points arriving time processed discarded data point Gaussian approximation posterior distribution maintained use lower bound distribution initialized data point absorbed corresponding variational parameter differentiating quantity defined respect variational parameter update equation Bayesian logistic regression model exercise derive re-estimation equations variational parameters Bayesian logistic regression model Section direct maximization lower bound set derivative respect equal making use result derivative log expressions define mean covariance variational posterior distribution Derive result lower bound variational logistic regression easily substituting expressions Gaussian prior lower bound likelihood integral defines gather terms depend exponential complete square Gaussian evaluated invoking standard result normalization coefficient multivariate Finally logarithm obtain Consider ADF approximation scheme discussed Section inclusion factor leads update model evidence form APPROXIMATE INFERENCE Zj normalization constant defined applying result initializing derive result Zj www Consider expectation propagation algorithm Section suppose factors definition exponential family functional form approximating distribution factor initialized EP update refine leaves situation typically arises factors prior prior factor incorporated exactly does need exercise shall verify results expectation propagation algorithm applied clutter Begin division formula derive expressions completing square inside exponential identify mean normalization constant defined clutter problem making use general result mean variance EP applied clutter problem prove following results expectations lnZn lnZn make use result prove results completing square use derive result 
Sampling Methods probabilistic models practical exact inference resort form Chapter discussed inference algorithms based deterministic include methods variational Bayes expectation consider approximate inference methods based numerical known Monte Carlo applications posterior distribution unobserved variables direct situations posterior distribution required primarily purpose evaluating example order make fundamental problem wish address chapter involves finding expectation function respect probability distribution components comprise discrete continuous variables combination case continuous SAMPLING METHODS Figure Schematic illustration function expectation evaluated respect distribution wish evaluate expectation dz integral replaced summation case discrete illustrated schematically single continuous variable Figure shall suppose expectations complex evaluated exactly analytical general idea sampling methods obtain set samples drawn independently distribution allows expectation approximated finite sum long samples drawn distribution estimator correct variance estimator byExercise variance function distribution worth emphasizing accuracy estimator does depend dimensionality high accuracy achievable relatively small number samples independent samples suffice estimate expectation sufficient samples effective sample size smaller apparent sample referring Figure note small regions vice expectation dominated regions small implying relatively large sample sizes required achieve sufficient joint distribution conveniently specified terms graphical case directed graph observed SAMPLING METHODS straightforward sample joint distribution possible sample conditional distributions following ancestral sampling discussed briefly Section joint distribution specified zi set variables associated node pai denotes set variables associated parents node obtain sample joint make pass set variables order zM sampling conditional distributions possible step parent values pass obtained sample joint consider case directed graph nodes instantiated observed principle extend case nodes representing discrete following logic sampling approach seen special case importance sampling discussed Section sampled value obtained variable zi value sampled value compared observed agree sample value retained algorithm proceeds variable sampled value observed value sample far discarded algorithm starts node algorithm samples correctly posterior distribution corresponds simply drawing samples joint distribution hidden variables data variables discarding samples disagree observed data slight saving continuing sampling joint distribution soon contradictory value overall probability accepting sample posterior decreases rapidly number observed variables increases number states variables approach rarely used case probability distributions defined undirected one-pass sampling strategy sample prior distribution observed computationally expensive techniques Gibbs discussed Section sampling conditional require samples marginal strategy sampling joint distribution straightforward obtain samples marginal distribution simply ignoring values numerous texts dealing Monte Carlo particular statistical inference perspective include Chen et Gamerman Gilks et Liu Neal Robert Casella review articles Besag et Brooks Diaconis Saloff-Coste Jerrum Sinclair Neal Tierney Andrieu et provide additional information sampling SAMPLING METHODS methods statistical Diagnostic tests convergence Markov chain Monte Carlo algorithms summarized Robert Casella practical guidance use sampling methods context machine learning Bishop Nabney Basic Sampling Algorithms consider simple strategies generating random samples samples generated algorithm fact pseudo-random deterministically pass appropriate tests Generating numbers raises subtleties et lie outside scope shall assume algorithm provided generates pseudo-random numbers distributed uniformly software environments facility built Standard distributions consider generate random numbers simple nonuniform assuming available source uniformly distributed random Suppose uniformly distributed interval transform values function distribution governed goal choose function resulting values specific desired distribution Integrating obtain indefinite integral toExercise transform uniformly distributed random numbers function inverse indefinite integral desired illustrated Figure Consider example exponential distribution case lower limit integral transform uniformly distributed variable exponential Basic Sampling Algorithms Figure Geometrical interpretation transformation method generating nonuniformly distributed random indefinite integral desired distribution uniformly distributed random variable transformed distributed according example distribution transformation method applied Cauchy distribution inverse indefinite integral expressed terms generalization multiple variables straightforward involves Jacobian change zM zM final example transformation method consider Box-Muller method generating samples Gaussian suppose generate pairs uniformly distributed random numbers transforming variable distributed uniformly 2z discard pair unless satisfies z21 z22 leads uniform distribution points inside unit circle illustrated Figure pair evaluate quantities Figure Box-Muller method generating Gaussian distributed random numbers starts generating samples uniform distribution inside unit 1z1 SAMPLING METHODS z21 z22 joint distribution byExercise independent Gaussian distribution zero mean unit Gaussian distribution zero mean unit Gaussian distribution mean variance generate vectorvalued variables having multivariate Gaussian distribution mean covariance make use Cholesky takes form LLT et vector valued random variable components independent Gaussian distributed zero mean unit Lz mean covariance transformation technique depends success ability calculate invert indefinite integral required operations feasible limited number simple turn alternative approaches search general consider techniques called rejection sampling importance mainly limited univariate distributions directly applicable complex problems form important components general Rejection sampling rejection sampling framework allows sample relatively complex subject certain begin considering univariate distributions discuss extension multiple dimensions Suppose wish sample distribution standard distributions considered sampling directly Furthermore easily able evaluate value normalizing constant Zp readily Zp order apply rejection need simpler distribution called proposal readily draw Basic Sampling Algorithms Figure rejection sampling samples drawn simple distribution rejected fall grey area unnormalized distribution scaled distribution resulting samples distributed according normalized version introduce constant value chosen values function called comparison function illustrated univariate distribution Figure step rejection sampler involves generating random generate number distribution generate number uniform distribution pair random numbers uniform distribution curve function sample pair rejected lies grey shaded region Figure remaining pairs uniform distribution curve corresponding values distributed according original values generated distribution samples accepted probability probability sample accepted dz fraction points rejected method depends ratio area unnormalized distribution area curve constant small possible subject limitation illustration use rejection consider task sampling gamma distribution bell-shaped shown Figure suitable proposal distribution Cauchy bell-shaped use transformation discussed sample need generalize Cauchy slightly ensure smaller value gamma achieved transforming uniform random variable tan gives random numbers distributed according SAMPLING METHODS Figure Plot showing gamma distribution green scaled Cauchy proposal distribution shown red Samples gamma distribution obtained sampling Cauchy applying rejection sampling minimum reject rate obtained setting 2a choosing constant small possible satisfying requirement resulting comparison function illustrated Figure Adaptive rejection sampling instances wish apply rejection proves difficult determine suitable analytic form envelope distribution alternative approach construct envelope function fly based measured values distribution Construction envelope function particularly straightforward cases log words derivatives nonincreasing functions construction suitable envelope function illustrated graphically Figure function gradient evaluated initial set grid intersections resulting tangent lines used construct envelope sample value drawn envelope straightforward log envelope distribution successionExercise Figure case distributions log envelope function use rejection sampling constructed tangent lines computed set grid sample point added set grid points used refine envelope Basic Sampling Algorithms Figure Illustrative example rejection sampling involving sampling Gaussian distribution shown green rejection sampling proposal distribution Gaussian scaled version shown red linear envelope distribution comprises piecewise exponential distribution form exp sample usual rejection criterion sample draw desired sample incorporated set grid new tangent line envelope function number grid points envelope function better approximation desired distribution probability rejection variant algorithm exists avoids evaluation derivatives adaptive rejection sampling framework extended distributions log simply following rejection sampling step Metropolis-Hastings step discussed Section giving rise adaptive rejection Metropolis sampling et Clearly rejection sampling practical require comparison function close required distribution rate rejection kept let examine happens try use rejection sampling spaces high sake somewhat artificial problem wish sample zero-mean multivariate Gaussian distribution covariance unit rejection sampling proposal distribution zero-mean Gaussian distribution having covariance order exists D-dimensions optimum value illustrated Figure acceptance rate ratio volumes distributions just acceptance rate diminishes exponentially exceeds just acceptance ratio approximately illustrative example comparison function close required practical desired distribution multimodal sharply extremely difficult good proposal distribution comparison SAMPLING METHODS Figure Importance sampling addresses problem evaluating expectation function respect distribution difficult draw samples samples drawn simpler distribution corresponding terms summation weighted ratios exponential decrease acceptance rate dimensionality generic feature rejection rejection useful technique dimensions unsuited problems high play role subroutine sophisticated algorithms sampling high dimensional Importance sampling principal reasons wishing sample complicated probability distributions able evaluate expectations form technique importance sampling provides framework approximating expectations directly does provide mechanism drawing samples distribution finite sum approximation depends able draw samples distribution impractical sample directly evaluate easily value simplistic strategy evaluating expectations discretize z-space uniform grid evaluate integrand sum form obvious problem approach number terms summation grows exponentially dimensionality kinds probability distributions mass confined relatively small regions space uniform sampling inefficient high-dimensional small proportion samples make significant contribution really like choose sample points fall regions ideally product case rejection importance sampling based use proposal distribution easy draw illustrated Figure express expectation form finite sum Basic Sampling Algorithms samples drawn dz dz quantities rl known importance correct bias introduced sampling wrong Note unlike rejection samples generated case distribution evaluated normalization evaluated Zp wish use importance sampling distribution dz Zq Zp dz Zq Zp use sample set evaluate ratio result Zp Zq Zq dz dz defined wl rejection success importance sampling approach depends crucially sampling distribution matches desired SAMPLING METHODS distribution strongly varying significant proportion mass concentrated relatively small regions set importance weights dominated weights having large remaining weights relatively effective sample size smaller apparent sample size problem severe samples falls regions apparent variances rl small estimate expectation severely major drawback importance sampling method potential produce results arbitrarily error diagnostic highlights key requirement sampling distribution small zero regions distributions defined terms graphical apply importance sampling technique various discrete simple approach called uniform joint distribution directed graph defined sample joint distribution obtained setting variables zi evidence set equal observed remaining variables sampled independently uniform distribution space possible determine corresponding weight associated sample note sampling distribution uniform possible choices denotes subset variables equality follows fact sample generated necessarily consistent weights rl simply proportional Note variables sampled approach yield poor results posterior distribution far case improvement approach called likelihood weighted sampling Shachter based ancestral sampling variable variable evidence just set instantiated evidence sampled conditional distribution conditioning variables set currently sampled weighting associated resulting sample zi method extended self-importance sampling importance sampling distribution continually updated reflect current estimated posterior Sampling-importance-resampling rejection sampling method discussed Section depends success determination suitable value constant pairs distributions impractical determine suitable Basic Sampling Algorithms value value sufficiently large guarantee bound desired distribution lead impractically small acceptance case rejection sampling-importance-resampling approach makes use sampling distribution avoids having determine constant stages samples drawn second weights wL constructed second set samples drawn discrete distribution probabilities weights resulting samples approximately distributed according distribution correct limit consider univariate note cumulative distribution resampled values wl indicator function equals argument true Taking limit assuming suitable regularity replace sums integrals weighted according original sampling distribution dz dz dz cumulative distribution function normalization finite value initial sample resampled values approximately drawn desired rejection approximation improves sampling distribution gets closer desired distribution initial samples desired weights wn resampled values desired moments respect distribution SAMPLING METHODS evaluated directly original samples dz dz Sampling EM algorithm addition providing mechanism direct implementation Bayesian Monte Carlo methods play role frequentist example maximum likelihood sampling methods used approximate step EM algorithm models step performed Consider model hidden variables visible variables parameters function optimized respect step expected complete-data log use sampling methods approximate integral finite sum samples drawn current estimate posterior distribution function optimized usual way procedure called Monte Carlo EM straightforward extend problem finding mode posterior distribution MAP prior distribution simply adding function performing particular instance Monte Carlo EM called stochastic arises consider finite mixture draw just sample latent variable characterizes components mixture responsible generating data sample taken posterior distribution data effectively makes hard assignment data point components sampled approximation posterior distribution used update model parameters usual Markov Chain Monte Carlo suppose maximum likelihood approach Bayesian treatment wish sample posterior distribution parameter vector like draw samples joint posterior shall suppose computationally Suppose relatively straightforward sample complete-data parameter posterior inspires data augmentation alternates steps known I-step analogous P-step analogous IP Algorithm wish sample note relation draw sample current estimate use draw sample relation dZ use samples obtained I-step compute revised estimate posterior distribution feasible sample approximation Note making distinction parameters hidden variables blur distinction focus simply problem drawing samples posterior Markov Chain Monte Carlo previous discussed rejection sampling importance sampling strategies evaluating expectations saw suffer severe limitations particularly spaces high turn section general powerful framework called Markov chain Monte Carlo allows sampling large class SAMPLING METHODS scales dimensionality sample Markov chain Monte Carlo methods origins physics end 1980s started significant impact field rejection importance sample proposal maintain record current state proposal distribution depends current sequence samples forms Markov write assume readily evaluated value value Zp proposal distribution chosen sufficiently simple straightforward draw samples cycle generate candidate sample proposal distribution accept sample according appropriate basic Metropolis algorithm et assume proposal distribution values zA zB candidate sample accepted probability min achieved choosing random number uniform distribution unit interval accepting sample Note step causes increase value candidate point certain candidate sample candidate point set candidate sample drawn distribution contrast rejection rejected samples simply Metropolis algorithm candidate point previous sample included instead final list leading multiple copies practical single copy retained sample integer weighting factor recording times state shall long positive values zA zB sufficient necessary distribution tends sequence set independent samples successive samples highly wish obtain independent discard sequence just retain th sufficiently retained samples practical purposes Figure shows simple illustrative example sampling two-dimensional Gaussian distribution Metropolis algorithm proposal distribution isotropic insight nature Markov chain Monte Carlo algorithms gleaned looking properties specific simple random Markov Chain Monte Carlo Figure simple illustration Metropolis algorithm sample Gaussian distribution standard-deviation contour shown proposal distribution isotropic Gaussian distribution standard deviation Steps accepted shown green rejected steps shown total candidate samples Consider state space consisting probabilities denotes state step initial state symmetry expected state time zero similarly easily seen random walk trav-Exercise elled distance average proportional square root square root dependence typical random walk behaviour shows random walks inefficient exploring state shall central goal designing Markov chain Monte Carlo methods avoid random walk Markov chains discussing Markov chain Monte Carlo methods useful study general properties Markov chains ask circumstances Markov chain converge desired first-order Markov chain defined series random variables following conditional independence property holds course represented directed graph form example shown Figure specify Markov chain giving probability distribution initial variable SAMPLING METHODS conditional probabilities subsequent variables form transition probabilities Markov chain called homogeneous transition probabilities marginal probability particular variable expressed terms marginal probability previous variable chain form distribution said respect Markov chain step chain leaves distribution homogeneous Markov chain transition probabilities distribution invariant Note Markov chain invariant transition probabilities identity distribution sufficient condition ensuring required distribution invariant choose transition probabilities satisfy property detailed defined particular distribution easily seen transition probability satisfies detailed balance respect particular distribution leave distribution Markov chain respects detailed balance said goal use Markov chains sample achieve set Markov chain desired distribution require distribution converges required invariant distribution irrespective choice initial distribution property called invariant distribution called equilibrium ergodic Markov chain equilibrium shown homogeneous Markov chain subject weak restrictions invariant distribution transition probabilities practice construct transition probabilities set transitions BK achieved mixture distribution form Markov Chain Monte Carlo set mixing coefficients satisfying base transitions combined successive distribution invariant respect base obviously invariant respect case mixture base transitions satisfies detailed mixture transition satisfy detailed does hold transition probability constructed symmetrizing order application base form BK BK detailed balance common example use composite transition probabilities base transition changes subset Metropolis-Hastings algorithm Earlier introduced basic Metropolis actually demonstrating samples required giving discuss known Metropolis-Hastings algorithm case proposal distribution longer symmetric function particular step current state draw sample distribution accept probability min labels members set possible transitions evaluation acceptance criterion does require knowledge normalizing constant Zp probability distribution symmetric proposal distribution Metropolis-Hastings criterion reduces standard Metropolis criterion invariant distribution Markov chain defined Metropolis-Hastings algorithm showing detailed defined min min specific choice proposal distribution marked effect performance continuous state common choice Gaussian centred current leading important trade-off determining variance parameter variance SAMPLING METHODS Figure Schematic illustration use isotropic Gaussian proposal distribution sample correlated multivariate Gaussian distribution having different standard deviations different Metropolis-Hastings order rejection rate scale proposal distribution order smallest standard deviation leads random walk behaviour number steps separating states approximately independent order largest standard proportion accepted transitions progress state space takes form slow random walk leading long correlation variance parameter rejection rate high kind complex problems proposed steps states probability Consider multivariate distribution having strong correlations components illustrated Figure scale proposal distribution large possible incurring high rejection suggests order smallest length scale explores distribution extended direction means random number steps arrive state independent original state order fact increase rejection rate increases offset larger steps sizes transitions generally multivariate Gaussian number steps required obtain independent samples scales like second-smallest standard deviation details remains case length scales distributions vary different different Metropolis Hastings algorithm slow Gibbs Sampling Gibbs sampling simple widely applicable Markov chain Monte Carlo algorithm seen special case MetropolisHastings Consider distribution zM wish suppose chosen initial state Markov step Gibbs sampling procedure involves replacing value variables value drawn distribution variable conditioned values remaining replace zi value drawn distribution zi denotes ith component denotes zM zi procedure repeated cycling variables Gibbs Sampling particular order choosing variable updated step random suppose distribution step algorithm selected values replace new value obtained sampling conditional distribution replace value obtained sampling conditional distribution new value used straight away subsequent sampling update sample drawn cycling variables Gibbs Sampling Initialize Sample Sample Sample Sample Josiah Willard Gibbs Gibbs spent entire life living house built father New Gibbs granted PhD engineering United appointed chair mathematical physics United States post received salary time developed field vector analysis contributions crystallography planetary famous entitled Equilibrium Heterogeneous laid foundations science physical SAMPLING METHODS procedure samples required note distribution invariant Gibbs sampling steps individually Markov follows fact sample marginal distribution clearly invariant value step definition samples correct conditional distribution conditional marginal distributions specify joint joint distribution second requirement satisfied order Gibbs sampling procedure samples correct distribution sufficient condition ergodicity conditional distributions point space reached point finite number steps involving update component requirement conditional distributions proven distribution initial states specified order complete samples drawn iterations effectively independent successive samples Markov chain highly obtain samples nearly independent necessary subsample obtain Gibbs sampling procedure particular instance Metropolis-Hastings algorithm Consider Metropolis-Hastings sampling step involving variable zk remaining variables remain transition probability note components unchanged sampling factor determines acceptance probability Metropolis-Hastings used Metropolis-Hastings steps Metropolis gain insight behaviour Gibbs sampling investigating application Gaussian Consider correlated Gaussian illustrated Figure having conditional distributions width marginal distributions width typical step size governed conditional distributions order state evolves according random number steps needed obtain independent samples distribution order course Gaussian distribution Gibbs sampling procedure optimally simple rotate coordinate order decorrelate practical applications generally infeasible approach reducing random walk behaviour Gibbs sampling called over-relaxation original applies problems Gibbs Sampling Figure Illustration Gibbs sampling alternate updates variables distribution correlated step size governed standard deviation conditional distribution leading slow progress direction elongation joint distribution number steps needed obtain independent sample distribution conditional distributions represents general class distributions multivariate Gaussian non-Gaussian distribution Gaussian conditional step Gibbs sampling conditional distribution particular component zi mean variance over-relaxation value zi replaced Gaussian random variable zero mean unit parameter method equivalent standard Gibbs step biased opposite step leaves desired distribution invariant zi mean variance does effect over-relaxation encourage directed motion state space variables highly framework ordered over-relaxation generalizes approach nonGaussian practical applicability Gibbs sampling depends ease samples drawn conditional distributions case probability distributions specified graphical conditional distributions individual nodes depend variables corresponding Markov illustrated Figure directed wide choice conditional distributions individual nodes conditioned parents lead conditional distributions Gibbs sampling log adaptive rejection sampling methods discussed Section provide framework Monte Carlo sampling directed graphs broad graph constructed distributions exponential parent-child relationships preserve conditional distributions arising Gibbs sampling functional form orig546 SAMPLING METHODS Figure Gibbs sampling method requires samples drawn conditional distribution variable conditioned remaining graphical conditional distribution function states nodes Markov undirected graph comprises set shown directed graph Markov blanket comprises shown inal conditional distributions defining standard sampling techniques conditional distributions complex form does permit use standard sampling conditionals log sampling efficiently adaptive rejection sampling corresponding variable stage Gibbs sampling instead drawing sample corresponding conditional make point estimate variable maximum conditional obtain iterated conditional modes algorithm discussed Section ICM seen greedy approximation Gibbs basic Gibbs sampling technique considers variable strong dependencies successive opposite draw samples directly joint distribution operation supposing successive samples hope improve simple Gibbs sampler adopting intermediate strategy sample successively groups variables individual achieved blocking Gibbs sampling algorithm choosing blocks necessarily sampling jointly variables block conditioned remaining variables et Slice Sampling seen difficulties Metropolis algorithm sensitivity step result slow decorrelation random walk large result inefficiency high rejection technique slice sampling provides adaptive step size automatically adjusted match characteristics requires able evaluate unnormalized distribution Consider univariate Slice sampling involves augmenting additional variable drawing samples joint shall example approach discuss hybrid Monte Carlo Section goal sample uniformly area distribution Slice Sampling uzmin zmax Figure Illustration slice value value chosen uniformly region defines shown solid horizontal infeasible sample directly new sample drawn region zmin contains previous value Zp marginal distribution du Zp du Zp sample sampling ignoring achieved alternately sampling value evaluate sample uniformly range fix sample uniformly distribution defined illustrated Figure difficult sample directly slice distribution instead define sampling scheme leaves uniform distribution achieved ensuring detailed balance Suppose current value denoted obtained corresponding sample value obtained considering region zmin zmax contains choice region adaptation characteristic length scales distribution takes want region encompass slice possible allow large moves space having little possible region lying outside makes sampling approach choice region involves starting region containing having width testing end points lie end point does region extended direction increments value end point lies outside candidate value chosen uniformly lies forms lies outside region shrunk forms end point region contains SAMPLING METHODS candidate point drawn uniformly reduced region value lies Slice sampling applied multivariate distributions repeatedly sampling variable manner Gibbs requires able component function proportional Hybrid Monte Carlo Algorithm major limitations Metropolis algorithm exhibit random walk behaviour distance traversed state space grows square root number problem resolved simply taking bigger steps leads high rejection introduce sophisticated class transitions based analogy physical systems property able make large changes state keeping rejection probability applicable distributions continuous variables readily evaluate gradient log probability respect state discuss dynamical systems framework Section Section explain combined Metropolis algorithm yield powerful hybrid Monte Carlo background physics required section self-contained key results derived Dynamical systems dynamical approach stochastic sampling origins algorithms simulating behaviour physical systems evolving Hamiltonian Markov chain Monte Carlo goal sample probability distribution framework Hamiltonian dynamics exploited casting probabilistic simulation form Hamiltonian order remain keeping literature make use relevant dynamical systems terminology defined dynamics consider corresponds evolution state variable continuous denote Classical dynamics described second law motion acceleration object proportional applied corresponding second-order differential equation decompose second-order equation coupled firstorder equations introducing intermediate momentum variables corresponding rate change state variables having components ri dzi zi regarded position variables dynamics Hybrid Monte Carlo Algorithm position variable corresponding momentum joint space position momentum variables called phase loss write probability distribution form Zp exp interpreted potential energy state acceleration rate change momentum applied negative gradient potential energy dri convenient reformulate dynamical Hamiltonian define kinetic energy r2i total energy sum potential kinetic energies Hamiltonian express dynamics terms Hamiltonian equations byExercise dzi dri William Hamilton William Rowan Hamilton Irish mathematician child appointed Professor Astronomy Trinity important contributions new formulation played significant role later development quantum great achievement development generalize concept complex numbers introducing distinct square roots minus satisfy ijk said equations occurred walking Royal Canal Dublin October promptly carved equations Broome longer evidence stone plaque bridge commemorating discovery displaying quaternion SAMPLING METHODS evolution dynamical value Hamiltonian easily seen differentiation dH dzi dri second important property Hamiltonian dynamical known preserve volume phase consider region space variables region evolves equations Hamiltonian shape change volume seen noting flow field change location phase dz dr divergence field vanishes div dzi dri consider joint distribution phase space total energy distribution ZH results conservation volume conservation follows Hamiltonian dynamics leave seen considering small region phase space approximately follow evolution Hamiltonian equations finite volume region remain unchanged value probability function values integrating Hamiltonian dynamics finite time duration possible make large changes systematic way avoids random walk Evolution Hamiltonian dynamics sample ergodically value order arrive ergodic sampling introduce additional moves phase space change value leaving distribution simplest way achieve replace value drawn distribution conditioned regarded Gibbs sampling Hybrid Monte Carlo Algorithm Section leaves desired distribution Noting independent distribution conditional distribution Gaussian straightforward practical application address problem performing numerical integration Hamiltonian necessarily introduce numerical errors devise scheme minimizes impact turns integration schemes devised theorem holds property important hybrid Monte Carlo discussed Section scheme achieving called leapfrog discretization involves alternately updating discrete-time approximations position momentum variables takes form half-step update momentum variables step size followed full-step update position variables step size followed second half-step update momentum leapfrog steps applied seen half-step updates momentum variables combined full-step updates step size successive updates position momentum variables leapfrog order advance dynamics time interval need error involved discretized approximation continuous time dynamics assuming smooth function limit nonzero used residual error shall Section effects errors eliminated hybrid Monte Carlo summary Hamiltonian dynamical approach involves alternating series leapfrog updates resampling momentum variables marginal Note Hamiltonian dynamics unlike basic Metropolis able make use information gradient log probability distribution distribution analogous situation familiar domain function cases gradient information highly advantageous make use follows fact space dimension additional computational cost evaluating gradient compared evaluating function typically fixed factor independent D-dimensional gradient vector conveys pieces information compared piece information function SAMPLING METHODS Hybrid Monte Carlo discussed previous nonzero step size discretization leapfrog algorithm introduce errors integration Hamiltonian dynamical Hybrid Monte Carlo et combines Hamiltonian dynamics Metropolis algorithm removes bias associated algorithm uses Markov chain consisting alternate stochastic updates momentum variable Hamiltonian dynamical updates leapfrog application leapfrog resulting candidate state accepted rejected according Metropolis criterion based value Hamiltonian initial state state leapfrog candidate state accepted probability min leapfrog integration simulate Hamiltonian dynamics candidate step automatically accepted value numerical value like Metropolis criterion remove bias effect ensure resulting samples drawn required order need ensure update equations corresponding leapfrog integration satisfy detailed balance easily achieved modifying leapfrog scheme start leapfrog integration choose equal integrate forwards time step size backwards time step size note leapfrog integration scheme integration steps step size exactly undo effect integration steps step size leapfrog integration preserves phase-space volume follows fact step leapfrog scheme updates zi variable ri variable function shown Figure effect shearing region phase space altering use results detailed balance Consider small region phase space sequence leapfrog iterations step size maps region conservation volume leapfrog volume choose initial point distribution update leapfrog probability transition going ZH min factor arises probability choosing integrate positive step size negative probability starting Hybrid Monte Carlo Algorithm ri zi Figure step leapfrog algorithm modifies position variable zi momentum variable change variable function region phase space sheared change region integrating backwards time end region ZH min easily seen probabilities detailed balance Note proof ignores overlap regionsExercise easily generalized allow difficult construct examples leapfrog algorithm returns starting position finite number random replacement momentum values leapfrog integration sufficient ensure ergodicity position variables phenomena easily avoided choosing magnitude step size random small leapfrog gain insight behaviour hybrid Monte Carlo algorithm considering application multivariate consider Gaussian distribution independent Hamiltonian z2i r2i conclusions equally valid Gaussian distribution having correlated components hybrid Monte Carlo algorithm exhibits rotational leapfrog pair phase-space variables ri evolves acceptance rejection candidate point based value depends values significant integration error variables lead high probability order discrete leapfrog integration reasonably SAMPLING METHODS good approximation true continuous-time necessary leapfrog integration scale smaller shortest length-scale potential varying governed smallest value denote Recall goal leapfrog integration hybrid Monte Carlo substantial distance phase space new state relatively independent initial state achieve high probability order achieve leapfrog integration continued number iterations order consider behaviour simple Metropolis algorithm isotropic Gaussian proposal distribution variance considered order avoid high rejection value order exploration state space proceeds random walk takes order steps arrive roughly independent Estimating Partition Function sampling algorithms considered chapter require functional form probability distribution multiplicative write ZE value normalization constant ZE known partition needed order draw samples knowledge value ZE useful Bayesian model comparison represents model evidence probability observed data consider value assume direct evaluation function state space model actually ratio partition functions models Multiplication ratio ratio prior probabilities gives ratio posterior used model selection model way estimate ratio partition functions use importance sampling distribution energy function ZE ZG Estimating Partition Function samples drawn distribution defined distribution pG partition function evaluated example absolute value ZE approach yield accurate results importance sampling distribution pG closely matched distribution pE ratio does wide suitable analytically specified importance sampling distributions readily kinds complex models considered alternative approach use samples obtained Markov chain define importance-sampling transition probability Markov chain sample set sampling distribution written ZG exp used directly Methods estimating ratio partition functions require success corresponding distributions reasonably closely especially problematic wish absolute value partition function complex distribution relatively simple distributions partition function evaluated attempting estimate ratio partition functions directly unlikely problem tackled technique known chaining Barber involves introducing succession intermediate distributions interpolate simple distribution evaluate normalization coefficient desired complex distribution pM ZM ZM intermediate ratios determined Monte Carlo methods discussed way construct sequence intermediate systems use energy function containing continuous parameter interpolates distributions intermediate ratios Monte efficient use single Markov chain run restart Markov chain Markov chain run initially suitable number steps moves distribution remain close equilibrium distribution SAMPLING METHODS Exercises www finite sample estimator defined mean equal variance Suppose random variable uniform distribution transform distribution random variable uniformly distributed transformation Cauchy distribution Suppose uniformly distributed unit shown Figure make change variables distributed according www Let D-dimensional random variable having Gaussian distribution zero mean unit covariance suppose positive definite symmetric matrix Cholesky decomposition LLT lowertriangular matrix zeros leading variable Lz Gaussian distribution mean covariance provides technique generating samples general multivariate Gaussian samples univariate Gaussian having zero mean unit www carefully rejection sampling does draw samples desired distribution Suppose proposal distribution probability sample value accepted unnormalized distribution proportional constant set smallest value ensures values Note probability drawing value probability drawing value times probability accepting value Make use sum product rules write normalized form distribution equals Suppose uniform distribution interval variable tan Cauchy distribution Determine expressions coefficients ki envelope distribution adaptive rejection sampling requirements continuity making use technique discussed Section sampling single exponential devise algorithm sampling piecewise exponential distribution defined simple random walk integers defined property induction Exercises Figure probability distribution variables uniform shaded regions zero www Gibbs sampling discussed Section satisfies detailed balance defined Consider distribution shown Figure Discuss standard Gibbs sampling procedure distribution sample correctly distribution Consider simple 3-node graph shown Figure observed node Gaussian distribution mean precision Suppose marginal distributions mean precision denotes gamma Write expressions conditional distributions required order apply Gibbs sampling posterior distribution Verify over-relaxation update zi mean variance zero mean unit gives value mean variance www Hamiltonian equation equivalent equivalent making use conditional distribution Figure graph involving observed Gaussian variable prior distributions mean precision SAMPLING METHODS www Verify probabilities detailed balance holds hybrid Monte Carlo 
Continuous Latent Variables Chapter discussed probabilistic models having discrete latent mixture explore models latent variables important motivation models data sets property data points lie close manifold lower dimensionality original data consider artificial data set constructed taking off-line represented pixel grey-level embedding largerAppendix image size padding pixels having value zero white location orientation digit varied illustrated Figure resulting images represented point 000-dimensional data data set degrees freedom corresponding vertical horizontal translations data points live subspace data space intrinsic dimensionality Note CONTINUOUS LATENT VARIABLES Figure synthetic data set obtained taking off-line digit images creating multiple copies digit undergone random displacement rotation larger image resulting images manifold nonlinear translate digit past particular pixel value zero zero clearly nonlinear function digit translation rotation parameters latent variables observe image vectors told values translation rotation variables used create real digit image degree freedom arising multiple additional degrees freedom associated complex deformations variability writing differences writing styles number degrees freedom small compared dimensionality data example provided oil flow data ge-Appendix ometrical configuration oil degrees freedom variability corresponding fraction oil pipe fraction water fraction gas data space comprises data set points lie close two-dimensional manifold embedded manifold comprises distinct segments corresponding different flow segment continuous two-dimensional goal data density benefits exploiting manifold data points confined precisely smooth lowdimensional interpret departures data points manifold leads naturally generative view models select point manifold according latent variable distribution generate observed data point adding drawn conditional distribution data variables latent simplest continuous latent variable model assumes Gaussian distributions latent observed variables makes use linear-Gaussian dependence observed variables state latent leadsSection probabilistic formulation well-known technique principal component analysis related model called factor chapter begin nonprobabilistic treatment PCA arises naturally maximum likelihood solution Principal Component Analysis Figure Principal component analysis seeks space lower known principal subspace denoted magenta orthogonal projection data points subspace maximizes variance projected points alternative definition PCA based minimizing sum-of-squares projection indicated blue xn particular form linear-Gaussian latent variable probabilistic refor-Section mulation brings use EM parameter principled extensions mixtures PCA Bayesian formulations allow number principal components determined automatically discuss briefly generalizations latent variable concept linear-Gaussian assumption including non-Gaussian latent leads framework independent component models having nonlinear relationship latent observed Principal Component Analysis Principal component technique widely used applications dimensionality lossy data feature data visualization known commonly used definitions PCA rise PCA defined orthogonal projection data lower dimensional linear known principal variance projected data maximized defined linear projection minimizes average projection defined mean squared distance data points projections process orthogonal projection illustrated Figure consider definitions Maximum variance formulation Consider data set observations xn Euclidean variable dimensionality goal project data space having dimensionality maximizing variance projected shall assume value Later CONTINUOUS LATENT VARIABLES shall consider techniques determine appropriate value begin consider projection one-dimensional space define direction space D-dimensional vector convenience loss shall choose unit vector uT1 interested direction defined magnitude data point xn projected scalar value uT1 mean projected data uT1 sample set mean xn variance projected data uT1 xn uT1 uT1 Su1 data covariance matrix defined maximize projected variance uT1 Su1 respect constrained maximization prevent appropriate constraint comes normalization condition uT1 enforce introduce Lagrange multiplier shall denote make anAppendix unconstrained maximization uT1 Su1 uT1 setting derivative respect equal quantity stationary point Su1 says eigenvector left-multiply uT1 make use uT1 variance uT1 Su1 variance maximum set equal eigenvector having largest eigenvalue eigenvector known principal define additional principal components incremental fashion choosing new direction maximizes projected variance Principal Component Analysis possible directions orthogonal consider general case -dimensional projection optimal linear projection variance projected data maximized defined eigenvectors data covariance matrix corresponding largest eigenvalues easily shown proof principal component analysis involves evaluating mean covariance matrix data set finding eigenvectors corresponding largest Algorithms finding eigenvectors additional theorems related eigenvector Golub Van Loan Note computational cost computing eigenvector decomposition matrix size plan project data principal need eigenvalues efficient power method Van scale like alternatively make use EM Minimum-error formulation discuss alternative formulation PCA based projection error introduce complete orthonormal set D-dimensionalAppendix basis vectors satisfy uTi uj basis data point represented exactly linear combination basis vectors xn coefficients different different data simply corresponds rotation coordinate new defined original components replaced equivalent set Taking inner product uj making use orthonormality obtain xTnuj loss generality write xn xTnui approximate data point representation involving restricted number variables corresponding projection lower-dimensional -dimensional linear subspace loss basis approximate data point xn zniui biui CONTINUOUS LATENT VARIABLES depend particular data constants data free choose minimize distortion introduced reduction distortion shall use squared distance original data point xn approximation averaged data goal minimize Consider minimization respect quantities Substituting setting derivative respect znj making use orthonormality obtain znj xTnuj setting derivative respect bi making use orthonormality gives bj xTuj substitute zni make use general expansion obtain xn ui displacement vector xn lies space orthogonal principal linear combination illustrated Figure expected projected points lie principal freely minimum error orthogonal obtain expression distortion measure function purely form xTnui xTui uTi remains task minimizing respect constrained minimization obtain vacuous result ui constraints arise orthonormality conditions shall solution expressed terms eigenvector expansion covariance considering formal let try obtain intuition result considering case two-dimensional data space onedimensional principal subspace choose direction Principal Component Analysis minimize uT2 subject normalization constraint uT2 Lagrange multiplier enforce consider minimization uT2 Su2 uT2 Setting derivative respect obtain Su2 eigenvector eigenvalue eigenvector define stationary point distortion value back-substitute solution distortion measure obtain minimum value choosing eigenvector corresponding smaller choose principal subspace aligned eigenvector having larger result accords intuition order minimize average squared projection choose principal component subspace pass mean data points aligned directions maximum case eigenvalues choice principal direction rise value general solution minimization arbitrary arbitrary obtained choosing eigenvectors covariance matrix Sui usual eigenvectors chosen corresponding value distortion measure simply sum eigenvalues eigenvectors orthogonal principal obtain minimum value selecting eigenvectors having smallest eigenvectors defining principal subspace corresponding largest considered PCA analysis holds case dimensionality reduction simply rotation coordinate axes align principal worth noting exists closely related linear dimensionality reduction technique called canonical correlation CCA Bach PCA works single random CCA considers variables tries corresponding pair linear subspaces high component subspaces correlated single component solution expressed terms generalized eigenvector Applications PCA illustrate use PCA data compression considering offline digits data eigenvector covariance matrix vectorAppendix CONTINUOUS LATENT VARIABLES Mean Figure mean vector PCA eigenvectors off-line digits data corresponding original D-dimensional represent eigenvectors images size data corresponding shown Figure plot complete spectrum sorted decreasing shown Figure distortion measure associated choosing particular value sum eigenvalues plotted different values Figure substitute write PCA approximation data vector xn form xTnui xTui ui Figure Plot eigenvalue spectrum off-line digits data Plot sum discarded represents sum-of-squares distortion introduced projecting data principal component subspace dimensionality Principal Component Analysis Original Figure original example off-line digits data set PCA reconstructions obtained retaining principal components various values increases reconstruction accurate perfect use relation xTui ui follows completeness represents compression data data point replaced D-dimensional vector xn -dimensional vector having components xTnui xTui smaller value greater degree Examples PCA reconstructions data points digits data set shown Figure application principal component analysis data goal dimensionality reduction transformation data set order standardize certain important allowing subsequent pattern recognition algorithms applied successfully data original variables measured various different units significantly different instance Old Faithful data time eruptions typically order magnitude greater thanAppendix duration applied K-means algorithm data separate linear re-scaling individual variables thatSection variable zero mean unit known standardizing covariance matrix standardized data components variance known correlation matrix original data property components xi xj data perfectly PCA make substantial normalization data zero mean unit different variables write eigenvector equation form SU UL CONTINUOUS LATENT VARIABLES Figure Illustration effects linear pre-processing applied Old Faithful data plot left shows original centre plot shows result standardizing individual variables zero mean unit shown principal axes normalized data plotted range plot right shows result whitening data zero mean unit diagonal matrix elements orthogonal matrix columns data point transformed value yn sample mean defined set zero covariance identity matrix ynyTn operation known whitening sphereing data illustrated Old Faithful data set Figure interesting compare PCA Fisher linear discriminant discussed Section methods viewed techniques linear dimensionality PCA unsupervised depends values xn Fisher linear discriminant uses class-label difference highlighted example Figure common application principal component analysis data data point projected two-dimensional principal data point xn plotted Cartesian coordinates xTnu1 eigenvectors corresponding largest second largest example oil flow data isAppendix shown Figure Principal Component Analysis Figure comparison principal component analysis linear discriminant linear dimensionality data belonging classes shown red projected single PCA chooses direction maximum shown magenta leads strong class Fisher linear discriminant takes account class labels leads projection green curve giving better class Figure Visualization oil flow data set obtained projecting data principal green points correspond flow configurations PCA high-dimensional data applications principal component number data points smaller dimensionality data want apply PCA data set corresponds vector space potentially million dimensions colour values pixels Note D-dimensional space set defines linear subspace dimensionality little point applying PCA values greater perform PCA eigenvalues corresponding eigenvectors directions data set zero typical algorithms finding eigenvectors matrix computational cost scales like applications image direct application PCA computationally resolve problem let define CONTINUOUS LATENT VARIABLES dimensional centred data nth row covariance matrix written corresponding eigenvector equation XTXui pre-multiply sides define vi obtain XXTvi eigenvector equation matrix eigenvalues original covariance matrix additional eigenvalues value solve eigenvector problem spaces lower dimensionality computational cost instead order determine multiply sides XT XTX eigenvector eigenvalue eigenvectors need determine appropriate re-scale ui XTvi constant assuming vi normalized unit gives ui apply approach evaluate XXT eigenvectors eigenvalues compute eigenvectors original data space Probabilistic PCA formulation PCA discussed previous section based linear projection data subspace lower dimensionality original data PCA expressed maximum likelihood solution probabilistic latent variable reformulation known probabilistic brings advantages compared conventional Probabilistic PCA represents constrained form Gaussian distribution number free parameters restricted allowing model capture dominant correlations data Probabilistic PCA derive EM algorithm PCA computationally efficient situations leading eigenvectors required avoids having evaluate data covariance matrix intermediate combination probabilistic model EM allows deal missing values data Mixtures probabilistic PCA models formulated principled way trained EM Probabilistic PCA forms basis Bayesian treatment PCA dimensionality principal subspace automatically existence likelihood function allows direct comparison probabilistic density conventional PCA assign low reconstruction cost data points close principal subspace lie arbitrarily far training Probabilistic PCA used model class-conditional densities applied classification probabilistic PCA model run generatively provide samples formulation PCA probabilistic model proposed independently Tipping Bishop Roweis shall closely related factor analysis Probabilistic PCA simple example linear-Gaussian inSection marginal conditional distributions formulate probabilistic PCA introducing explicit latent variable corresponding principal-component define Gaussian prior distribution latent Gaussian conditional distribution observed variable conditioned value latent prior distribution zero-mean unit-covariance Gaussian conditional distribution observed variable conditioned value latent variable form mean general linear function governed matrix D-dimensional vector Note factorizes respect elements words example naive Bayes AsSection shall columns span linear subspace data space corresponds principal parameter model scalar governing variance conditional Note CONTINUOUS LATENT VARIABLES Figure illustration generative view probabilistic PCA model two-dimensional data space one-dimensional latent observed data point generated drawing value bz latent variable prior distribution drawing value isotropic Gaussian distribution red having mean wbz covariance green ellipses density contours marginal distribution loss generality assuming zero unit covariance Gaussian latent distribution general Gaussian distribution rise equivalent probabilistic view probabilistic PCA model generative viewpoint sampled value observed variable obtained choosing value latent variable sampling observed variable conditioned latent D-dimensional observed variable defined linear transformation -dimensional latent variable plus additive Gaussian Wz -dimensional Gaussian latent D-dimensional zero-mean Gaussian-distributed noise variable covariance generative process illustrated Figure Note framework based mapping latent space data contrast conventional view PCA discussed reverse data space latent obtained shortly Suppose wish determine values parameters maximum write likelihood need expression marginal distribution observed sum product rules form corresponds linear-Gaussian marginal distribution byExercise Probabilistic PCA covariance matrix defined WWT result derived directly noting predictive distribution Gaussian evaluating mean covariance gives WzzTWT WWT used fact independent random variables think distribution defined taking isotropic Gaussian moving principal subspace spraying Gaussian ink density determined weighted prior accumulated ink density gives rise shaped distribution representing marginal density predictive distribution governed parameters redundancy parameterization corresponding rotations latent space consider matrix WR orthogonal orthogonality property RRT quantity appears covariance matrix takes form WRRTWT WWT independent family matrices rise predictive invariance understood terms rotations latent shall return discussion number independent parameters model evaluate predictive require involves inversion computation required reduced making use matrix inversion identity matrix defined WTW invert inverting cost evaluating reduced predictive distribution require posterior distribution written directly result linear-Gaussian models giveExercise Note posterior mean depends posterior covariance independent CONTINUOUS LATENT VARIABLES Figure probabilistic PCA model data set observations expressed directed graph observation xn associated value latent xn Maximum likelihood PCA consider determination model parameters maximum data set observed data probabilistic PCA model expressed directed shown Figure corresponding log likelihood function Setting derivative log likelihood respect equal zero gives expected result data mean defined Back-substituting write log likelihood function form Tr data covariance matrix defined log likelihood quadratic function solution represents unique confirmed computing second Maximization respect complex nonetheless exact closed-form shown Tipping Bishop stationary points log likelihood function written WML UM UM matrix columns subset size eigenvectors data covariance matrix diagonal matrix LM elements corresponding eigenvalues arbitrary orthogonal Tipping Bishop showed maximum likelihood function obtained eigenvectors chosen eigenvalues largest solutions saddle similar result conjectured independently Roweis proof Probabilistic PCA shall assume eigenvectors arranged order decreasing values corresponding principal eigenvectors columns define principal subspace standard corresponding maximum likelihood solution average variance associated discarded interpreted rotation matrix latent substitute solution expression make use orthogonality property RRT independent simply says predictive density unchanged rotations latent space discussed particular case columns principal component eigenvectors scaled variance parameters interpretation scaling factors clear recognize convolution independent Gaussian distributions case latent space distribution noise variances variance direction eigenvector ui composed sum contribution projection unit-variance latent space distribution data space corresponding column plus isotropic contribution variance added directions noise worth taking moment study form covariance matrix Consider variance predictive distribution direction specified unit vector vTv suppose orthogonal principal words linear combination discarded vTU vTCv model predicts noise variance orthogonal principal just average discarded suppose ui ui retained eigenvectors defining principal vTCv model correctly captures variance data principal approximates variance remaining directions single average value way construct maximum likelihood density model simply eigenvectors eigenvalues data covariance matrix evaluate results choose maximum likelihood solution numerical optimization likelihood instance algorithm conjugate gradients Nocedal Bishop EM resulting value es-Section sentially implies columns need orthogonal basis matrix post-processed appropriately Van EM algorithm modified way yield orthonormal principal sorted descending order corresponding directly CONTINUOUS LATENT VARIABLES rotational invariance latent space represents form statistical analogous encountered mixture models case discrete latent continuum parameters lead predictive contrast discrete nonidentifiability associated component re-labelling mixture consider case reduction UM LM Making use orthogonality properties UUT RRT covariance marginal distribution ULUT obtain standard maximum likelihood solution unconstrained Gaussian distribution covariance matrix sample Conventional PCA generally formulated projection points Ddimensional data space -dimensional linear Probabilistic naturally expressed mapping latent space data space applications visualization data reverse mapping point data space summarized posterior mean covariance latent mean projects point data space Note takes form equations regularized linear regressionSection consequence maximizing likelihood function linear Gaussian posterior covariance independent limit posterior mean reduces represents orthogonal projection data point latent recover standard PCA posterior covariance limit isExercise density latent projection shifted relative orthogonal note important role probabilistic PCA model defining multivariate Gaussian distribution number degrees words number independent controlled whilst allowing model capture dominant correlations Recall general Gaussian distribution independent parameters covariance matrix parameters number ofSection parameters scales quadratically excessive spaces high Probabilistic PCA restrict covariance matrix independent number parameters grows linearly treats variables independent longer express correlations Probabilistic PCA provides elegant compromise significant correlations captured ensuring total number parameters grows linearly evaluating number degrees freedom PPCA model covariance matrix depends parameters size giving total parameter count DM seen redundancy parameterization associated rotations coordinate latent orthogonal matrix expresses rotations size column matrix independent column vector normalized unit second column independent column normalized orthogonal previous Summing arithmetic total independent number degrees freedom covariance matrix DM number independent parameters model grows linearly fixed recover standard result covariance variance linearly in-Exercise dependent directions controlled columns variance remaining direction model equivalent isotropic covariance EM algorithm PCA probabilistic PCA model expressed terms marginalization continuous latent space data point corresponding latent variable make use EM algorithm maximum likelihood estimates model pointless obtained exact closed-form solution maximum likelihood parameter spaces high computational advantages iterative EM procedure working directly sample covariance EM procedure extended factor analysis noSection closed-form allows missing data handled principled derive EM algorithm probabilistic PCA following general framework write complete-data log likelihood takeSection expectation respect posterior distribution latent distribution evaluated parameter Maximization expected completedata log likelihood yields parameter data points CONTINUOUS LATENT VARIABLES assumed complete-data log likelihood function takes form nth row matrix know exact maximum likelihood solution sample mean defined convenient substitute Making use expressions latent conditional taking expectation respect posterior distribution latent obtain Tr Tr TW Note depends posterior distribution sufficient statistics use old parameter values evaluate follow directly posterior distribution standard result defined maximize respect keeping posterior statistics Maximization respect maximization respect make use obtain M-step equationsExercise Wnew ND newWnew EM algorithm probabilistic PCA proceeds initializing parameters alternately computing sufficient statistics latent space posterior distribution step revising parameter values benefits EM algorithm PCA computational efficiency large-scale applications Unlike conventional PCA based Probabilistic PCA eigenvector decomposition sample covariance EM approach iterative appear cycle EM algorithm computationally efficient conventional PCA spaces high note eigendecomposition covariance matrix requires interested eigenvectors corresponding case use algorithms evaluation covariance matrix takes number data Algorithms snapshot method assume eigenvectors linear combinations data avoid direct evaluation covariance matrix unsuited large data EM algorithm described does construct covariance matrix computationally demanding steps involving sums data set large significant saving compared offset iterative nature EM Note EM algorithm implemented on-line form D-dimensional data point read processed discarded data point note quantities evaluated step -dimensional vector computed data point step need accumulate sums data approach advantageous fully probabilistic model deal missing provided missing marginalizing distribution unobserved missing values treated EM example use approach data visualization Figure elegant feature EM approach limit corresponding standard obtain valid EM-like algorithm quantity need compute step step simplified emphasize simplicity let define matrix size nth row vector xn similarly define matrix size nth row vector step EM algorithm PCA step takes form Wnew implemented on-line equations simple interpretation earlier step involves orthogonal projection data points current estimate principal step represents re-estimation principal CONTINUOUS LATENT VARIABLES Figure Probabilistic PCA visualization portion oil flow data set data left-hand plot shows posterior mean projections data points principal right-hand plot obtained randomly omitting variable values EM handle missing Note data point missing measurement plot similar obtained missing subspace minimize squared reconstruction error projections simple physical analogy EM easily visualized Consider collection data points let one-dimensional principal subspace represented solid attach data point rod spring obeying law energy proportional square rod fixed allow attachment points slide rod minimize causes attachment point position orthogonal projection corresponding data point attachment points fixed release rod allow minimum energy steps repeated suitable convergence criterion illustrated Figure Bayesian PCA far discussion assumed value dimensionality principal subspace choose suitable value according generally choose applications appropriate choice approach plot eigenvalue spectrum data analogous example Figure off-line digits data look eigenvalues naturally form groups comprising set small values separated significant gap set relatively large indicating natural choice gap Probabilistic PCA Figure Synthetic data illustrating EM algorithm PCA defined data set data points shown true principal components eigenvectors scaled square roots Initial configuration principal subspace defined shown projections latent points data shown latent space updated held successive values giving orthogonal held second second probabilistic PCA model well-defined likelihood employ cross-validation determine value dimensionality selectingSection largest log likelihood validation data computationally particularly consider probabilistic mixture PCA models seek determine appropriate dimensionality separately component probabilistic formulation natural seek Bayesian approach model need marginalize model parameters respect appropriate prior variational framework approximate analytically intractable marginalizations marginal likelihood variational lower compared range different values value giving largest marginal likelihood consider simpler approach introduced based evidence ap582 CONTINUOUS LATENT VARIABLES Figure Probabilistic graphical model Bayesian PCA distribution parameter matrix governed vector xn appropriate number data points relatively large corresponding posterior distribution tightly peaked involves specific choice prior allows surplus dimensions principal subspace pruned corresponds example automatic relevance discussed Section define independent Gaussian prior column represent vectors defining principal Gaussian independent variance governed precision hyperparameter exp wi wi ith column resulting model represented directed graph shown Figure values iteratively maximizing marginal likelihood function integrated result driven corresponding parameters vector wi driven zero posterior distribution delta function giving sparse effective dimensionality principal subspace determined number finite corresponding vectors wi thought modelling data Bayesian approach automatically making trade-off improving fit larger number vectors wi corresponding eigenvalues tuned reducing complexity model suppressing wi origins sparsity discussed earlier context relevance vector values re-estimated training maximizing log marginal likelihood dW log Note simplicity treat parameters defining priors Probabilistic PCA integration make use Laplace assume posterior distribution sharply occur forSection sufficiently large data re-estimation equations obtained maximizing marginal likelihood respect simple formSection wTi wi follows noting dimensionality wi reestimations interleaved EM algorithm updates determining E-step equations Mstep equation change M-step equation modified Wnew 2A value sample choose values model represents full-covariance infinity model equivalent isotropic model encompass permissible values effective dimensionality principal possible consider smaller values save computational cost limit maximum dimensionality comparison results algorithm standard probabilistic PCA shown Figure Bayesian PCA provides opportunity illustrate Gibbs sampling algorithm discussed Section Figure shows example samples hyperparameters data set dimensions dimensionality latent space data set generated probabilistic PCA model having direction high remaining directions comprising low variance result shows clearly presence distinct modes posterior step hyperparameters small value remaining large latent variables course Gibbs solution makes sharp transitions model described involves prior matrix fully Bayesian treatment including priors solved variational described Bishop discussion various Bayesian approaches determining appropriate dimensionality PCA Minka Factor analysis Factor analysis linear-Gaussian latent variable model closely related probabilistic definition differs probabilistic PCA conditional distribution observed variable latent variable CONTINUOUS LATENT VARIABLES Figure diagrams matrix element matrix depicted square positive black negative area proportional magnitude synthetic data set comprises data points dimensions sampled Gaussian distribution having standard deviation directions standard deviation remaining directions data set dimensions having directions larger variance remaining left-hand plot shows result maximum likelihood probabilistic left-hand plot shows corresponding result Bayesian Bayesian model able discover appropriate dimensionality suppressing surplus degrees taken diagonal isotropic covariance diagonal Note factor analysis common probabilistic assumes observed variables xD latent variable factor analysis model explaining observed covariance structure data representing independent variance associated coordinate matrix capturing covariance variables matrix factor analysis columns capture correlations observed called factor diagonal elements represent independent noise variances called origins factor analysis old discussions factor analysis books Everitt Bartholomew Basilevsky Links factor analysis PCA investigated Lawley Anderson showed stationary points likelihood factor analysis model columns scaled eigenvectors sample covariance average discarded Tipping Bishop showed maximum log likelihood function occurs eigenvectors comprising chosen principal Making use marginal distribution observed Probabilistic PCA Figure Gibbs sampling Bayesian PCA showing plots versus iteration number showing transitions modes posterior variable WWT probabilistic model invariant rotations latent factor analysis subject controversy attempts place interpretation individual factors coordinates proven problematic nonidentifiability factor analysis associated rotations shall view factor analysis form latent variable density form latent space particular choice coordinates used wish remove degeneracy associated latent space consider non-Gaussian latent variable giving rise independent component analysis determine parameters factor analysis model maximum solution sample unlike probabilistic longer closed-form maximum likelihood solution factor analysis latent variable EM algorithm analogous used probabilistic E-step equations defined Note expressed form involves inversion matrices size diagonal matrix inverse trivial CONTINUOUS LATENT VARIABLES compute convenient M-step equations formExercise Wnew diag operator sets nondiagonal elements matrix Bayesian treatment factor analysis model obtained straightforward application techniques discussed difference probabilistic PCA factor analysis concerns different behaviour transformations data PCA probabilis-Exercise tic rotate coordinate data obtain exactly fit data matrix transformed corresponding rotation factor analogous property make component-wise re-scaling data absorbed corresponding re-scaling elements Kernel PCA Chapter saw technique kernel substitution allows algorithm expressed terms scalar products form generalize algorithm replacing scalar products nonlinear apply technique kernel substitution principal component obtaining nonlinear generalization called kernel PCA et Consider data set space dimensionality order notation shall assume subtracted sample mean vectors xn step express conventional PCA form data vectors appear form scalar products Recall principal components defined eigenvectors ui covariance matrix Sui sample covariance matrix defined xnxTn eigenvectors normalized uTi ui consider nonlinear transformation -dimensional feature data point xn projected point Kernel PCA Figure Schematic illustration kernel data set original data space projected nonlinear transformation feature space performing PCA feature obtain principal shown blue denoted vector green lines feature space indicate linear projections principal correspond nonlinear projections original data Note general possible represent nonlinear principal component vector perform standard PCA feature implicitly defines nonlinear principal component model original data illustrated Figure let assume projected data set zero shall return point sample covariance matrix feature space eigenvector expansion defined Cvi goal solve eigenvalue problem having work explicitly feature definition eigenvector equations tells vi satisfies vector vi linear combination written form vi CONTINUOUS LATENT VARIABLES Substituting expansion eigenvector obtain key step express terms kernel function multiplying sides written matrix notation K2ai ai -dimensional column vector elements ani solutions ai solving following eigenvalue problem Kai removed factor sides Note solutions differ eigenvectors having zero eigenvalues affect principal components normalization condition coefficients ai obtained requiring eigenvectors feature space vTi vi aTi Kai Having solved eigenvector resulting principal component projections cast terms kernel function projection point eigenvector expressed terms kernel original D-dimensional space orthogonal eigenvectors linear principal dimensionality feature larger number nonlinear principal components exceed number nonzero eigenvalues exceed number data covariance matrix feature space rank equal reflected fact kernel PCA involves eigenvector expansion matrix Kernel PCA far assumed projected data set zero general simply compute subtract wish avoid working directly feature formulate algorithm purely terms kernel projected data points denoted corresponding elements Gram matrix expressed matrix notation 1NK1N 1N denotes matrix element takes value evaluate kernel function use determine eigenvalues Note standard PCA algorithm recovered special case use linear kernel Figure shows anExercise example kernel PCA applied synthetic data set et kernel form applied synthetic data lines correspond contours projection corresponding principal defined CONTINUOUS LATENT VARIABLES Figure Example kernel Gaussian kernel applied synthetic data set showing eigenfunctions contours lines projection corresponding principal component Note eigenvectors separate eigenvectors split cluster following eigenvectors split clusters halves directions orthogonal previous obvious disadvantage kernel PCA involves finding eigenvectors matrix matrix conventional linear practice large data sets approximations note standard linear retain reduced number eigenvectors approximate data vector xn projection L-dimensional principal defined xTnui kernel general note mapping maps D-dimensional space D-dimensional manifold -dimensional feature space vector known pre-image corresponding point projection points feature space linear PCA subspace space typically lie nonlinear Ddimensional manifold corresponding pre-image data Techniques proposed finding approximate pre-images et Nonlinear Latent Variable Models Nonlinear Latent Variable Models focussed simplest class models having continuous latent based linear-Gaussian having great practical models relatively easy analyse fit data used components complex consider briefly generalizations framework models nonlinear issues nonlinearity non-Gaussianity related general probability density obtained simple fixed reference making nonlinear change idea forms theExercise basis practical latent variable models shall Independent component analysis begin considering models observed variables related linearly latent latent distribution important class known independent component arises consider distribution latent variables understand role consider situation people talking record voices ignore effects time delay signals received microphones point time linear combinations amplitudes coefficients linear combination infer values sample invert mixing process obtain clean signals contains voice just example problem called blind source separation refers fact mixed original sources mixing coefficients observed type problem addressed following approach ignore temporal nature signals treat successive samples consider generative model latent variables corresponding unobserved speech signal observed variables signal values latent variables joint distribution factorizes observed variables linear combination latent need include noise distribution number latent variables equals number observed marginal distribution observed variables general observed variables simply deterministic linear combinations latent data set CONTINUOUS LATENT VARIABLES likelihood function model function coefficients linear log likelihood maximized gradient-based optimization giving rise particular version independent component success approach requires latent variables non-Gaussian recall probabilistic PCA factor latent-space distribution zero-mean isotropic model distinguish different choices latent variables differ simply rotation latent verified directly noting marginal density likelihood unchanged make transformation WR orthogonal matrix satisfying RRT matrix Extending model allow general Gaussian latent distributions does change conclusion model equivalent zero-mean isotropic Gaussian latent variable way Gaussian latent variable distribution linear model insufficient independent components note principal components represent rotation coordinate data space diagonalize covariance data distribution new coordinates zero correlation necessary condition independence common choice latent-variableExercise distribution heavy tails compared reflecting observation real-world distributions exhibit original ICA model based optimization objective function defined information advantage probabilistic latent variable formulation helps motivate formulate generalizations basic independent factor analysis considers model number latent observed variables observed variables individual latent variables flexible distributions modelled mixtures log likelihood model maximized reconstruction latent variables approximated variational types model huge literature ICA applications Comon et Amari et Pearlmutter Hinton et Miskin Hojen-Sorensen et Choudrey Chan et Autoassociative neural networks Chapter considered neural networks context supervised role network predict output variables values Nonlinear Latent Variable Models Figure autoassociative multilayer perceptron having layers network trained map input vectors minimization sum-of-squares nonlinear units hidden network equivalent linear principal component Links representing bias parameters omitted xD zM xD inputs outputs input neural networks applied unsupervised learning used dimensionality achieved network having number outputs optimizing weights minimize measure reconstruction error inputs outputs respect set training Consider multilayer perceptron form shown Figure having output units hidden targets used train network simply input vectors network attempting map input vector network said form autoassociative number hidden units smaller number perfect reconstruction input vectors general determine network parameters minimizing error function captures degree mismatch input vectors shall choose sum-of-squares error form hidden units linear activations shown error function unique global minimum network performs projection -dimensional subspace spanned principal components data Baldi vectors weights lead hidden units Figure form basis set spans principal vectors need orthogonal result principal component analysis neural network linear dimensionality reduction minimizing sum-of-squares error thought limitations linear dimensionality reduction overcome nonlinear activation functions hidden units network Figure nonlinear hidden minimum error solution projection principal component subspace advantage twolayer neural networks perform dimensionality Standard techniques principal component analysis singular value guaranteed correct solution finite generate ordered set eigenvalues corresponding orthonormal CONTINUOUS LATENT VARIABLES Figure Addition extra hidden layers nonlinear units gives autoassociative network perform nonlinear dimensionality xD xD inputs outputs non-linear situation additional hidden layers permitted Consider four-layer autoassociative network shown Figure output units units second hidden layer hidden layers sigmoidal nonlinear activation network trained minimization error function view network successive functional mappings indicated Figure mapping projects original Ddimensional data -dimensional subspace defined activations units second hidden presence hidden layer nonlinear mapping particular restricted second half network defines arbitrary functional mapping -dimensional space original D-dimensional input simple geometrical indicated case Figure network effectively performs nonlinear principal component Figure Geometrical interpretation mappings performed network Figure case inputs units middle hidden function maps -dimensional space D-dimensional space defines way space embedded original mapping embedding indicated mapping defines projection points original D-dimensional space -dimensional subspace Nonlinear Latent Variable Models advantage limited linear contains standard principal component analysis special training network involves nonlinear optimization error function longer quadratic function network Computationally intensive nonlinear optimization techniques risk finding suboptimal local minimum error dimensionality subspace specified training Modelling nonlinear manifolds natural sources data correspond possibly nonlinear manifolds embedded higher dimensional observed data Capturing property explicitly lead improved density modelling compared general consider briefly range techniques attempt way model nonlinear structure combination linear make piece-wise linear approximation clustering technique K-means based Euclidean distance partition data set local groups standard PCA applied better approach use reconstruction error cluster assignment Hinton et common cost function optimized approaches suffer limitations absence overall density probabilistic PCA straightforward define fully probabilistic model simply considering mixture distribution components probabilistic PCA models model discrete latent corresponding discrete continuous latent likelihood function maximized EM fully Bayesian based variational inference allows number components effective dimensionalities individual inferred variants model parameters matrix noise variances tied components isotropic noise distributions replaced diagonal giving rise mixture factor analysers Ghahramani mixture probabilistic PCA models extended hierarchically produce interactive data visualization algorithm alternative considering mixture linear models consider single nonlinear Recall conventional PCA finds linear subspace passes close data least-squares concept extended onedimensional nonlinear surfaces form principal curves curve D-dimensional data space vector-valued function vector elements function scalar possible ways parameterize natural choice arc length point data point curve closest Euclidean denote point CONTINUOUS LATENT VARIABLES gf depends particular curve continuous data density principal curve defined point curve mean points data space project continuous principal interested finite data wish restrict attention smooth Hastie Stuetzle propose two-stage iterative procedure finding principal somewhat reminiscent EM algorithm curve initialized principal algorithm alternates data projection step curve re-estimation projection data point assigned value corresponding closest point re-estimation point curve weighted average points project nearby points points closest curve greatest case subspace constrained procedure converges principal component equivalent power method finding largest eigenvector covariance Principal curves generalized multidimensional manifolds called principal surfaces limited use difficulty data smoothing higher dimensions two-dimensional PCA used project data set lower-dimensional example purposes linear technique similar aim multidimensional MDS finds low-dimensional projection data closely pairwise distances data involves finding eigenvectors distance case distances gives equivalent results MDS concept extended wide variety data types specified terms similarity giving nonmetric nonprobabilistic methods dimensionality reduction data visualization worthy Locally linear LLE computes set coefficients best reconstructs data point coefficients arranged invariant scalings data point characterize local geometrical properties LLE maps high-dimensional data points lower dimensional space preserving neighbourhood local neighbourhood particular data point considered transformation achieved combination preserve angles formed data points weights invariant expect weight values reconstruct data points low-dimensional space high-dimensional data spite optimization LLE does exhibit local isometric feature isomap et goal project data lower-dimensional space dissimilarities defined terms geodesic distances measured Nonlinear Latent Variable Models points lie geodesic arc-length distance measured circumference circle straight line distance measured chord connecting algorithm defines neighbourhood data finding nearest neighbours finding points sphere radius graph constructed linking neighbouring points labelling Euclidean geodesic distance pair points approximated sum arc lengths shortest path connecting standard metric MDS applied geodesic distance matrix low-dimensional focus chapter models observed variables consider models having continuous latent variables discrete observed giving rise latent trait models marginalization continuous latent linear relationship latent observed performed sophisticated techniques Tipping uses variational inference model two-dimensional latent allowing binary data set visualized analogously use PCA visualize continuous Note model dual Bayesian logistic regression problem discussed Section case logistic regression observations feature vector parameterized single parameter vector latent space visualization model single latent space variable copies latent variable generalization probabilistic latent variable models general exponential family distributions described Collins et noted arbitrary distribution formed taking Gaussian random variable transforming suitable exploited general latent variable model called density network MacKay nonlinear function governed multilayered neural network hidden approximate nonlinear function desired downside havingChapter flexible model marginalization latent required order obtain likelihood longer analytically likelihood approximated Monte Carlo techniques drawing samplesChapter Gaussian marginalization latent variables simple sum term large number sample points required order accurate representation procedure computationally consider restricted forms nonlinear make appropriate choice latent variable construct latent variable model nonlinear efficient generative topographic GTM et Bishop et Bishop et uses latent distribution defined finite regular grid delta functions latent Marginalization latent space simply involves summing contributions grid CONTINUOUS LATENT VARIABLES Figure Plot oil flow data set visualized PCA left GTM GTM data point plotted mean posterior distribution latent nonlinearity GTM model allows separation groups data points seen nonlinear mapping linear regression model allows generalChapter nonlinearity linear function adaptive Note usual limitation linear regression models arising curse dimensionalitySection does arise context GTM manifold generally dimensions irrespective dimensionality data consequence choices likelihood function expressed analytically closed form optimized efficiently EM resulting GTM model fits two-dimensional nonlinear manifold data evaluating posterior distribution latent space data projected latent space visualization Figure shows comparison oil data set visualized linear PCA nonlinear GTM seen probabilistic version earlier model called self organizing SOM represents two-dimensional nonlinear manifold regular array discrete SOM somewhat reminiscent K-means algorithm data points assigned nearby prototype vectors subsequently prototypes distributed training process approximate smooth Unlike SOM optimizing well-defined cost function et making difficult set parameters model assess guarantee place dependent choice appropriate parameter values particular data GTM optimizes log likelihood resulting model defines probability density data corresponds constrained mixture Gaussians components share common means constrained lie smooth two-dimensional probaExercises bilistic foundation makes straightforward define generalizations GTM et Bayesian dealing missing principled extension discrete use Gaussian processes toSection define hierarchical GTM model manifold GTM defined continuous just prototype vectors possible compute magnification factors corresponding local expansions compressions manifold needed fit data set et directional curvatures manifold et visualized projected data provide additional insight Exercises www use proof induction linear projection -dimensional subspace maximizes variance projected data defined eigenvectors data covariance matrix corresponding largest Section result proven case suppose result holds general value consequently holds dimensionality set derivative variance projected data respect vector defining new direction data space equal subject constraints orthogonal existing vectors normalized unit Use Lagrange multipliers enforce make use orthonormality properties ofAppendix vectors new vector eigenvector variance maximized eigenvector chosen corresponding eigenvector eigenvalues ordered decreasing minimum value PCA distortion measure respect subject orthonormality constraints obtained ui eigenvectors data covariance matrix introduce matrix Lagrange modified distortion matrix notation reads Tr Tr matrix dimension columns minimize respect solution satisfies possible solution columns eigenvectors case diagonal matrix containing corresponding obtain general assumed symmetric eigenvector expansion general solution gives value specific solution columns CONTINUOUS LATENT VARIABLES eigenvectors solutions convenient choose eigenvector Verify eigenvectors defined normalized unit assuming eigenvectors vi unit www Suppose replace unit-covariance latent space distribution probabilistic PCA model general Gaussian distribution formN redefining parameters leads identical model marginal distribution observed variables valid choice Let D-dimensional random variable having Gaussian distribution consider -dimensional random variable Ax Gaussian expressions mean Discuss form Gaussian distribution www Draw directed probabilistic graph probabilistic PCA model described Section components observed variable shown explicitly separate verify probabilistic PCA model independence structure naive Bayes model discussed Section making use results mean covariance general derive result marginal distribution probabilistic PCA www making use result posterior distribution probabilistic PCA model Verify maximizing log likelihood probabilistic PCA model respect parameter gives result mean data evaluating second derivatives log likelihood function probabilistic PCA model respect parameter stationary point represents unique www limit posterior mean probabilistic PCA model orthogonal projection principal conventional posterior mean probabilistic PCA model shifted origin relative orthogonal optimal reconstruction data point probabilistic according squares projection cost conventional Exercises number independent parameters covariance matrix probabilistic PCA model -dimensional latent space D-dimensional data space Verify case number independent parameters general covariance Gaussian isotropic www Derive M-step equations probabilistic PCA model maximization expected complete-data log likelihood function Figure showed application probabilistic PCA data set data values missing Derive EM algorithm maximizing likelihood function probabilistic PCA model Note missing data values components vectors latent special case data values reduces EM algorithm probabilistic PCA derived Section www Let matrix columns define linear subspace dimensionality embedded data space dimensionality let D-dimensional data set approximate data points linear mapping set -dimensional vectors xn approximated Wzn associated sum-ofsquares reconstruction cost minimizing respect leads analogous expression xn replaced zero-mean variables denote sample minimizing respect kept gives rise PCA step minimizing respect kept gives rise PCA step Derive expression number independent parameters factor analysis model described Section www factor analysis model described Section invariant rotations latent space considering second stationary point log likelihood function factor analysis model discussed Section respect parameter sample mean defined stationary point Derive formulae step EM algorithm factor Note result Exercise parameter replaced sample mean CONTINUOUS LATENT VARIABLES Write expression expected complete-data log likelihood function factor analysis derive corresponding step equations www Draw directed probabilistic graphical model representing discrete mixture probabilistic PCA models PCA model values draw modified graph parameter values shared components saw Section t-distribution viewed infinite mixture Gaussians marginalize respect continuous latent exploiting formulate EM algorithm maximizing log likelihood function multivariate t-distribution observed set data derive forms step www Consider linear-Gaussian latent-variable model having latent space distribution conditional distribution observed variable arbitrary positivedefinite noise covariance suppose make nonsingular linear transformation data variables WML represent maximum likelihood solution corresponding original untransformed represent corresponding maximum likelihood solution transformed data form model preserved diagonal matrix diagonal corresponds case factor transformed remains factor analysis covariant component-wise re-scaling data orthogonal proportional unit matrix corresponds probabilistic transformed matrix remains proportional unit probabilistic PCA covariant rotation axes data case conventional vector ai satisfies satisfy solution having eigenvalue add multiple eigenvector having zero obtain solution eigenvalue modifications affect principal-component projection conventional linear PCA algorithm recovered special case kernel PCA choose linear kernel function www Use transformation property probability density change variable density obtained fixed density nonzero making nonlinear change variable monotonic function Write differential equation satisfied draw diagram illustrating transformation Exercises www Suppose variables independent covariance matrix variables shows independence sufficient condition variables consider variables y22 Write conditional distribution observe dependent showing variables covariance matrix variables use relation off-diagonal terms counter-example shows zero correlation sufficient condition 
Sequential Data far focussed primarily sets data points assumed independent identically distributed assumption allowed express likelihood function product data points probability distribution evaluated data assumption poor consider particularly important class data sequential arise measurement time example rainfall measurements successive days particular daily values currency exchange acoustic features successive time frames used speech example involving speech data shown Figure Sequential data arise contexts time example sequence nucleotide base pairs strand DNA sequence characters English shall refer observations models explored chapter equally applicable SEQUENTIAL DATA Figure Example spectrogram spoken words showing plot intensity spectral coefficients versus time forms sequential just temporal useful distinguish stationary nonstationary sequential stationary data evolves distribution generated remains complex nonstationary generative distribution evolving shall focus stationary financial wish able predict value time series observations previous expect recent observations likely informative historical observations predicting future example Figure shows successive observations speech spectrum highly impractical consider general dependence future observations previous observations complexity model grow limit number observations leads consider Markov models assume future predictions Markov Models Figure simplest approach modelling sequence observations treat corresponding graph pendent recent models severely obtain general retaining introduction latent leading state space Chapters shall complex models constructed simpler components distributions belonging exponential readily characterized framework probabilistic graphical focus important examples state space hidden Markov latent variables linear dynamical latent variables models described directed graphs having tree structure inference performed efficiently sum-product Markov Models easiest way treat sequential data simply ignore sequential aspects treat observations corresponding graph Figure fail exploit sequential patterns correlations observations close observe binary variable denoting particular day rained time series recent observations wish predict rain treat data information glean data relative frequency rainy know practice weather exhibits trends Observing rains today significant help predicting rain express effects probabilistic need relax simplest ways consider Markov note loss use product rule express joint distribution sequence observations form assume conditional distributions right-hand independent previous observations obtain first-order Markov depicted graphical model Figure SEQUENTIAL DATA Figure first-order Markov chain observations distribution particular observation xn conditioned value previous observation joint distribution sequence observations model d-separation conditional distribution observa-Section tion observations time easily verified direct evaluation starting product rule use model predict observationExercise distribution predictions depend value immediately preceding observation independent earlier applications conditional distributions define model constrained corresponding assumption stationary time model known homogeneous Markov conditional distributions depend adjustable parameters values inferred set training conditional distributions chain share values general independence sequential anticipate trends data successive observations provide important information predicting way allow earlier observations influence higher-order Markov allow predictions depend previous-but-one obtain second-order Markov represented graph Figure joint distribution d-separation direct conditional distribution xn independent observations Figure second-order Markov conditional distribution particular observation xn depends values previous observations Markov Models Figure represent sequential data Markov chain latent observation conditioned state corresponding latent important graphical structure forms foundation hidden Markov model linear dynamical xn observation influenced previous similarly consider extensions th order Markov chain conditional distribution particular variable depends previous paid price increased flexibility number parameters model Suppose observations discrete variables having conditional distribution first-order Markov chain specified set parameters states giving total suppose extend model th order Markov joint distribution built conditionals variables conditional distributions represented general conditional probability number parameters model grows exponentially render approach impractical larger values continuous use linear-Gaussian conditional distributions node Gaussian distribution mean linear function known autoregressive AR model et Thiesson et alternative approach use parametric model neural technique called tapped delay line corresponds storing previous values observed variable order predict number parameters smaller completely general model example grow linearly achieved expense restricted family conditional Suppose wish build model sequences limited Markov assumption order specified limited number free achieve introducing additional latent variables permit rich class models constructed simple did mixture distributions Chapter continuous latent variable models Chapter observation introduce corresponding latent variable different type dimensionality observed assume latent variables form Markov giving rise graphical structure known state space shown Figure satisfies key conditional independence property independent SEQUENTIAL DATA joint distribution model d-separation path connecting observed variables xn xm latent path predictive distribution observation previous observations does exhibit conditional independence predictions depends previous observed satisfy Markov property shall discuss evaluate predictive distribution later sections important models sequential data described latent variables obtain hidden Markov HMM et Note observed variables HMM maySection discrete variety different conditional distributions used model latent observed variables Gaussian linear-Gaussian dependence conditional distributions obtain linear dynamical Hidden Markov Models hidden Markov model viewed specific instance state space model Figure latent variables examine single time slice corresponds mixture component densities interpreted extension mixture model choice mixture component observation selected independently depends choice component previous HMM widely used speech recognition Rabiner natural language modelling on-line handwriting recognition et analysis biological sequences proteins DNA et Durbin et Baldi case standard mixture latent variables discrete multinomial variables describing component mixture responsible generating corresponding observation convenient use 1-of-K coding used mixture models Chapter allow probability distribution depend state previous latent variable conditional distribution latent variables K-dimensional binary conditional distribution corresponds table numbers denote elements known transition Ajk satisfy Ajk Ajk matrix Hidden Markov Models Figure Transition diagram showing model latent variables possible states corresponding black lines denote elements transition matrix A12 A23 A31 A21 A32 A13 A11 A22 A33 independent write conditional distribution explicitly form jk initial latent node special does parent marginal distribution represented vector probabilities elements transition matrix illustrated diagrammatically drawing states nodes state transition diagram shown Figure case Note does represent probabilistic graphical nodes separate variables states single shown states boxes useful state transition kind shown Figure unfold gives alternative representation transitions latent known lattice trellis isSection shown case hidden Markov model Figure specification probabilistic model completed defining conditional distributions observed variables set parameters governing known emission example Gaussians form elements continuous conditional probability tables xn distribution value vector numbers corresponding possible states binary vector SEQUENTIAL DATA Figure unfold state transition diagram Figure obtain representation latent column diagram corresponds latent variables A11 A11 A11 A33 A33 A33 represent emission probabilities form shall focuss attention homogeneous models conditional distributions governing latent variables share parameters similarly emission distributions share parameters extension general cases Note mixture model data set corresponds special case parameters Ajk values conditional distribution independent corresponds deleting horizontal links graphical model shown Figure joint probability distribution latent observed variables denotes set parameters governing discussion hidden Markov model independent particular choice emission model tractable wide range emission distributions including discrete mixtures possible exploit discriminative models neural used model theExercise emission density provide representation converted required emission density theorem et gain better understanding hidden Markov model considering generative point Recall generate samples mixture Hidden Markov Models Figure Illustration sampling hidden Markov model having 3-state latent variable Gaussian emission model Contours constant probability density emission distributions corresponding states latent sample points drawn hidden Markov colour coded according component generated lines connecting successive transition matrix fixed state probability making transition consequently probability remaining chose components random probability mixing coefficients generate sample vector corresponding Gaussian process repeated times generate data set independent case hidden Markov procedure modified choose initial latent variable probabilities governed parameters sample corresponding observation choose state variable according transition probabilities instantiated value suppose sample corresponds state choose state probabilities Ajk know draw sample sample latent variable example ancestral sampling directed graphical model diago-Section nal transition elements Akk larger off-diagonal typical data sequence long runs points generated single infrequent transitions component generation samples hidden Markov model illustrated Figure variants standard HMM obtained instance imposing constraints form transition matrix mention particular practical importance called left-to-right obtained setting elements Ajk zero illustrated SEQUENTIAL DATA Figure Example state transition diagram 3-state left-to-right hidden Markov Note state later A11 A22 A33 A12 A23 A13 state transition diagram 3-state HMM Figure Typically models initial state probabilities modified words sequence constrained start state transition matrix constrained ensure large changes state index Ajk type model illustrated lattice diagram Figure applications hidden Markov example speech on-line character make use left-to-right illustration left-to-right hidden Markov consider example involving handwritten uses on-line meaning digit represented trajectory pen function time form sequence pen contrast off-line digits discussed Appendix comprises static two-dimensional pixellated images Examples online digits shown Figure train hidden Markov model subset data comprising examples digit generate line segment fixed length having possible emission distribution simply table probabilities associated allowed angle values state index Transition probabilities set zero state index increment model parameters optimized iterations gain insight resulting model running shown Figure Figure Lattice diagram 3-state leftto-right HMM state index allowed increase A11 A11 A11 A33 A33 A33 Hidden Markov Models Figure examples on-line handwritten synthetic digits sampled generatively left-to-right hidden Markov model trained data set handwritten powerful properties hidden Markov models ability exhibit degree invariance local warping time understand consider way digit written on-line handwritten digits typical digit comprises distinct sections joined starts sweeping arc cusp loop followed second moreor-less straight sweep ending Natural variations writing style cause relative sizes sections location cusp loop temporal sequence generative perspective variations accommodated hidden Markov model changes number transitions state versus number transitions successive digit written reverse starting right ending pen tip coordinates identical example training probability observations model extremely speech recognition warping time axis associated natural variations speed hidden Markov model accommodate distortion penalize Maximum likelihood HMM observed data set determine parameters HMM maximum likelihood function obtained joint distribution marginalizing latent variables joint distribution does factorize contrast mixture distribution considered Chapter simply treat summations perform summations explicitly variables summed resulting total KN number terms summation grows SEQUENTIAL DATA exponentially length summation corresponds summing exponentially paths lattice diagram Figure encountered similar difficulty considered inference problem simple chain variables Figure able make use conditional independence properties graph re-order summations order obtain algorithm cost scales instead length shall apply similar technique hidden Markov difficulty expression likelihood function corresponds generalization mixture represents summation emission models different settings latent Direct maximization likelihood function lead complex expressions closed-form case simple mixture modelsSection mixture model data special case turn expectation maximization algorithm efficient framework maximizing likelihood function hidden Markov EM algorithm starts initial selection model denote parameter values posterior distribution latent variables use posterior distribution evaluate expectation logarithm complete-data likelihood function parameters function defined convenient introduce shall use denote marginal posterior distribution latent variable denote joint posterior distribution successive latent value store set nonnegative numbers sum similarly store matrix nonnegative numbers sum shall use denote conditional probability znk similar use notation probabilistic variables introduced expectation binary random variable just probability takes value substitute joint distribution Hidden Markov Models make use definitions obtain lnAjk goal step evaluate quantities shall discuss maximize respect parameters treat Maximization respect easily achieved appropriate Lagrange multipliers resultsExercise Ajk EM algorithm initialized choosing starting values course respect summation constraints associated probabilistic Note elements set zero initially remain zero subsequent EM typical initialization procedure wouldExercise involve selecting random starting values parameters subject summation non-negativity Note particular modification EM results required case left-to-right models choosing initial values elements Ajk appropriate elements set remain zero maximize respect notice final term depends furthermore term exactly form data-dependent term corresponding function standard mixture distribution seen comparison case Gaussian quantities playing role parameters independent different term decouples sum terms value maximized simply maximizing weighted log likelihood function emission density weights shall suppose maximization case SEQUENTIAL DATA Gaussian emission densities maximization function gives case discrete multinomial observed conditional distribution observations takes form corresponding M-step equations byExercise analogous result holds Bernoulli observed EM algorithm requires initial values parameters emission way set treat data initially fit emission density maximum use resulting values initialize parameters forward-backward algorithm seek efficient procedure evaluating quantities corresponding step EM graph hidden Markov shown Figure know posterior distribution latent variables obtained efficiently twostage message passing particular context hidden MarkovSection known forward-backward algorithm Baum-Welch algorithm fact variants basic lead exact according precise form Hidden Markov Models messages propagated chain shall focus widely used known alpha-beta great practical importance forwardbackward algorithm provides nice illustration concepts introduced earlier shall begin section derivation forward-backward making use sum product rules exploiting conditional independence properties shall obtain corresponding graphical model Section shall forward-backward algorithm obtained simply specific example sum-product algorithm introduced Section worth emphasizing evaluation posterior distributions latent variables independent form emission density observed variables continuous require values quantities value section shall omit explicit dependence model parameters fixed begin writing following conditional independence properties relations easily proved instance note path nodes node xn passes node paths follows conditional independence property reader moments verify properties exercise application relations proved significantly greater joint distribution hidden Markov model sum product rules Let begin evaluating Recall discrete multinomial random variable expected value components just probability component having value interested finding posterior distribution observed data set SEQUENTIAL DATA represents vector length entries correspond expected values Note denominator implicitly conditioned parameters HMM represents likelihood conditional independence property product rule obtain defined quantity represents joint probability observing data time value represents conditional probability future data time value represent set possible settings 1-of-K coded binary vector shall use notation denote value znk analogous interpretation derive recursion relations allow evaluated shall make use conditional independence particular sum product allowing express terms follows Making use definition obtain Hidden Markov Models Figure Illustration forward recursion evaluation fragment quantity obtained taking elements step summing weights corresponding values multiplying data contribution A11 A21 A31 worth taking moment study recursion relation Note terms right-hand evaluated values step recursion computational cost scaled like forward recursion equation illustrated lattice diagram Figure order start need initial condition tells takes value Starting node work chain evaluate latent step recursion involves multiplying overall cost evaluating quantities chain similarly recursion relation quantities making use conditional independence properties giving SEQUENTIAL DATA Figure Illustration backward recursion evaluation fragment quantity obtained taking components step summing weights products corresponding values corresponding values emission density A11 A12 A13 Making use definition obtain Note case backward message passing algorithm evaluates terms absorb effect observation emission probability multiply transition matrix marginalize illustrated Figure need starting condition value obtained setting replacing definition correct provided settings step quantity cancel M-step equation takes form quantity represents likelihood function value typically wish monitor EM useful able evaluate sum sides use fact left-hand normalized obtain Hidden Markov Models evaluate likelihood function computing convenient choice want evaluate likelihood running recursion start end use result making use fact vector case recursion simply Let moment interpret result Recall compute likelihood joint distribution sum possible values value represents particular choice hidden state time words term summation path lattice recall exponentially expressing likelihood function form reduced computational cost exponential length chain linear swapping order summation time step sum contributions paths passing states znk intermediate quantities consider evaluation quantities correspond values conditional probabilities settings definition applying use conditional independence property definitions calculate directly results Let summarize steps required train hidden Markov model EM make initial selection parameters parameters initialized uniformly randomly uniform distribution non-negativity summation Initialization parameters depend form instance case parameters initialized applying K-means algorithm initialized covariance matrix corresponding means run forward recursion backward recursion use results evaluate evaluate likelihood SEQUENTIAL DATA completes use results revised set parameters M-step equations Section continue alternate steps convergence criterion instance change likelihood function Note recursion relations observations enter conditional distributions form recursions independent type dimensionality observed variables form conditional long value computed possible states observed variables quantities pre-computed functions start EM remain fixed seen earlier chapters maximum likelihood approach effective number data points large relation number note hidden Markov model trained maximum provided training sequence sufficiently make use multiple shorter requires straightforward modification hidden Markov model EM case left-to-rightExercise particularly important observation state transition corresponding nondiagonal element seen quantity predictive observed data wish predict important real-time applications financial make use sum product rules conditional independence properties giving evaluated running forward recursion computing final summations result summation stored used value observed order run recursion forward step order predict subsequent value Hidden Markov Models Figure fragment factor graph representation hidden Markov gn xn Note influence data xN summarized values predictive distribution carried forward indefinitely fixed required real-time discussed estimation parameters HMM maximum framework easily extended regularized maximum likelihood introducing priors model parameters values estimated maximizing posterior EM algorithm step discussed step involves adding log prior distribution function maximization represents straightforward application techniques developed various points use variational methods fully Bayesian treatment HMM marginalize theSection parameter distributions maximum leads two-pass forward-backward recursion compute posterior sum-product algorithm HMM directed graph represents hidden Markov shown Figure tree solve problem finding local marginals hidden variables sum-product turns toSection equivalent forward-backward algorithm considered previous sum-product algorithm provides simple way derive alpha-beta recursion begin transforming directed graph Figure factor representative fragment shown Figure form factor graph shows latent purpose solving inference shall conditioning variables simplify factor graph absorbing emission probabilities transition probability leads simplified factor graph representation Figure factors SEQUENTIAL DATA Figure simplified form factor graph hidden Markov fn derive alpha-beta denote final hidden variable root pass messages leaf node general results message messages propagated hidden Markov model form equations represent propagation messages forward chain equivalent alpha recursions derived previous shall Note variable nodes perform eliminate recursion messages form recall definition define obtain alpha recursion need verify quantities equivalent defined easily initial condition noting identical initial iteratively computed subsequent quantities consider messages propagated root node leaf form eliminated messages type variable nodes perform definition substitute defining Hidden Markov Models obtain beta recursion verify beta variables equivalent noting implies initial message send root variable node identical initialization Section sum-product algorithm specifies evaluate marginals messages result shows local marginal node product incoming conditioned variables computing joint distribution Dividing sides obtain agreement result similarly derived Scaling factors important issue addressed make use forward backward algorithm recursion relation note step new value obtained previous value multiplying quantities probabilities significantly work way forward values zero exponentially moderate lengths chain calculation soon exceed dynamic range double precision floating point case implicitly circumvented problem evaluation likelihood functions taking help forming sums products small numbers fact implicitly summing possible paths lattice diagram Figure work re-scaled versions values remain order shall corresponding scaling factors cancel use re-scaled quantities EM defined representing joint distribution observations xn latent variable define normalized version expect behaved numerically probability distribution variables value order relate scaled original alpha introduce scaling factors defined conditional distributions observed variables SEQUENTIAL DATA product cm cm turn recursion equation Note stage forward message passing used evaluate evaluate store easily coefficient normalizes right-hand similarly define re-scaled variables cm remain machine precision quantities simply ratio conditional probabilities recursion result gives following recursion re-scaled variables applying recursion make use scaling factors previously computed likelihood function required marginals byExercise Hidden Markov Models note alternative formulation forward-backward algorithm backward pass defined recursion based quantities instead recursion requires forward pass completed quantities available backward forward backward passes algorithm algorithms comparable computational version commonly encountered case hidden Markov linear dynamical systems aSection recursion analogous form Viterbi algorithm applications hidden Markov latent variables meaningful probable sequence hidden states observation instance speech wish probable phoneme sequence series acoustic graph hidden Markov model directed problem solved exactly max-sum recall discussion Section problem finding probable sequence latent states finding set states individually problem solved running forward-backward algorithm latent variable marginals maximizing individually et set states correspond probable sequence set states represent sequence having zero happens successive isolation individually transition matrix element connecting usually interested finding probable sequence solved efficiently max-sum context hidden Markov models known Viterbi algorithm Note max-sum algorithm works log probabilities need use re-scaled variables forward-backward Figure shows fragment hidden Markov model expanded lattice number possible paths lattice grows exponentially length Viterbi algorithm searches space paths efficiently probable path computational cost grows linearly length sum-product represent hidden Markov model factor shown Figure treat variable node pass messages root starting leaf results messages passed max-sum algorithm max SEQUENTIAL DATA Figure fragment HMM lattice showing possible Viterbi algorithm efficiently determines probable path exponentially corresponding probability product elements transition matrix corresponding probabilities segment emission densities associated node eliminate make use obtain recursion messages form max introduced notation messages initialized used Note notation omit dependence model parameters held fixed finding probable Viterbi algorithm derived directly definition joint distribution taking logarithm exchanging maximizations easily seen quantities probabilisticExercise interpretation max completed final maximization obtain value joint distribution corresponding probable wish sequence latent variable values corresponds simply make use back-tracking procedure discussed Section note maximization performed possible values Suppose record values correspond maxima value values Let denote function passed messages end chain probable state use function backtrack chain applying recursively kmaxn max Hidden Markov Models understand Viterbi algorithm consider explicitly exponentially paths evaluate probability select path having highest notice make dramatic saving computational cost Suppose path evaluate probability summing products transition emission probabilities work way forward path Consider particular time step particular state time possible paths converging corresponding node lattice need retain particular path far highest states time step need track time step possible paths comprising possible paths leading current need retain corresponding best path state time reach final time step discover state corresponds overall probable unique path coming state trace path step state occupied lattice state Extensions hidden Markov model basic hidden Markov standard training algorithm based maximum extended numerous ways meet requirements particular discuss important digits example Figure hidden Markov models quite poor generative models synthetic digits look quite unrepresentative training goal sequence significant benefit determining parameters hidden Markov models discriminative maximum likelihood Suppose training set observation sequences labelled according class separate hidden Markov model parameters treat problem determining parameter values standard classification problem optimize cross-entropy theorem expressed terms sequence probabilities associated hidden Markov models prior probability class Optimization cost function complex maximum likelihood particular SEQUENTIAL DATA Figure Section autoregressive hidden Markov distribution observation xn depends subset previous observations hidden state distribution xn depends previous observations xn requires training sequence evaluated models order compute denominator Hidden Markov coupled discriminative training widely used speech recognition significant weakness hidden Markov model way represents distribution times remains note probability sequence sampled hidden Markov model spend precisely steps state make transition different state exp exponentially decaying function unrealistic model state problem resolved modelling state duration directly diagonal coefficients Akk set state explicitly associated probability distribution possible duration generative point state value representing number time steps remain state drawn model emits values observed variable generally assumed independent corresponding emission density simply approach requires straightforward modifications EM optimization procedure limitation standard HMM poor capturing longrange correlations observed variables variables separated time mediated first-order Markov chain hidden Longer-range effects principle included adding extra links graphical model Figure way address generalize HMM autoregressive hidden Markov model et example shown Figure discrete corresponds expanded tables conditional probabilities emission case Gaussian emission use linearGaussian framework conditional distribution xn values previous value Gaussian mean linear combination values conditioning Clearly number additional links graph limited avoid excessive number free example shown Figure observation depends Hidden Markov Models Figure Example input-output hidden Markov emission probabilities transition probabilities depend values sequence observations xn preceding observed variables hidden graph looks appeal d-separation fact simple probabilistic imagine conditioning standard values corresponding conditional independence property easily verified noting path node node passes observed node head-to-tail respect use forward-backward recursion step EM algorithm determine posterior distributions latent variables computational time linear length step involves minor modification standard M-step case Gaussian emission densities involves estimating parameters standard linear regression discussed Chapter seen autoregressive HMM appears natural extension standard HMM viewed graphical fact probabilistic graphical modelling viewpoint motivates plethora different graphical structures based example input-output hidden Markov model sequence observed variables addition output variables values influence distribution latent variables output example shown Figure extends HMM framework domain supervised learning sequential easy use d-separation Markov property chain latent variables verify simply note path node node head-to-tail respect observed node conditional independence property allows formulation computationally efficient learning determine parameters model maximizing likelihood function matrix rows uTn consequence conditional independence property likelihood function maximized efficiently EM algorithm step involves forward backward variant HMM worthy mention factorial hidden Markov model multiple independent SEQUENTIAL DATA Figure factorial hidden Markov model comprising Markov chains latent continuous observed variables possible choice emission model linear-Gaussian density mean Gaussian linear combination states corresponding latent xn Markov chains latent distribution observed variable time step conditional states corresponding latent variables time Figure shows corresponding graphical motivation considering factorial HMM seen noting order bits information time standard HMM need latent factorial HMM make use binary latent primary disadvantage factorial lies additional complexity training step factorial HMM model observation variables introduces dependencies latent leading difficulties seen noting Figure variables connected path head-to-head node xn exact step model does correspond running forward backward recursions Markov chains confirmed noting key conditional independence property satisfied individual Markov chains factorial HMM shown d-separation Figure suppose chains hidden nodes simplicity suppose latent variables number approach note KM combinations latent variables time step Figure Example highlighted head-to-head observed nodes head-to-tail unobserved nodes path blocked conditional independence property does hold individual latent chains factorial HMM efficient exact step xn Linear Dynamical Systems transform model equivalent standard HMM having single chain latent variables KM latent run standard forward-backward recursions computational complexity exponential number latent chains intractable small values solution use sampling methods Chapter elegant deterministic Ghahramani Jordan exploited variational inference techniquesSection obtain tractable algorithm approximate simple variational posterior distribution fully factorized respect latent alternatively powerful approach variational distribution described independent Markov chains corresponding chains latent variables original variational inference algorithms involves running independent forward backward recursions computationally efficient able capture correlations variables possible probabilistic structures constructed according needs particular Graphical models provide general technique analysing variational methods provide powerful framework performing inference models exact solution Linear Dynamical Systems order motivate concept linear dynamical let consider following simple arises practical Suppose wish measure value unknown quantity noisy sensor returns observation representing value plus zero-mean Gaussian single best guess assume improve estimate taking lots measurements averaging random noise terms tend cancel make situation complicated assuming wish measure quantity changing regular measurements point time obtained wish corresponding values simply average error random noise unfortunately just obtain single averaged averaged changing value introducing new source imagine doing bit better estimate value recent say just average changing random noise level sensor make sense choose relatively long window observations signal changing noise levels better just use xN directly estimate better weighted recent measurements SEQUENTIAL DATA make greater contribution recent sort intuitive argument does tell form weighted sort hand-crafted weighing hardly likely address problems systematically defining probabilistic model captures time evolution measurement processes applying inference learning methods developed earlier shall focus widely used model known linear dynamical HMM corresponds state space model shown Figure latent variables discrete arbitrary emission probability graph course describes broader class probability factorize according consider extensions distributions latent consider continuous latent variables summations sum-product algorithm general form inference algorithms hidden Markov interesting note hidden Markov models linear dynamical systems developed expressed graphical deep relationship immediately key requirement retain efficient algorithm inference linear length requires quantity representing posterior probability observations multiply transition probability emission probability marginalize obtain distribution functional form distribution complex change parameter distributions property closed multiplication belonging exponential consider important example practical consider linear-Gaussian state space model latent variables observed variables multivariate Gaussian distributions means linear functions states parents seen directed graph linear-Gaussian units equivalent joint Gaussian distribution marginals functional form messages preserved obtain efficient inference suppose emission densities comprise mixture Gaussians mean linear quantity mixture mixture exact inference practical seen hidden Markov model viewed extension mixture models Chapter allow sequential correlations similar view linear dynamical generalization continuous latent variable models Chapter probabilistic PCA factor pair nodes represents linear-Gaussian latent variable Linear Dynamical Systems model particular latent variables longer treated independent form Markov model represented tree-structured directed inference problems solved efficiently sum-product forward analogous messages hidden Markov known Kalman filter equations Zarchan backward analogous known Kalman smoother Rauch-Tung-Striebel equations et Kalman filter widely used real-time tracking linear dynamical linear-Gaussian joint distribution marginals follows sequence individually probable latent variable values probable latent need consider theExercise analogue Viterbi algorithm linear dynamical model linear-Gaussian conditional write transition emission distributions general form initial latent variable Gaussian distribution write Note order simplify omitted additive constant terms means straightforward include distributions commonly expressed equiv-Exercise alent form terms noisy linear equations wn xn Czn vn noise terms distributions parameters denoted determined maximum likelihood EM need solve inference problem determining local posterior marginals latent solved efficiently sum-product discuss SEQUENTIAL DATA Inference LDS turn problem finding marginal distributions latent variables conditional observation parameter wish make predictions latent state observation xn conditioned observed data use real-time inference problems solved efficiently sum-product context linear dynamical gives rise Kalman filter Kalman smoother worth emphasizing linear dynamical linearGaussian joint distribution latent observed variables simply principle solve inference problems standard results derived previous chapters marginals conditionals multivariate role sum-product algorithm provide efficient way perform Linear dynamical systems identical hidden Markov described factor graphs Figures Inference algorithms precisely form summations latent variables replaced begin considering forward equations treat root propagate messages leaf node initial message factors subsequent messages shall propagate messages normalized marginal distributions corresponding denote precisely analogous propagation scaled variables discrete case hidden Markov recursion equation takes form Substituting conditionals making use cnN supposing evaluating integral wish determine values integral easily evaluated making use result follows Linear Dynamical Systems defined combine result factor right-hand making use Vn use matrix inverse identities defined Kalman gain matrix Kn values new observation evaluate Gaussian marginal having mean covariance normalization coefficient initial conditions recursion equations obtained make use calculate calculate giving V0CT CV0CT likelihood function linear dynamical factors Kalman filtering interpret steps involved going posterior marginal posterior marginal view quantity prediction mean obtained simply taking mean projecting forward step transition probability matrix predicted mean predicted observation xn obtained applying emission probability matrix predicted hidden state view update equation mean hidden variable distribution taking predicted mean adding correction proportional error xn predicted observation actual coefficient correction Kalman gain view Kalman filter process making successive predictions correcting predictions light new illustrated graphically Figure SEQUENTIAL DATA Figure linear dynamical viewed sequence steps increasing uncertainty state variable diffusion compensated arrival new left-hand blue curve shows distribution incorporates data step diffusion arising nonzero variance transition probability gives distribution shown red centre Note broader shifted relative blue curve shown dashed centre plot data observation xn contributes emission density shown function green right-hand Note density respect normalized Inclusion new data point leads revised distribution state density shown observation data shifted narrowed distribution compared shown dashed right-hand plot consider situation measurement noise small compared rate latent variable posterior distribution depends current measurement accordance withExercise intuition simple example start latent variable evolving slowly relative observation noise posterior mean obtained averaging measurements obtained important applications Kalman filter illustrated simple example object moving dimensions Figure solved inference problem finding posterior marginal node observations turn problem finding marginal node observations xN temporal corresponds inclusion future past used real-time plays key role learning parameters analogy hidden Markov problem solved propagating messages node xN node combining information obtained forward message passing stage used compute LDS usual formulate backward recursion terms terms write form derive required start backward recursion Linear Dynamical Systems Figure illustration linear dynamical used track moving blue points indicate true positions object two-dimensional space successive time green points denote noisy measurements red crosses indicate means inferred posterior distributions positions obtained running Kalman filtering covariances inferred positions indicated red correspond contours having standard continuous latent written form multiply sides substitute make use manipulation obtainExercise Jn Vn Jn JTn defined Jn VnAT use AVn PnJTn Note recursions require forward pass completed quantities Vn available backward EM require pairwise posterior obtained form Substituting Gaussian mean components covariance byExercise SEQUENTIAL DATA Learning LDS considered inference problem linear dynamical assuming model parameters consider determination parameters maximum likelihood model latent addressed EM discussed general terms Chapter derive EM algorithm linear dynamical Let denote estimated parameter values particular cycle algorithm parameter run inference algorithm determine posterior distribution latent variables precisely local posterior marginals required shall require following expectations znzTn used consider complete-data log likelihood obtained taking logarithm dependence parameters expectation complete-data log likelihood respect posterior distribution defines function function maximized respect components Consider parameters substitute expectation respect obtain const terms dependent absorbed additive Maximization respect easily performed making use maximum likelihood solution Gaussian distribution discussed Section givingExercise Linear Dynamical Systems Vnew0 optimize substitute giving const constant comprises terms independent Maximizing respect parameters givesExercise Anew znzTn AnewE Note Anew evaluated result used determine order determine new values substitute giving Maximizing respect givesExercise Cnew xnE zTn znzTn xnxTn zTn Cnew CnewE znzTn Cnew SEQUENTIAL DATA approached parameter learning linear dynamical maximum Inclusion priors MAP estimate fully Bayesian treatment applying analytical approximation techniques discussed Chapter detailed treatment precluded lack Extensions LDS hidden Markov considerable extending basic linear dynamical order increase assumption linear-Gaussian model leads efficient algorithms inference implies marginal distribution observed variables simply represents significant simple extension linear dynamical use Gaussian mixture initial distribution mixture forward recursion equations lead mixture Gaussians hidden variable model Gaussian emission density poor instead try use mixture Gaussians emission posterior mixture posterior comprise mixture mixture Kn number components grows exponentially length model introducing transition emission models depart linear-Gaussian exponential model leads intractable inference make deterministic approximations assumed density filtering expectation make use sampling discussed Section widely used approach make Gaussian approximation linearizing mean predicted gives rise extended Kalman filter hidden Markov develop interesting extensions basic linear dynamical expanding graphical switching state space model viewed combination hidden Markov model set linear dynamical model multiple Markov chains continuous linear-Gaussian latent analogous latent chain linear dynamical discussed Markov chain discrete variables form used hidden Markov output time step determined stochastically choosing continuous latent state discrete latent variable emitting observation corresponding conditional output Exact inference model variational methods lead efficient inference scheme involving forward-backward recursions continuous discrete Markov chains Note consider multiple chains discrete latent use switch select obtain analogous model having discrete latent variables known switching hidden Markov Linear Dynamical Systems Particle filters dynamical systems use non-Gaussian emission turn sampling methods orderChapter tractable inference apply samplingimportance-resampling formalism Section obtain sequential Monte Carlo algorithm known particle Consider class distributions represented graphical model Figure suppose observed values Xn wish draw samples posterior distribution dzn dzn dzn set samples drawn use conditional independence property follows graph Figure sampling weights defined samples used numerator posterior distribution represented set samples corresponding weights Note weights satisfy wish sequential sampling shall suppose set samples weights obtained time step subsequently observed value wish weights samples time step sample distribution SEQUENTIAL DATA straightforward theorem dzn dzn dzn dzn use conditional independence properties follow application d-separation criterion graph Figure distribution mixture samples drawn choosing component probability mixing coefficients drawing sample corresponding view step particle filter algorithm comprising time step sample representation posterior distribution expressed samples corresponding weights viewed mixture representation form obtain corresponding representation time draw samples mixture distribution sample use new observation evaluate corresponding weights case single variable Figure particle sequential Monte approach appeared literature various names including bootstrap filter et survival fittest et condensation algorithm Exercises www Use technique discussed Section verify Markov model shown Figure having nodes total satisfies conditional independence properties model described graph Figure nodes total Exercises Figure Schematic illustration operation particle filter one-dimensional latent time step posterior represented mixture shown schematically circles sizes proportional weights set samples drawn distribution new weights evaluated satisfies conditional independence properties Consider joint probability distribution corresponding directed graph Figure sum product rules verify joint distribution satisfies conditional independence property second-order Markov model described joint distribution satisfies conditional independence property distribution observed data state space model represented directed graph Figure does satisfy conditional independence properties does exhibit Markov property finite www Consider hidden Markov model emission densities represented parametric model linear regression model neural vector adaptive parameters learned data maximum SEQUENTIAL DATA Verify M-step equations initial state probabilities transition probability parameters hidden Markov model maximization expected complete-data log likelihood function appropriate Lagrange multipliers enforce summation constraints components elements parameters hidden Markov model initially set elements remain zero subsequent updates EM Consider hidden Markov model Gaussian emission maximization function respect mean covariance parameters Gaussians gives rise M-step equations www hidden Markov model having discrete observations governed multinomial conditional distribution observations hidden variables corresponding step equations Write analogous equations conditional distribution step equations case hidden Markov multiple binary output variables governed Bernoulli conditional refer Sections discussion corresponding maximum likelihood solutions data www Use d-separation criterion verify conditional independence properties satisfied joint distribution hidden Markov model defined applying sum product rules verify conditional independence properties satisfied joint distribution hidden Markov model defined Starting expression marginal distribution variables factor factor results messages sum-product algorithm obtained Section derive result joint posterior distribution successive latent variables hidden Markov Suppose wish train hidden Markov model maximum likelihood data comprises independent sequences denote step EM simply evaluate posterior probabilities latent variables running recursions independently initial probability transition probability parameters re-estimated Exercises modified forms Ajk notational assumed sequences length generalization sequences different lengths M-step equation re-estimation means Gaussian emission models Note M-step equations emission model parameters distributions analogous www Use definition messages passed factor node variable node factor expression joint distribution hidden Markov definition alpha message definition Use definition messages passed factor node variable node factor expression joint distribution hidden Markov definition beta message definition Use expressions marginals hidden Markov model derive corresponding results expressed terms re-scaled derive forward message passing equation Viterbi algorithm directly expression joint involves maximizing hidden variables taking logarithm exchanging maximizations derive recursion SEQUENTIAL DATA quantities defined initial condition recursion www directed graph input-output hidden Markov Figure expressed tree-structured factor graph form shown Figure write expressions initial factor general factor result Exercise derive recursion including initial forward-backward algorithm input-output hidden Markov model shown Figure www Kalman filter smoother equations allow posterior distributions individual latent conditioned observed efficiently linear dynamical sequence latent variable values obtained maximizing posterior distributions individually probable sequence latent simply note joint distribution latent observed variables linear dynamical conditionals marginals make use result www Use result prove Use results matrix identities derive results Kalman gain matrix Kn defined www definitions result derive definitions result derive www Consider generalization include constant terms Gaussian extension re-case framework discussed chapter defining state vector additional component fixed augmenting matrices extra columns corresponding parameters Kalman filter equations applied independent reduce results Section maximum likelihood solution single Gaussian Consider problem finding mean single Gaussian random variable set independent observations model use Exercises linear dynamical governed latent variables identity matrix transition probability observations Let parameters initial state denoted suppose Write corresponding Kalman filter equations starting general results equivalent results obtained directly considering independent Consider special case linear dynamical Section equivalent probabilistic transition matrix covariance noise covariance making use matrix inversion identity emission density matrix denoted posterior distribution hidden states defined reduces result probabilistic www Consider linear dynamical form discussed Section amplitude observation noise goes posterior distribution mean xn zero accords intuition just use current observation xn estimate state variable ignore previous Consider special case linear dynamical Section state variable constrained equal previous state corresponds assume initial conditions predictions determined purely Use proof induction posterior mean state determined average corresponds intuitive result state variable best estimate obtained averaging Starting backwards recursion equation derive RTS smoothing equations Gaussian linear dynamical Starting result pairwise posterior marginal state space derive specific form case Gaussian linear dynamical Starting result substituting verify result covariance www Verify results M-step equations linear dynamical Verify results M-step equations linear dynamical SEQUENTIAL DATA Verify results M-step equations linear dynamical 
Combining Models earlier explored range different models solving classification regression improved performance obtained combining multiple models instead just single model train different models make predictions average predictions combinations models called Section discuss ways apply committee concept insight effective important variant committee known involves training multiple models sequence error function used train particular model depends performance previous produce substantial improvements performance compared use single model discussed Section Instead averaging predictions set alternative form COMBINING MODELS model combination select models make choice model function input different models responsible making predictions different regions input widely used framework kind known decision tree selection process described sequence binary selections corresponding traversal tree structure discussed Section individual models generally chosen overall flexibility model arises input-dependent selection Decision trees applied classification regression limitation decision trees division input space based hard splits model responsible making predictions value input decision process softened moving probabilistic framework combining discussed Section set models conditional distribution input target indexes form probabilistic mixture form represent input-dependent mixing models viewed mixture distributions component mixing conditioned input variables known mixtures closely related mixture density network model discussed Section Bayesian Model Averaging important distinguish model combination methods Bayesian model understand consider example density estimation mixture Gaussians severalSection Gaussian components combined model contains binary latent variable indicates component mixture responsible generating corresponding data model specified terms joint distribution corresponding density observed variable obtained marginalizing latent variable Committees case Gaussian mixture leads distribution form usual interpretation example model identically distributed use write marginal probability data set form observed data point xn corresponding latent variable suppose different models indexed prior probabilities instance model mixture Gaussians model mixture Cauchy marginal distribution data set example Bayesian model interpretation summation just model responsible generating data probability distribution simply reflects uncertainty model size data set uncertainty posterior probabilities increasingly focussed just highlights key difference Bayesian model averaging model Bayesian model averaging data set generated single combine multiple different data points data set potentially generated different values latent variable different considered marginal probability considerations apply predictive density conditional distributions Committees simplest way construct committee average predictions set individual procedure motivated frequentist perspective considering trade-off bias decomposes er-Section ror model bias component arises differences model true function variance component represents sensitivity model individual data Recall Figure COMBINING MODELS trained multiple polynomials sinusoidal averaged resulting contribution arising variance term tended leading improved averaged set low-bias models higher order obtained accurate predictions underlying sinusoidal function data single data way introduce variability different models approach use bootstrap data discussed Section Consider regression problem trying predict value single continuous suppose generate bootstrap data sets use train separate copy predictive model committee prediction procedure known bootstrap aggregation bagging Suppose true regression function trying predict output models written true value plus error form average sum-of-squares error takes form denotes frequentist expectation respect distribution input vector average error models acting individually EAV expected error committee ECOM assume errors zero mean Boosting obtainExercise ECOM apparently dramatic result suggests average error model reduced factor simply averaging versions depends key assumption errors individual models errors typically highly reduction overall error generally shown expected committee error exceed expected error constituent ECOM order achieve significant turn moreExercise sophisticated technique building known Boosting Boosting powerful technique combining multiple classifiers produce form committee performance significantly better base widely used form boosting algorithm called short developed Freund Schapire Boosting good results base classifiers performance slightly better base classifiers known weak Originally designed solving classification boosting extended regression principal difference boosting committee methods bagging discussed base classifiers trained base classifier trained weighted form data set weighting coefficient associated data point depends performance previous points misclassified base classifiers greater weight used train classifier classifiers predictions combined weighted majority voting illustrated schematically Figure Consider two-class classification training data comprises input vectors corresponding binary target variables data point associated weighting parameter initially set data shall suppose procedure available training base classifier weighted data function stage AdaBoost trains new classifier data set weighting coefficients adjusted according performance previously trained classifier greater weight misclassified data desired number base classifiers combined form committee coefficients different weight different base precise form AdaBoost algorithm COMBINING MODELS Figure Schematic illustration boosting base classifier trained weighted form training set weights depend performance previous base classifier base classifiers combined final classifier sign AdaBoost Initialize data weighting coefficients setting Fit classifier training data minimizing weighted error function Jm indicator function equals Evaluate quantities use evaluate Update data weighting coefficients exp Boosting Make predictions final sign base classifier trained weighting coefficients corresponds usual procedure training single subsequent iterations weighting coefficients increased data points misclassified decreased data points correctly Successive classifiers forced place greater emphasis points misclassified previous data points continue misclassified successive classifiers receive greater quantities represent weighted measures error rates base classifiers data weighting coefficients defined greater weight accurate classifiers computing overall output AdaBoost algorithm illustrated Figure subset data points taken toy classification data set shown Figure base learners consists threshold input simple classifier corresponds form decision tree known deci-Section sion tree single base learner classifies input according input features exceeds threshold simply partitions space regions separated linear decision surface parallel Minimizing exponential error Boosting originally motivated statistical learning leading upper bounds generalization bounds turn loose practical actual performance boosting better bounds Friedman et gave different simple interpretation boosting terms sequential minimization exponential error Consider exponential error function defined exp classifier defined terms linear combination base classifiers form training set target goal minimize respect weighting coefficients parameters base classifiers COMBINING MODELS Figure Illustration boosting base learners consist simple thresholds applied figure shows number base learners trained decision boundary recent base learner black combined decision boundary ensemble green data point depicted circle radius indicates weight assigned data point training recently added base points misclassified base learner greater weight training base Instead doing global error function shall suppose base classifiers coefficients minimizing respect Separating contribution base classifier write error function form exp exp coefficients viewed constants optimizing denote Tm set data points correctly classified denote remaining misclassified points turn rewrite error function Boosting form minimize respect second term equivalent minimizing overall multiplicative factor summation does affect location minimizing respect obtain defined having weights data points updated exp Making use fact weights updated iteration exp term independent weights data points factor obtain base classifiers new data points classified evaluating sign combined function defined according factor does affect sign giving Error functions boosting exponential error function minimized AdaBoost algorithm differs considered previous gain insight nature exponential error consider expected error perform variational minimization respect possible functions obtainExercise COMBINING MODELS Figure Plot exponential rescaled cross-entropy error functions hinge error used support vector misclassification error Note large negative values cross-entropy gives linearly increasing exponential loss gives exponentially increasing half AdaBoost algorithm seeking best approximation log odds space functions represented linear combination base subject constrained minimization resulting sequential optimization result motivates use sign function arrive final classification seen minimizer cross-entropy error two-class classification posterior class case target variable seen error function bySection compared exponential error function Figure divided cross-entropy error constant factor passes point ease seen continuous approximations ideal misclassification error advantage exponential error sequential minimization leads simple AdaBoost penalizes large negative values strongly large negative values cross-entropy grows linearly exponential error function grows exponentially exponential error function robust outliers misclassified data important difference cross-entropy exponential error function interpreted log likelihood function well-defined probabilistic exponential error does notExercise generalize classification problems having contrast cross-entropy probabilistic easily generalized interpretation boosting sequential optimization additive model exponential error et opens door wide range boosting-like including multiclass altering choice error motivates extension regression problems consider sum-of-squares error function sequential minimization additive model form simply involves fitting new base classifier residual errors previous AsExercise sum-of-squares error robust Tree-based Models Figure Comparison squared error absolute error showing places emphasis large errors robust outliers mislabelled data addressed basing boosting algorithm absolute deviation error functions compared Figure Tree-based Models various widely models work partitioning input space cuboid edges aligned assigning simple model viewed model combination method model responsible making predictions point input process selecting specific new input described sequential decision making process corresponding traversal binary tree splits branches focus particular tree-based framework called classification regression CART et variants going names ID3 Figure shows illustration recursive binary partitioning input corresponding tree step Figure Illustration two-dimensional input space partitioned regions axis-aligned COMBINING MODELS Figure Binary tree corresponding partitioning input space shown Figure divides input space regions according parameter creates subdivided region subdivided according giving rise regions denoted recursive subdivision described traversal binary tree shown Figure new input determine region falls starting tree root node following path specific leaf node according decision criteria Note decision trees probabilistic graphical separate model predict target regression simply predict constant classification assign region specific key property treebased makes popular fields medical readily interpretable humans correspond sequence binary decisions applied individual input predict ask temperature greater answer ask blood pressure leaf tree associated specific order learn model training determine structure including input variable chosen node form split criterion value threshold parameter determine values predictive variable Consider regression problem goal predict single target variable D-dimensional vector input training data consists input vectors corresponding continuous labels partitioning input space minimize sum-of-squares error optimal value predictive variable region just average values data points fall consider determine structure decision fixed number nodes problem determining optimal structure choice input variable split corresponding Tree-based Models minimize sum-of-squares error usually computationally infeasible combinatorially large number possible greedy optimization generally starting single root corresponding input growing tree adding nodes step number candidate regions input space corresponding addition pair leaf nodes existing choice input variables value joint optimization choice region choice input variable efficiently exhaustive search noting choice split variable optimal choice predictive variable local average noted repeated possible choices variable gives smallest residual sum-of-squares error greedy strategy growing remains issue stop adding simple approach stop reduction residual error falls empirically available splits produces significant reduction splits substantial error reduction common practice grow large stopping criterion based number data points associated leaf prune resulting pruning based criterion balances residual error measure model denote starting tree pruning define subtree obtained pruning nodes collapsing internal nodes combining corresponding Suppose leaf nodes indexed leaf node representing region input space having data denoting total number leaf optimal prediction region corresponding contribution residual sum-of-squares pruning criterion regularization parameter determines trade-off overall residual sum-of-squares error complexity model measured number leaf value chosen classification process growing pruning tree sum-of-squares error replaced appropriate measure COMBINING MODELS define proportion data points region assigned class commonly used choices cross-entropy Gini index vanish maximum encourage formation regions high proportion data points assigned cross entropy Gini index better measures misclassification rate growing tree sensitive node unlike misclassification differentiable andExercise better suited gradient based optimization subsequent pruning misclassification rate generally human interpretability tree model CART seen major practice particular tree structure learned sensitive details data small change training data result different set splits et problems tree-based methods kind considered splits aligned axes feature separate classes optimal decision boundary runs degrees axes need large number axis-parallel splits input space compared single non-axis-aligned splits decision tree region input space associated leaf node issue particularly problematic regression typically aiming model smooth tree model produces piecewise-constant predictions discontinuities split Conditional Mixture Models seen standard decision trees restricted axis-aligned splits input constraints expense allowing probabilistic splits functions input just leaf models probabilistic arrive fully probabilistic tree-based model called hierarchical mixture consider Section alternative way motivate hierarchical mixture experts model start standard probabilistic mixtures unconditional density models Gaussians replace component densities conditional HereChapter consider mixtures linear regression models mixtures Conditional Mixture Models logistic regression models simplest mixing coefficients independent input make generalization allow mixing coefficients depend inputs obtain mixture experts allow component mixture model mixture experts obtain hierarchical mixture Mixtures linear regression models advantages giving probabilistic interpretation linear regression model used component complex probabilistic viewing conditional distribution representing linear regression model node directed probabilistic consider simple example corresponding mixture linear regression represents straightforward extension Gaussian mixture model discussed Section case conditional Gaussian consider linear regression governed weight parameter appropriate use common noise governed precision parameter case consider restrict attention single target variable extension multiple outputs denoteExercise mixing coefficients mixture distribution written denotes set adaptive parameters log likelihood function data set observations takes form denotes vector target order maximize likelihood appeal EM turn simple extension EM algorithm unconditional Gaussian mixtures Section build experience unconditional mixture introduce set binary latent variables znk data point elements zero single value indicating component mixture responsible generating data joint distribution latent observed variables represented graphical model shown Figure complete-data log likelihood function takes formExercise znk COMBINING MODELS Figure Probabilistic directed graph representing mixture linear regression defined NW EM algorithm begins choosing initial value model parameter values used evaluate posterior component data point responsibilities used determine respect posterior distribution complete-data log takes form EZ lnN maximize function respect keeping optimization respect mixing coefficients need account constraint aid Lagrange leading M-step re-estimation equation formExercise Note exactly form corresponding result simple mixture unconditional Gaussians consider maximization respect parameter vector wk kth linear regression Substituting Gaussian function function parameter vector takes form const constant term includes contributions weight vectors wj Note quantity maximizing similar standard sum-of-squares error single linear regression inclusion responsibilities represents weighted squares Conditional Mixture Models term corresponding nth data point carries weighting coefficient interpreted effective precision data component linear regression model governed parameter vector fitted separately data set data point weighted responsibility model takes data Setting derivative respect wk equal zero gives write matrix notation Rk diagonal matrix size Solving obtain wk represents set modified normal equations corresponding weighted squares form context logistic Note matrix Rk change solve normal equations afresh subsequent maximize respect Keeping terms depend function written Setting derivative respect equal obtain M-step equation form Figure illustrate EM algorithm simple example fitting mixture straight lines data set having input variable target variable predictive density plotted Figure converged parameter values obtained EM corresponding right-hand plot Figure shown figure result fitting single linear regression gives unimodal predictive mixture model gives better representation data reflected higher likelihood mixture model assigns significant probability mass regions data predictive distribution bimodal values problem resolved extending model allow mixture coefficients functions leading models mixture density networks discussed Section hierarchical mixture experts discussed Section COMBINING MODELS Figure Example synthetic data shown green having input variable target variable mixture linear regression models mean functions shown blue red upper plots initial configuration result running iterations EM result iterations EM initialized reciprocal true variance set target lower plots corresponding responsibilities plotted vertical line data point length blue segment gives posterior probability blue line data point similarly red Mixtures logistic models logistic regression model defines conditional distribution target input straightforward use component distribution mixture giving rise richer family conditional distributions compared single logistic regression example involves straightforward combination ideas encountered earlier sections book help consolidate conditional distribution target probabilistic mixture logistic regression feature yk wTk output component denotes adjustable parameters suppose data set corresponding likelihood Conditional Mixture Models Figure left plot shows predictive conditional density corresponding converged solution Figure gives log likelihood value vertical slice plots particular value represents corresponding conditional distribution plot right shows predictive density single linear regression model fitted data set maximum model smaller log likelihood function nk ynk maximize likelihood function iteratively making use EM involves introducing latent variables znk correspond 1-of-K coded binary indicator variable data point complete-data likelihood function nk matrix latent variables elements initialize EM algorithm choosing initial value model use parameter values evaluate posterior probabilities components data point nk nj ynj responsibilities used expected complete-data log likelihood function EZ ynk COMBINING MODELS step involves maximization function respect keeping Maximization respect usual Lagrange multiplier enforce summation constraint giving familiar result determine note function comprises sum terms indexed depends vectors different vectors decoupled step EM different components interact fixed Note step does closed-form solution solved iteratively iterative reweighted squares gradient Hessian vector wk bySection Hk denotes gradient respect fixed independent solve wk separately IRLS M-step equations component correspond simply fittingSection single logistic regression model weighted data set data point carries weight Figure shows example mixture logistic regression models applied simple classification extension model mixture softmax models classes Mixtures experts Section considered mixture linear regression Section discussed analogous mixture linear simple mixtures extend flexibility linear models include complex predictive increase capability models allowing mixing coefficients functions input known mixture experts model et mixing coefficients known gating functions individual component densities called notion terminology different components model distribution different regions input space Conditional Mixture Models Figure Illustration mixture logistic regression left plot shows data points drawn classes denoted red background colour varies pure red pure denotes true probability class centre plot shows result fitting single logistic regression model maximum background colour denotes corresponding probability class colour near-uniform model assigns probability classes input right plot shows result fitting mixture logistic regression gives higher probability correct labels points blue making predictions gating functions determine components dominant gating functions satisfy usual constraints mixing linear softmax models form experts linear model fitted efficiently EM iterative reweighted squares employed step model significant limitations use linear models gating expert flexible model obtained multilevel gating function hierarchical mixture HME model understand structure imagine mixture distribution component mixture mixture simple unconditional hierarchical mixture trivially equivalent single flat mixture mixingExercise coefficients input hierarchical model HME model viewed probabilistic version decision trees discussed Section trained efficiently maximum likelihood EM algorithm IRLS Bayesian treatment HME beenSection Bishop based variational shall discuss HME worth pointing close connection mixture density network discussed Section principal advantage mixtures experts model optimized EM step mixture component gating model involves convex optimization overall optimization advantage mixture density network approach component COMBINING MODELS densities mixing coefficients share hidden units neural mixture density splits input space relaxed compared hierarchical mixture experts constrained axis Exercises www Consider set models form input target indexes different zh latent variable model set parameters model Suppose models prior probabilities training set Write formulae needed evaluate predictive distribution latent variables model index marginalized Use formulae highlight difference Bayesian averaging different models use latent variables single expected sum-of-squares error EAV simple committee model defined expected error committee Assuming individual errors satisfy derive result www making use inequality special case convex function average expected sum-of-squares error EAV members simple committee expected error ECOM committee satisfy ECOM making use equality result derived previous exercise hods error function just provided convex function www Consider committee allow unequal weighting constituent order ensure predictions remain sensible suppose require bounded value minimum maximum values members necessary sufficient condition constraint coefficients satisfy Exercises www differentiating error function respect parameters AdaBoost algorithm updated defined making variational minimization expected exponential error function respect possible functions minimizing function exponential error function minimized AdaBoost does correspond log likelihood well-behaved probabilistic showing corresponding conditional distribution correctly www sequential minimization sum-of-squares error function additive model form style boosting simply involves fitting new base classifier residual errors previous Verify minimize sum-of-squares error set training values single predictive value optimal solution mean Consider data set comprising data points class data points class Suppose tree model splits leaf node second leaf denotes points assigned points assigned suppose second tree model splits Evaluate misclassification rates trees evaluate cross-entropy Gini index trees lower tree tree Extend results Section mixture linear regression models case multiple target values described vector make use results Section www Verify complete-data log likelihood function mixture linear regression models Use technique Lagrange multipliers M-step re-estimation equation mixing coefficients mixture linear regression models trained maximum likelihood EM www noted use squared loss function regression corresponding optimal prediction target variable new input vector conditional mean predictive conditional mean mixture linear regression models discussed Section linear combination means component Note conditional distribution target data conditional mean poor COMBINING MODELS Extend logistic regression mixture model Section mixture softmax classifiers representing Write EM algorithm determining parameters model maximum www Consider mixture model conditional distribution form mixture component mixture two-level hierarchical mixture equivalent conventional single-level mixture suppose mixing coefficients levels hierarchical model arbitrary functions hierarchical model equivalent single-level model x-dependent mixing consider case mixing coefficients levels hierarchical mixture constrained linear classification hierarchical mixture general represented single-level mixture having linear classification models mixing sufficient construct single consider mixture components components mixture mixing coefficients linear-logistic represented single-level mixture components having mixing coefficients determined linear-softmax 
